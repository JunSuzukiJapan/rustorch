# Token Generation Debugging Analysis

## å•é¡Œã®æ¦‚è¦

Llama-2ãƒ¢ãƒ‡ãƒ«ï¼ˆQ4_K_Mé‡å­åŒ–ï¼‰ãŒBOSãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦é–“é•ã£ãŸäºˆæ¸¬ã‚’è¡Œã†ã€‚

### ç—‡çŠ¶
- å…¥åŠ›: BOS token (1)
- æœŸå¾…: Token 450 (" The") - é«˜ã„logit
- å®Ÿéš›: Token 20780 (é–“é•ã„) - logit 9.579ï¼ˆæœ€é«˜ï¼‰
- Token 450ã®logit: 0.063ï¼ˆéå¸¸ã«ä½ã„ï¼‰

## æ¤œè¨¼æ¸ˆã¿ï¼ˆæ­£ã—ã„å‹•ä½œï¼‰âœ…

### 1. Embedding Extraction
- **å®Ÿè£…**: Column-major layoutï¼ˆ`embedding[dim] = data[dim * vocab_size + token_id]`ï¼‰
- **æ¤œè¨¼**: Token 1ã® embeddingå€¤ãŒæœŸå¾…é€šã‚Š
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `llama.rs:528-538`

### 2. Metal GPU Matmul
- **å®Ÿè£…**: Row-majoræ¨™æº–å®Ÿè£…
- **æ¤œè¨¼**: æ‰‹å‹•è¨ˆç®—ã¨100%ä¸€è‡´ï¼ˆ`test_exact_hidden_state.rs`ã§ç¢ºèªï¼‰
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `metal_shaders.metal:matmul_f32`

### 3. RMSNorm
- **å®Ÿè£…**: `output[i] = (x[i] / RMS) * weight[i]`
- **æ¤œè¨¼**: å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›ã€æ•°å­¦çš„ã«æ­£ã—ã„
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `llama.rs:272-303`

### 4. Element-wise Operations
- **add**: `test_add_operation.rs`ã§æ¤œè¨¼æ¸ˆã¿
- **SwiGLU**: `silu = g / (1 + exp(-g)); output = silu * u` - æ¨™æº–å®Ÿè£…

### 5. Q/K/V Projections (Layer 0)
- **æ¤œè¨¼**: å®Œå…¨ãªå…¥åŠ›ï¼ˆ2048è¦ç´ ï¼‰ã§Q projectionè¨ˆç®—ã€æœŸå¾…å€¤ã¨ä¸€è‡´
- **ãƒ†ã‚¹ãƒˆ**: `test_q_with_full_input.rs`
- **çµæœ**: 100%ä¸€è‡´

### 6. RoPE (Rotary Position Embedding)
- **Position 0ã§ã®å‹•ä½œ**: `cos=1, sin=0` â†’ å€¤ã¯å¤‰åŒ–ã—ãªã„ï¼ˆã“ã‚Œã¯æ­£ã—ã„ï¼‰
- **æ¤œè¨¼**: Debugå‡ºåŠ›ã§ç¢ºèª

### 7. Attention Mechanism (Layer 0)
- **é‡è¦ãªç™ºè¦‹**: BOSãƒˆãƒ¼ã‚¯ãƒ³æ™‚ã€Attentionå‡ºåŠ› = Vå€¤ãã®ã¾ã¾
- **ç†ç”±**: BOSã¯è‡ªåˆ†è‡ªèº«ã«ã®ã¿attendã§ãã‚‹ â†’ attention weight = 1.0
- **ã“ã‚Œã¯ç†è«–çš„ã«æ­£ã—ã„å‹•ä½œ**

### 8. Weight Shapes
ã™ã¹ã¦æ­£ã—ã„:
```
token_embd.weight: [2048, 32000]
blk.0.attn_q.weight: [2048, 2048]
blk.0.attn_k.weight: [2048, 256]  # GQA
blk.0.attn_v.weight: [2048, 256]  # GQA
blk.0.ffn_gate.weight: [2048, 5632]
blk.0.ffn_up.weight: [2048, 5632]
blk.0.ffn_down.weight: [5632, 2048]
output.weight: [2048, 32000]
```

## å•é¡Œã®ç‰¹å®š âŒ

### æœ€çµ‚Hidden StateãŒä¸æ­£ç¢º

**Layer 21å‡ºåŠ›**:
```
[0.70855033, 1.0006536, -0.22543797, 0.7980008, ...]
```

**RMSNormå¾Œï¼ˆLM headã¸ã®å…¥åŠ›ï¼‰**:
```
[1.1820991, 1.5812036, -0.38069266, 1.3746278, ...]
```

**æœ€çµ‚Logits**:
- Token 450: 0.063170ï¼ˆæ­£è§£ã ãŒä½ã„ï¼‰
- Token 20780: 9.579187ï¼ˆé–“é•ã„ã ãŒæœ€é«˜ï¼‰

### æ¤œè¨¼æ¸ˆã¿ã®äº‹å®Ÿ:
1. LM head matmulè‡ªä½“ã¯æ­£ã—ã„ï¼ˆ`test_exact_hidden_state.rs`ã§ç¢ºèªï¼‰
2. ã¤ã¾ã‚Šã€**Layer 0-21ã®ã„ãšã‚Œã‹ã§å€¤ãŒãšã‚Œã¦ã„ã‚‹**
3. Layer 0ã¨Layer 1ã®ä¸­é–“å€¤ã¯å¦¥å½“ãªç¯„å›²å†…

## æ®‹ã‚‹å¯èƒ½æ€§ ğŸ¤”

### 1. Q4_K_M Dequantization
**æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„**

- **å®Ÿè£…å ´æ‰€**: `gguf.rs:606-693`
- **è¤‡é›‘æ€§**: 256è¦ç´ ã®super-blockã€12ãƒã‚¤ãƒˆã®scale dataã€è¤‡é›‘ãªbitæ“ä½œ
- **æ¤œè¨¼æ–¹æ³•**: llama.cppã®å®Ÿè£…ã¨line-by-lineæ¯”è¼ƒ

#### ç¾åœ¨ã®å®Ÿè£…ã®è¦ç‚¹:
```rust
// Super-blockæ§‹é€  (144 bytes):
// - d (f16): super-scale
// - dmin (f16): super-min
// - scales[12]: quantized scales
// - qs[128]: 4-bit quantized values

// Dequantizationå¼:
output = (d * scale * q_val - dmin * min) as f64
```

### 2. Weight Layoutè§£é‡ˆ
**å¯èƒ½æ€§ã¯ä½ã„**

- GGUFã‹ã‚‰ã®shapeã¯ç›´æ¥ä½¿ç”¨
- Matmulå®Ÿè£…ã¯æ¨™æº–çš„ãªrow-major
- ã—ã‹ã—ã€ç‰¹å®šã®weightï¼ˆç‰¹ã«FFN weightsï¼‰ã®transposeãŒå¿…è¦ãªå¯èƒ½æ€§ï¼Ÿ

### 3. Numerical Precision
**å¯èƒ½æ€§ã¯ä½ã„**

- f32 â†’ f64å¤‰æ›æ™‚ã®ç²¾åº¦å•é¡Œï¼Ÿ
- ã—ã‹ã—åŸºæœ¬æ¼”ç®—ã¯æ­£ç¢ºã«å‹•ä½œã—ã¦ã„ã‚‹

## æ¨å¥¨ã•ã‚Œã‚‹ Next Steps ğŸ“‹

### Priority 1: Q4_K_M Dequantizationæ¤œè¨¼
1. llama.cppã®`ggml-quants.c:dequantize_row_q4_K()`ã¨æ¯”è¼ƒ
2. å°ã•ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§ dequant çµæœã‚’ç›´æ¥æ¯”è¼ƒ
3. ç‰¹ã«bit shiftæ“ä½œã¨scale/minè¨ˆç®—ã‚’ç¢ºèª

### Priority 2: Alternative Quantization Test
1. ã‚ˆã‚Šå˜ç´”ãªQ4_0ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¢ãƒ‡ãƒ«ã§è©¦ã™
2. ã¾ãŸã¯ float16/float32 ã®éé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§è©¦ã™
3. å•é¡ŒãŒé‡å­åŒ–ç‰¹æœ‰ã‹ã©ã†ã‹ã‚’ç‰¹å®š

### Priority 3: llama.cppç›´æ¥æ¯”è¼ƒ
1. llama.cppã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–
2. å„å±¤ã®å‡ºåŠ›ã‚’ RusTorch ã¨æ•°å€¤æ¯”è¼ƒ
3. æœ€åˆã«ãšã‚Œã‚‹å±¤ã‚’ç‰¹å®š

## ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«

ä½œæˆã—ãŸãƒ‡ãƒãƒƒã‚°ç”¨ãƒ†ã‚¹ãƒˆ:
- `test_add_operation.rs`: addæ¼”ç®—æ¤œè¨¼
- `test_exact_hidden_state.rs`: final matmulæ¤œè¨¼
- `test_q_projection.rs`: Q projectionæ¤œè¨¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
- `test_q_with_full_input.rs`: Q projectionæ¤œè¨¼ï¼ˆå®Œå…¨ç‰ˆï¼‰
- `test_token_generation.rs`: è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
- `test_single_token.rs`: å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†

## ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›ç®‡æ‰€

Layer 0ã®è©³ç´°ãƒ­ã‚°ï¼ˆ`llama.rs`ï¼‰:
- Line 577-607: Attentionå…¥åŠ›ã€Q/K/Vå€¤
- Line 651-654: Grouped attentionå‡ºåŠ›
- Line 673-676: Output projectionå¾Œ
- Line 727-760: Transformer layerä¸­é–“å€¤

## å‚è€ƒæƒ…å ±

### Llama-2 Architecture
- Hidden size: 2048
- Num layers: 22
- Num heads: 32
- Num KV heads: 4 (Grouped Query Attention)
- Head dim: 64
- FFN intermediate: 5632
- Vocab size: 32000

### Q4_K_M Format
- Super-block size: 256 elements
- Block structure: 144 bytes total
  - 2 bytes: d (f16 super-scale)
  - 2 bytes: dmin (f16 super-min)
  - 12 bytes: quantized scales
  - 128 bytes: 4-bit quantized values (256 nibbles)

## çµè«–

åŸºæœ¬çš„ãªã™ã¹ã¦ã®æ¼”ç®—ã¨Attentionæ©Ÿæ§‹ã¯æ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã€‚å•é¡Œã¯**Q4_K_M dequantization**ã¾ãŸã¯**ç‰¹å®šã®weightå‡¦ç†**ã«èµ·å› ã™ã‚‹å¯èƒ½æ€§ãŒæœ€ã‚‚é«˜ã„ã€‚æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯llama.cppã®å®Ÿè£…ã¨ã®è©³ç´°ãªæ¯”è¼ƒã€‚


## ğŸ¯ å®Œå…¨æ¤œè¨¼çµæœï¼ˆUPDATEï¼‰

### âœ… 100%æ­£ç¢ºã«å‹•ä½œï¼ˆå®Œå…¨ãª2048è¦ç´ å…¥åŠ›ã§ç¢ºèªï¼‰:
1. **RMSNorm**: å®Œå…¨ãª2048è¦ç´ å…¥åŠ›ã§è¨ˆç®—ã€ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã¨100%ä¸€è‡´
2. **FFNè¨ˆç®—**: Gate/Up/Down projectionsã€SwiGLU - ã™ã¹ã¦æ­£ç¢º
3. **å±¤é–“ãƒ‡ãƒ¼ã‚¿ä¼é”**: Layer 0å‡ºåŠ› = Layer 1å…¥åŠ›ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
4. **ã™ã¹ã¦ã®åŸºæœ¬æ¼”ç®—**: Embedding, Matmul, Add, RoPE, Attention - ã™ã¹ã¦æ¤œè¨¼æ¸ˆã¿

### ğŸ” æœ€çµ‚çµè«–

**ã™ã¹ã¦ã®æ¼”ç®—ãŒ100%æ­£ç¢ºã«å‹•ä½œã—ã¦ã„ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€æœ€çµ‚äºˆæ¸¬ã¯é–“é•ã£ã¦ã„ã‚‹ã€‚**

ã“ã‚Œã¯ä»¥ä¸‹ã‚’æ„å‘³ã—ã¾ã™ï¼š
1. å®Ÿè£…ãƒ­ã‚¸ãƒƒã‚¯ã¯å®Œå…¨ã«æ­£ã—ã„
2. æ¼”ç®—ã®é †åºã¨çµ„ã¿åˆã‚ã›ã‚‚æ­£ã—ã„
3. **Weightå€¤ãã®ã‚‚ã®ãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒæ¥µã‚ã¦é«˜ã„**

### æ¨å¥¨ã•ã‚Œã‚‹æ±ºå®šçš„ãƒ†ã‚¹ãƒˆ:
1. **Float16/Float32ã®éé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«**ã§è©¦ã™ â†’ Dequantizationã‚’ãƒã‚¤ãƒ‘ã‚¹
2. **Q4_0ãªã©ã€ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªé‡å­åŒ–**ã§è©¦ã™ â†’ Q4_K_Mç‰¹æœ‰ã®å•é¡Œã‹ç¢ºèª
3. **llama.cppã®ãƒ‡ãƒãƒƒã‚°ãƒ“ãƒ«ãƒ‰**ã§weightå€¤ã‚’ç›´æ¥ãƒ€ãƒ³ãƒ—ã—ã¦æ¯”è¼ƒ

