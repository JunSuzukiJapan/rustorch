# RusTorch ğŸš€

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-201%20passing-brightgreen.svg)](#testing)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](#testing)
[![GPU](https://img.shields.io/badge/GPU-CUDA%2FMetal%2FOpenCL-blue.svg)](#gpu-acceleration)
[![Performance](https://img.shields.io/badge/performance-SIMD%20optimized-orange.svg)](#performance)

**A production-ready deep learning library in Rust with PyTorch-like API, GPU acceleration, and enterprise-grade performance**  
**æœ¬ç•ªç’°å¢ƒå¯¾å¿œã®Rustè£½ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - PyTorchãƒ©ã‚¤ã‚¯ãªAPIã€GPUåŠ é€Ÿã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**

RusTorch is a fully functional deep learning library that leverages Rust's safety and performance, providing comprehensive tensor operations, automatic differentiation, neural network layers, transformer architectures, multi-backend GPU acceleration (CUDA/Metal/OpenCL), advanced SIMD optimizations, and enterprise-grade memory management features.  
RusTorchã¯ã€Rustã®å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ´»ã‹ã—ãŸå®Œå…¨æ©Ÿèƒ½ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚åŒ…æ‹¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã€è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒãƒ«ãƒãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰GPUåŠ é€Ÿï¼ˆCUDA/Metal/OpenCLï¼‰ã€é«˜åº¦ãªSIMDæœ€é©åŒ–ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªç®¡ç†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## âœ¨ Features / ä¸»ãªç‰¹å¾´

- ğŸ”¥ **Comprehensive Tensor Operations**: Math operations, broadcasting, indexing, and statistics  
  **åŒ…æ‹¬çš„ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: æ•°å­¦æ¼”ç®—ã€ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã€çµ±è¨ˆæ©Ÿèƒ½
- ğŸ¤– **Transformer Architecture**: Complete transformer implementation with multi-head attention  
  **Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä»˜ãå®Œå…¨ãªTransformerå®Ÿè£…
- ğŸ“ **Embedding Systems**: Word embeddings, positional encoding, sinusoidal encoding  
  **åŸ‹ã‚è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ **: å˜èªåŸ‹ã‚è¾¼ã¿ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ğŸ“Š **Advanced Statistics**: Mean, variance, std, median, quantile, covariance, correlation  
  **é«˜åº¦ãªçµ±è¨ˆ**: å¹³å‡ã€åˆ†æ•£ã€æ¨™æº–åå·®ã€ä¸­å¤®å€¤ã€åˆ†ä½æ•°ã€å…±åˆ†æ•£ã€ç›¸é–¢
- ğŸ¯ **Broadcasting Support**: Automatic shape compatibility and dimension expansion  
  **ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: è‡ªå‹•å½¢çŠ¶äº’æ›æ€§ã¨æ¬¡å…ƒæ‹¡å¼µ
- ğŸ” **Flexible Indexing**: Select operations, slicing, and advanced tensor manipulation  
  **æŸ”è»Ÿãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: é¸æŠæ“ä½œã€ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã€é«˜åº¦ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œ
- ğŸ§® **Mathematical Functions**: Trigonometric, exponential, power, and activation functions  
  **æ•°å­¦é–¢æ•°**: ä¸‰è§’é–¢æ•°ã€æŒ‡æ•°é–¢æ•°ã€ã¹ãä¹—ã€æ´»æ€§åŒ–é–¢æ•°
- ğŸ§  **Automatic Differentiation**: Tape-based computational graph for gradient computation  
  **è‡ªå‹•å¾®åˆ†**: ãƒ†ãƒ¼ãƒ—ãƒ™ãƒ¼ã‚¹ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã«ã‚ˆã‚‹å‹¾é…è¨ˆç®—
- ğŸ—ï¸ **Neural Network Layers**: Linear, Conv2d, RNN/LSTM/GRU, BatchNorm, Dropout, and more  
  **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤**: Linearã€Conv2dã€RNN/LSTM/GRUã€BatchNormã€Dropoutç­‰
- âš¡ **SIMD Optimizations**: AVX2/SSE4.1 vectorized operations for high performance  
  **SIMDæœ€é©åŒ–**: é«˜æ€§èƒ½ãªAVX2/SSE4.1ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—
- ğŸ”„ **Unified Parallel Operations**: Trait-based parallel tensor operations with intelligent scheduling  
  **çµ±ä¸€ä¸¦åˆ—æ“ä½œ**: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä»˜ããƒˆãƒ¬ã‚¤ãƒˆãƒ™ãƒ¼ã‚¹ä¸¦åˆ—ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- ğŸš€ **Multi-threaded Processing**: Rayon-based parallel batch operations and reductions  
  **ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†**: Rayonãƒ™ãƒ¼ã‚¹ä¸¦åˆ—ãƒãƒƒãƒæ¼”ç®—ã¨ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³
- ğŸ® **GPU Integration**: CUDA/Metal/OpenCL support with automatic device selection  
  **GPUçµ±åˆ**: è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é¸æŠä»˜ãCUDA/Metal/OpenCLã‚µãƒãƒ¼ãƒˆ
- ğŸ’¾ **Advanced Memory Management**: Zero-copy operations, SIMD-aligned allocation, and memory pools  
  **é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†**: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã€SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‰²ã‚Šå½“ã¦ã€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
- ğŸ›¡ï¸ **Rust Safety**: Memory safety and thread safety guarantees  
  **Rustå®‰å…¨æ€§**: ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚’ä¿è¨¼
- âœ… **Production Ready**: All 201 tests passing, fully functional library  
  **æœ¬ç•ªç’°å¢ƒå¯¾å¿œ**: 201å€‹å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼ã€å®Œå…¨æ©Ÿèƒ½ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

## Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
rustorch = "0.1.7"

# For GPU acceleration (optional)
[features]
default = []
cuda = ["rustorch/cuda"]
metal = ["rustorch/metal"] 
opencl = ["rustorch/opencl"]
all-gpu = ["cuda", "metal", "opencl"]
```

## ğŸ“Š Performance / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

Latest benchmark results with SIMD and parallel optimizations:  
SIMDãƒ»ä¸¦åˆ—æœ€é©åŒ–å¾Œã®æœ€æ–°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:

| Operation / æ¼”ç®— | Execution Time / å®Ÿè¡Œæ™‚é–“ | Status / çŠ¶æ³ |
|------------------|---------------------------|---------------|
| SIMD Matrix Multiplication / SIMDè¡Œåˆ—ä¹—ç®— | 45Âµs | âœ… AVX2/SSE4.1 optimized / AVX2/SSE4.1æœ€é©åŒ– |
| Parallel Batch Operations / ä¸¦åˆ—ãƒãƒƒãƒæ¼”ç®— | 180Âµs | âœ… Unified trait system / çµ±ä¸€ãƒˆãƒ¬ã‚¤ãƒˆã‚·ã‚¹ãƒ†ãƒ  |
| Parallel Tensor Reductions / ä¸¦åˆ—ãƒ†ãƒ³ã‚½ãƒ«ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³ | 95Âµs | âœ… Multi-threaded processing / ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç† |
| GPU Parallel Operations / GPUä¸¦åˆ—æ“ä½œ | 65Âµs | âœ… CUDA/Metal/OpenCL support / CUDA/Metal/OpenCLã‚µãƒãƒ¼ãƒˆ |
| Zero-Copy Operations / ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œ | 8Âµs | âœ… Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– |
| SIMD-Aligned Allocation / SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‰²ã‚Šå½“ã¦ | 45ns | âœ… 32-byte alignment / 32ãƒã‚¤ãƒˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ |
| Transformer Forward Pass / Transformeré †ä¼æ’­ | 2.1ms | âœ… Multi-head attention / ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ |
| Embedding Lookup / åŸ‹ã‚è¾¼ã¿æ¤œç´¢ | 12Âµs | âœ… Optimized indexing / æœ€é©åŒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ |
| Memory Pool Allocation / ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«å‰²ã‚Šå½“ã¦ | 85ns | âœ… 1.56x speedup / 1.56å€é«˜é€ŸåŒ– |

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Basic Tensor Operations / åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create tensors / ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    let a = Tensor::from_vec(vec![1.0f32, 2.0, 3.0, 4.0], vec![2, 2]);
    let b = Tensor::from_vec(vec![5.0f32, 6.0, 7.0, 8.0], vec![2, 2]);
    
    // Basic operations / åŸºæœ¬æ¼”ç®—
    let c = &a + &b;  // Addition / åŠ ç®—
    let d = a.matmul(&b);  // Matrix multiplication / è¡Œåˆ—ä¹—ç®—
    
    // Mathematical functions / æ•°å­¦é–¢æ•°
    let e = a.sin();  // Sine function / ã‚µã‚¤ãƒ³é–¢æ•°
    let f = a.exp();  // Exponential function / æŒ‡æ•°é–¢æ•°
    
    println!("Shape: {:?}", c.shape());
    println!("Result: {:?}", c.as_slice());
}
```

### Advanced Tensor Operations / é«˜åº¦ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create a 3x4 matrix / 3x4è¡Œåˆ—ã‚’ä½œæˆ
    let data = Tensor::from_vec(
        vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0],
        vec![3, 4]
    );
    
    // Statistical operations / çµ±è¨ˆæ¼”ç®—
    let mean = data.mean(None);  // Overall mean / å…¨ä½“å¹³å‡
    let std_dev = data.std(Some(0), true);  // Standard deviation along axis 0 / è»¸0ã®æ¨™æº–åå·®
    let median = data.median(Some(1));  // Median along axis 1 / è»¸1ã®ä¸­å¤®å€¤
    
    // Broadcasting operations / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ¼”ç®—
    let broadcasted = data.broadcast_to(&[6, 4]).unwrap();
    
    // Indexing operations / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¼”ç®—
    let selected = data.select(0, &[0, 2]).unwrap();  // Select rows 0 and 2 / è¡Œ0ã¨2ã‚’é¸æŠ
    
    println!("Mean: {:?}", mean.as_slice());
    println!("Selected shape: {:?}", selected.shape());
}
```

### Automatic Differentiation and Neural Networks / è‡ªå‹•å¾®åˆ†ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

```rust
use rustorch::prelude::*;
use rustorch::nn::{Linear, loss::mse_loss};
use rustorch::optim::{SGD, Optimizer};

fn main() {
    // Create model / ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    let model = Linear::new(784, 10);
    let params = model.parameters();
    let mut optimizer = SGD::new(params, 0.01, None, None, None, None);
    
    // Prepare data / ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let input = Variable::new(
        Tensor::from_vec((0..784).map(|i| i as f32 * 0.01).collect(), vec![1, 784]),
        false
    );
    let target = Variable::new(
        Tensor::from_vec(vec![1.0; 10], vec![1, 10]),
        false
    );
    
    // Training loop / è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in 0..100 {
        optimizer.zero_grad();
        
        let output = model.forward(&input);
        let loss = mse_loss(&output, &target);
        
        loss.backward();
        optimizer.step();
        
        if epoch % 10 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch, loss.data().as_array()[[0]]);
        }
    }
}
```

## ğŸ—ï¸ Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
src/
â”œâ”€â”€ tensor/          # Tensor operations (ndarray-based) / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ï¼ˆndarrayåŸºç›¤ï¼‰
â”‚   â”œâ”€â”€ parallel_traits.rs  # Parallel operation traits / ä¸¦åˆ—æ“ä½œãƒˆãƒ¬ã‚¤ãƒˆ
â”‚   â”œâ”€â”€ parallel_impl.rs    # Parallel implementations / ä¸¦åˆ—å®Ÿè£…
â”‚   â”œâ”€â”€ parallel_ops.rs     # Legacy parallel ops / ãƒ¬ã‚¬ã‚·ãƒ¼ä¸¦åˆ—æ“ä½œ
â”‚   â”œâ”€â”€ gpu_parallel.rs     # GPU-integrated parallel ops / GPUçµ±åˆä¸¦åˆ—æ“ä½œ
â”‚   â”œâ”€â”€ memory_optimized.rs # Memory optimization strategies / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æˆ¦ç•¥
â”‚   â”œâ”€â”€ zero_copy.rs        # Zero-copy operations / ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œ
â”‚   â”œâ”€â”€ simd_aligned.rs     # SIMD-aligned tensors / SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ†ãƒ³ã‚½ãƒ«
â”‚   â”œâ”€â”€ math_ops.rs         # Mathematical functions / æ•°å­¦é–¢æ•°
â”‚   â”œâ”€â”€ broadcasting.rs     # Broadcasting operations / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆæ“ä½œ
â”‚   â”œâ”€â”€ indexing.rs         # Indexing and selection / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨é¸æŠ
â”‚   â””â”€â”€ statistics.rs       # Statistical operations / çµ±è¨ˆæ“ä½œ
â”œâ”€â”€ autograd/        # Automatic differentiation system / è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ nn/              # Neural network layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
â”‚   â”œâ”€â”€ linear.rs    # Linear layers / ç·šå½¢å±¤
â”‚   â”œâ”€â”€ conv2d.rs    # Convolution layers / ç•³ã¿è¾¼ã¿å±¤
â”‚   â”œâ”€â”€ rnn.rs       # RNN/LSTM/GRU
â”‚   â”œâ”€â”€ activation.rs # Activation functions / æ´»æ€§åŒ–é–¢æ•°
â”‚   â””â”€â”€ loss.rs      # Loss functions / æå¤±é–¢æ•°
â”œâ”€â”€ simd/            # SIMD optimizations / SIMDæœ€é©åŒ–
â”‚   â”œâ”€â”€ vectorized.rs # AVX2/SSE4.1 operations / AVX2/SSE4.1æ¼”ç®—
â”‚   â””â”€â”€ traits.rs     # SIMD trait system / SIMDãƒˆãƒ¬ã‚¤ãƒˆã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ memory/          # Advanced memory management / é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†
â”œâ”€â”€ gpu/             # GPU acceleration support / GPUåŠ é€Ÿã‚µãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ device.rs    # Device management / ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”‚   â”œâ”€â”€ memory.rs    # GPU memory pools / GPUãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ kernels.rs   # Kernel execution / ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
â”‚   â”œâ”€â”€ cuda_kernels.rs   # CUDA implementations / CUDAå®Ÿè£…
â”‚   â”œâ”€â”€ metal_kernels.rs  # Metal implementations / Metalå®Ÿè£…
â”‚   â””â”€â”€ opencl_kernels.rs # OpenCL implementations / OpenCLå®Ÿè£…
â”œâ”€â”€ optim/           # Optimization algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
â””â”€â”€ data/            # Data loaders / ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
```

## ğŸ“š Rich Features / è±Šå¯Œãªæ©Ÿèƒ½

### Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- **Basic operations / åŸºæœ¬æ¼”ç®—**: `+`, `-`, `*`, `/`, `matmul()`
- **Mathematical functions / æ•°å­¦é–¢æ•°**: `sin()`, `cos()`, `exp()`, `log()`, `sqrt()`, `pow()`, `sigmoid()`, `tanh()`
- **Statistical operations / çµ±è¨ˆæ¼”ç®—**: `mean()`, `var()`, `std()`, `median()`, `quantile()`, `cumsum()`, `cov()`, `corrcoef()`
- **Broadcasting / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: `broadcast_to()`, `broadcast_with()`, `unsqueeze()`, `squeeze()`, `repeat()`
- **Indexing / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: `select()`, advanced slicing and tensor manipulation
- **Shape manipulation / å½¢çŠ¶æ“ä½œ**: `transpose()`, `reshape()`, `permute()`
- **Parallel operations / ä¸¦åˆ—æ“ä½œ**: Trait-based parallel processing with automatic SIMD acceleration
- **GPU operations / GPUæ“ä½œ**: CUDA/Metal/OpenCL integrated parallel operations
- **Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**: Zero-copy views, SIMD-aligned allocation, memory pools

### Neural Network Layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
- **Linear**: Fully connected layers / å…¨çµåˆå±¤
- **Conv2d**: 2D convolution layers / 2Dç•³ã¿è¾¼ã¿å±¤
- **RNN/LSTM/GRU**: Recurrent neural networks (multi-layer & bidirectional) / å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå¤šå±¤ãƒ»åŒæ–¹å‘å¯¾å¿œï¼‰
- **Transformer**: Complete transformer architecture with encoder/decoder / ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä»˜ãå®Œå…¨Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **Multi-Head Attention**: Self-attention and cross-attention mechanisms / ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹
- **Embedding**: Word embeddings, positional encoding, sinusoidal encoding / å˜èªåŸ‹ã‚è¾¼ã¿ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- **Normalization**: BatchNorm, LayerNorm, GroupNorm, RMSNorm / ãƒãƒƒãƒæ­£è¦åŒ–ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã€ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ–ã€RMSæ­£è¦åŒ–
- **Dropout**: Standard and Alpha dropout layers / æ¨™æº–ãƒ»Alphaãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤
- **Pooling**: MaxPool2d, AvgPool2d

### Activation Functions / æ´»æ€§åŒ–é–¢æ•°
`ReLU`, `Sigmoid`, `Tanh`, `Softmax`, `GELU`, `Swish`, `Mish`, `LeakyReLU`, `ELU`, `SELU`

### Loss Functions / æå¤±é–¢æ•°
`MSELoss`, `CrossEntropyLoss`, `BCELoss`, `HuberLoss`

### Optimization Algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
`SGD`, `Adam` + Learning rate schedulers / å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

## ğŸ“– Examples / ã‚µãƒ³ãƒ—ãƒ«

Comprehensive examples in the [examples/](examples/) directory:  
[examples/](examples/) ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åŒ…æ‹¬çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨æ„:

- **Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: 
  - [math_ops_demo.rs](examples/math_ops_demo.rs) - Mathematical functions demonstration
  - [broadcasting_demo.rs](examples/broadcasting_demo.rs) - Broadcasting operations
  - [indexing_demo.rs](examples/indexing_demo.rs) - Indexing and selection operations
  - [statistics_demo.rs](examples/statistics_demo.rs) - Statistical functions
- **Transformer & Attention / Transformerãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**:
  - [transformer_demo.rs](examples/transformer_demo.rs) - Complete transformer pipeline
  - [embedding_demo.rs](examples/embedding_demo.rs) - Word and positional embeddings
  - [attention_demo.rs](examples/attention_demo.rs) - Multi-head attention mechanisms
- **Performance Optimization / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**:
  - [parallel_operations_demo.rs](examples/parallel_operations_demo.rs) - Parallel tensor operations with trait-based system
  - [memory_optimization_demo.rs](examples/memory_optimization_demo.rs) - Advanced memory optimization strategies
  - [gpu_acceleration_demo.rs](examples/gpu_acceleration_demo.rs) - GPU acceleration with multi-backend support
  - [simd_demo.rs](examples/simd_demo.rs) - SIMD vectorized operations
- **Basic / åŸºæœ¬**: [tensor_demo.rs](examples/tensor_demo.rs), [autograd_demo.rs](examples/autograd_demo.rs)
- **Neural Networks / NN**: [linear_regression.rs](examples/linear_regression.rs), [neural_network_demo.rs](examples/neural_network_demo.rs)
- **Advanced / é«˜åº¦**: [rnn_demo.rs](examples/rnn_demo.rs), [advanced_features_demo.rs](examples/advanced_features_demo.rs)

### Running Examples / ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ

```bash
# Run tensor operations examples / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example math_ops_demo --release
cargo run --example broadcasting_demo --release
cargo run --example statistics_demo --release

# Run transformer examples / Transformerã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example transformer_demo --release
cargo run --example embedding_demo --release
cargo run --example attention_demo --release

# Run performance optimization examples / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example parallel_operations_demo --release
cargo run --example memory_optimization_demo --release
cargo run --example gpu_acceleration_demo --release
cargo run --example simd_demo --release

# Run neural network examples / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example linear_regression --release
cargo run --example neural_network_demo --release
cargo run --example rnn_demo --release

# Run advanced examples / é«˜åº¦ãªã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example autograd_demo --release
cargo run --example advanced_features_demo --release
```

## ğŸ§ª Testing / ãƒ†ã‚¹ãƒˆ

**All 201 tests passing** - Production-ready quality assurance  
**201å€‹å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼** - æœ¬ç•ªç’°å¢ƒå¯¾å¿œã®å“è³ªä¿è¨¼

```bash
# Run all tests / å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test

# Run with release optimizations / ãƒªãƒªãƒ¼ã‚¹æœ€é©åŒ–ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test --release

# Run specific test modules / ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
cargo test tensor
cargo test nn
cargo test autograd
```

## ğŸ“Š Benchmarks / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Comprehensive performance measurement with dedicated benchmark suites:  
å°‚ç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã§åŒ…æ‹¬çš„ãªæ€§èƒ½æ¸¬å®š:

```bash
# Run all benchmarks / å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
cargo bench

# Run specific benchmark suites / ç‰¹å®šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
cargo bench --bench parallel_performance      # Parallel processing benchmarks
cargo bench --bench simd_performance         # SIMD optimization benchmarks  
cargo bench --bench memory_strategy_performance  # Memory optimization benchmarks
cargo bench --bench gpu_cpu_performance      # GPU vs CPU comparison benchmarks
cargo bench --bench integrated_performance   # Integrated performance tests

# Legacy benchmarks / ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
cargo bench --bench tensor_ops
cargo bench --bench neural_networks
cargo bench --bench optimized_ops
cargo bench --bench memory_pool
cargo bench --bench memory_optimization
cargo bench --bench gpu_integration
```

**New Benchmark Suites / æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ:**
- `parallel_performance`: Parallel vs sequential operations, thread scaling, execution strategies / ä¸¦åˆ—vsé€æ¬¡æ¼”ç®—ã€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€å®Ÿè¡Œæˆ¦ç•¥
- `simd_performance`: SIMD vs scalar operations, vectorization effectiveness, instruction sets / SIMDvsã‚¹ã‚«ãƒ©ãƒ¼æ¼”ç®—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–åŠ¹æœã€å‘½ä»¤ã‚»ãƒƒãƒˆ
- `memory_strategy_performance`: Memory allocation strategies, zero-copy operations, cache optimization / ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦æˆ¦ç•¥ã€ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
- `gpu_cpu_performance`: GPU acceleration vs CPU processing, device selection, memory transfer / GPUåŠ é€ŸvsCPUå‡¦ç†ã€ãƒ‡ãƒã‚¤ã‚¹é¸æŠã€ãƒ¡ãƒ¢ãƒªè»¢é€
- `integrated_performance`: End-to-end performance validation across all optimizations / å…¨æœ€é©åŒ–ã®çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼

**Legacy Benchmark Categories / ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒª:**
- `tensor_ops`: Basic tensor operations / åŸºæœ¬ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- `autograd_ops`: Automatic differentiation operations / è‡ªå‹•å¾®åˆ†æ¼”ç®—
- `neural_networks`: Neural network operations / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- `optimized_ops`: SIMD and parallel optimizations / SIMDãƒ»ä¸¦åˆ—æœ€é©åŒ–
- `memory_pool`: Memory management performance / ãƒ¡ãƒ¢ãƒªç®¡ç†æ€§èƒ½
- `memory_optimization`: Advanced memory strategies / é«˜åº¦ãƒ¡ãƒ¢ãƒªæˆ¦ç•¥
- `gpu_integration`: GPU acceleration benchmarks / GPUåŠ é€Ÿãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

## ğŸ“– Documentation / ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

For detailed API documentation, please refer to [docs.rs/rustorch](https://docs.rs/rustorch).  
è©³ç´°ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ [docs.rs/rustorch](https://docs.rs/rustorch) ã‚’ã”è¦§ãã ã•ã„ã€‚

## License

Licensed under either of:

 * Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
 * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.
