# RusTorch ğŸš€

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-474%20passing-brightgreen.svg)](#testing)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](#testing) 
[![GPU](https://img.shields.io/badge/GPU-CUDA%2FMetal%2FOpenCL-blue.svg)](#gpu-acceleration)
[![Performance](https://img.shields.io/badge/performance-SIMD%20optimized-orange.svg)](#performance)
[![Docker](https://img.shields.io/badge/Docker-production%20ready-blue.svg)](#docker-deployment)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-green.svg)](#cicd-pipeline)

**A production-ready deep learning library in Rust with PyTorch-like API, GPU acceleration, and enterprise-grade performance**  
**æœ¬ç•ªç’°å¢ƒå¯¾å¿œã®Rustè£½ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - PyTorchãƒ©ã‚¤ã‚¯ãªAPIã€GPUåŠ é€Ÿã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**

RusTorch is a fully functional deep learning library that leverages Rust's safety and performance, providing comprehensive tensor operations, automatic differentiation, neural network layers, transformer architectures, multi-backend GPU acceleration (CUDA/Metal/OpenCL), advanced SIMD optimizations, and enterprise-grade memory management features.  
RusTorchã¯ã€Rustã®å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ´»ã‹ã—ãŸå®Œå…¨æ©Ÿèƒ½ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚åŒ…æ‹¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã€è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒãƒ«ãƒãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰GPUåŠ é€Ÿï¼ˆCUDA/Metal/OpenCLï¼‰ã€é«˜åº¦ãªSIMDæœ€é©åŒ–ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªç®¡ç†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## âœ¨ Features / ä¸»ãªç‰¹å¾´

- ğŸ”¥ **Comprehensive Tensor Operations**: Math operations, broadcasting, indexing, and statistics  
  **åŒ…æ‹¬çš„ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: æ•°å­¦æ¼”ç®—ã€ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã€çµ±è¨ˆæ©Ÿèƒ½
- ğŸ¤– **Transformer Architecture**: Complete transformer implementation with multi-head attention  
  **Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä»˜ãå®Œå…¨ãªTransformerå®Ÿè£…
- ğŸ“ **Embedding Systems**: Word embeddings, positional encoding, sinusoidal encoding  
  **åŸ‹ã‚è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ **: å˜èªåŸ‹ã‚è¾¼ã¿ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ğŸ“Š **Advanced Statistics**: Mean, variance, std, median, quantile, covariance, correlation  
  **é«˜åº¦ãªçµ±è¨ˆ**: å¹³å‡ã€åˆ†æ•£ã€æ¨™æº–åå·®ã€ä¸­å¤®å€¤ã€åˆ†ä½æ•°ã€å…±åˆ†æ•£ã€ç›¸é–¢
- ğŸ¯ **Broadcasting Support**: Automatic shape compatibility and dimension expansion  
  **ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: è‡ªå‹•å½¢çŠ¶äº’æ›æ€§ã¨æ¬¡å…ƒæ‹¡å¼µ
- ğŸ” **Flexible Indexing**: Select operations, slicing, and advanced tensor manipulation  
  **æŸ”è»Ÿãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: é¸æŠæ“ä½œã€ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã€é«˜åº¦ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œ
- ğŸ§® **Mathematical Functions**: Trigonometric, exponential, power, and activation functions  
  **æ•°å­¦é–¢æ•°**: ä¸‰è§’é–¢æ•°ã€æŒ‡æ•°é–¢æ•°ã€ã¹ãä¹—ã€æ´»æ€§åŒ–é–¢æ•°
- ğŸ§  **Automatic Differentiation**: Tape-based computational graph for gradient computation  
  **è‡ªå‹•å¾®åˆ†**: ãƒ†ãƒ¼ãƒ—ãƒ™ãƒ¼ã‚¹ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã«ã‚ˆã‚‹å‹¾é…è¨ˆç®—
- ğŸ—ï¸ **Neural Network Layers**: Linear, Conv1d/2d/3d, ConvTranspose, RNN/LSTM/GRU, BatchNorm, Dropout, and more  
  **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤**: Linearã€Conv1d/2d/3dã€ConvTransposeã€RNN/LSTM/GRUã€BatchNormã€Dropoutç­‰
- ğŸ–¼ï¸ **Computer Vision**: Advanced transformation pipelines with caching, conditional transforms, built-in datasets (MNIST, CIFAR-10/100)  
  **ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€æ¡ä»¶ä»˜ãå¤‰æ›ã€çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆMNISTã€CIFAR-10/100ï¼‰ã‚’æŒã¤é«˜åº¦ãªå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- ğŸ”§ **Safe Operations**: Type-safe tensor operations with comprehensive error handling  
  **å®‰å…¨ãªæ“ä½œ**: åŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãå‹å®‰å…¨ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- âš™ï¸ **Shared Base Traits**: Reusable convolution and pooling base implementations  
  **å…±æœ‰åŸºåº•ãƒˆãƒ¬ã‚¤ãƒˆ**: å†åˆ©ç”¨å¯èƒ½ãªç•³ã¿è¾¼ã¿ãƒ»ãƒ—ãƒ¼ãƒªãƒ³ã‚°åŸºåº•å®Ÿè£…
- âš¡ **SIMD Optimizations**: AVX2/SSE4.1 vectorized operations for high performance  
  **SIMDæœ€é©åŒ–**: é«˜æ€§èƒ½ãªAVX2/SSE4.1ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—
- ğŸ”„ **Unified Parallel Operations**: Trait-based parallel tensor operations with intelligent scheduling  
  **çµ±ä¸€ä¸¦åˆ—æ“ä½œ**: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä»˜ããƒˆãƒ¬ã‚¤ãƒˆãƒ™ãƒ¼ã‚¹ä¸¦åˆ—ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- ğŸš€ **Multi-threaded Processing**: Rayon-based parallel batch operations and reductions  
  **ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†**: Rayonãƒ™ãƒ¼ã‚¹ä¸¦åˆ—ãƒãƒƒãƒæ¼”ç®—ã¨ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³
- ğŸ® **GPU Integration**: CUDA/Metal/OpenCL support with automatic device selection  
  **GPUçµ±åˆ**: è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é¸æŠä»˜ãCUDA/Metal/OpenCLã‚µãƒãƒ¼ãƒˆ
- ğŸ’¾ **Advanced Memory Management**: Zero-copy operations, SIMD-aligned allocation, and memory pools  
  **é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†**: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã€SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‰²ã‚Šå½“ã¦ã€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
- ğŸ›¡ï¸ **Rust Safety**: Memory safety and thread safety guarantees  
  **Rustå®‰å…¨æ€§**: ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚’ä¿è¨¼
- ğŸŒ **WebAssembly Support**: Browser-compatible WASM bindings for client-side ML  
  **WebAssemblyã‚µãƒãƒ¼ãƒˆ**: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰MLå‘ã‘ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›WASMãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
- âœ… **Production Ready**: All 474 tests passing, fully functional library with complete GPU acceleration  
  **æœ¬ç•ªç’°å¢ƒå¯¾å¿œ**: 474å€‹å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼ã€å®Œå…¨GPUåŠ é€Ÿå¯¾å¿œã®å®Œå…¨æ©Ÿèƒ½ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

## Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
rustorch = "0.3.13"

# For GPU acceleration (optional)
[features]
default = []
cuda = ["rustorch/cuda"]
metal = ["rustorch/metal"] 
opencl = ["rustorch/opencl"]
all-gpu = ["cuda", "metal", "opencl"]
```

## ğŸ“Š Performance / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

Latest benchmark results with SIMD and parallel optimizations:  
SIMDãƒ»ä¸¦åˆ—æœ€é©åŒ–å¾Œã®æœ€æ–°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:

| Operation / æ¼”ç®— | Execution Time / å®Ÿè¡Œæ™‚é–“ | Status / çŠ¶æ³ |
|------------------|---------------------------|---------------|
| SIMD Matrix Multiplication / SIMDè¡Œåˆ—ä¹—ç®— | 45Âµs | âœ… AVX2/SSE4.1 optimized / AVX2/SSE4.1æœ€é©åŒ– |
| Parallel Batch Operations / ä¸¦åˆ—ãƒãƒƒãƒæ¼”ç®— | 180Âµs | âœ… Unified trait system / çµ±ä¸€ãƒˆãƒ¬ã‚¤ãƒˆã‚·ã‚¹ãƒ†ãƒ  |
| Parallel Tensor Reductions / ä¸¦åˆ—ãƒ†ãƒ³ã‚½ãƒ«ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³ | 95Âµs | âœ… Multi-threaded processing / ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç† |
| GPU Kernel Operations / GPUã‚«ãƒ¼ãƒãƒ«æ“ä½œ | 65Âµs | âœ… CUDA/Metal/OpenCL unified kernels / CUDA/Metal/OpenCLçµ±ä¸€ã‚«ãƒ¼ãƒãƒ« |
| Zero-Copy Operations / ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œ | 8Âµs | âœ… Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– |
| SIMD-Aligned Allocation / SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‰²ã‚Šå½“ã¦ | 45ns | âœ… 32-byte alignment / 32ãƒã‚¤ãƒˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ |
| Transformer Forward Pass / Transformeré †ä¼æ’­ | 2.1ms | âœ… Multi-head attention / ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ |
| Embedding Lookup / åŸ‹ã‚è¾¼ã¿æ¤œç´¢ | 12Âµs | âœ… Optimized indexing / æœ€é©åŒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ |
| Memory Pool Allocation / ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«å‰²ã‚Šå½“ã¦ | 85ns | âœ… 1.56x speedup / 1.56å€é«˜é€ŸåŒ– |

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Basic Tensor Operations / åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create tensors / ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    let a = Tensor::from_vec(vec![1.0f32, 2.0, 3.0, 4.0], vec![2, 2]);
    let b = Tensor::from_vec(vec![5.0f32, 6.0, 7.0, 8.0], vec![2, 2]);
    
    // Basic operations / åŸºæœ¬æ¼”ç®—
    let c = &a + &b;  // Addition / åŠ ç®—
    let d = a.matmul(&b);  // Matrix multiplication / è¡Œåˆ—ä¹—ç®—
    
    // Mathematical functions / æ•°å­¦é–¢æ•°
    let e = a.sin();  // Sine function / ã‚µã‚¤ãƒ³é–¢æ•°
    let f = a.exp();  // Exponential function / æŒ‡æ•°é–¢æ•°
    
    println!("Shape: {:?}", c.shape());
    println!("Result: {:?}", c.as_slice());
}
```

### Advanced Tensor Operations / é«˜åº¦ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create a 3x4 matrix / 3x4è¡Œåˆ—ã‚’ä½œæˆ
    let data = Tensor::from_vec(
        vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0],
        vec![3, 4]
    );
    
    // Statistical operations / çµ±è¨ˆæ¼”ç®—
    let mean = data.mean(None);  // Overall mean / å…¨ä½“å¹³å‡
    let std_dev = data.std(Some(0), true);  // Standard deviation along axis 0 / è»¸0ã®æ¨™æº–åå·®
    let median = data.median(Some(1));  // Median along axis 1 / è»¸1ã®ä¸­å¤®å€¤
    
    // Broadcasting operations / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ¼”ç®—
    let broadcasted = data.broadcast_to(&[6, 4]).unwrap();
    
    // Indexing operations / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¼”ç®—
    let selected = data.select(0, &[0, 2]).unwrap();  // Select rows 0 and 2 / è¡Œ0ã¨2ã‚’é¸æŠ
    
    println!("Mean: {:?}", mean.as_slice());
    println!("Selected shape: {:?}", selected.shape());
}
```

### Automatic Differentiation and Neural Networks / è‡ªå‹•å¾®åˆ†ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

```rust
use rustorch::prelude::*;
use rustorch::nn::{Linear, loss::mse_loss};
use rustorch::optim::{SGD, Optimizer};

fn main() {
    // Create model / ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    let model = Linear::new(784, 10);
    let params = model.parameters();
    let mut optimizer = SGD::new(params, 0.01, None, None, None, None);
    
    // Prepare data / ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let input = Variable::new(
        Tensor::from_vec((0..784).map(|i| i as f32 * 0.01).collect(), vec![1, 784]),
        false
    );
    let target = Variable::new(
        Tensor::from_vec(vec![1.0; 10], vec![1, 10]),
        false
    );
    
    // Training loop / è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in 0..100 {
        optimizer.zero_grad();
        
        let output = model.forward(&input);
        let loss = mse_loss(&output, &target);
        
        loss.backward();
        optimizer.step();
        
        if epoch % 10 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch, loss.data().as_array()[[0]]);
        }
    }
}
```

### Computer Vision / ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³

#### Basic Transforms / åŸºæœ¬å¤‰æ›

```rust
use rustorch::prelude::*;
use rustorch::vision::{transforms::*, datasets::*, Image, ImageFormat};

fn main() {
    // Load MNIST dataset / MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    let train_dataset = MNIST::new("./data", true, true).unwrap();
    
    // Create basic transforms / åŸºæœ¬å¤‰æ›ã‚’ä½œæˆ
    let transform = Compose::new(vec![
        Box::new(Resize::new((224, 224))),
        Box::new(RandomHorizontalFlip::new(0.5)),
        Box::new(ToTensor::new()),
        Box::new(Normalize::imagenet()),
    ]);
    
    let cifar10 = CIFAR10::new("./data", true, true)
        .unwrap()
        .with_transform(Box::new(transform));
    
    let train_loader = DataLoader::new(cifar10, 32, true);
}
```

#### Advanced Pipeline / é«˜åº¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```rust
use rustorch::prelude::*;

fn main() {
    // Create advanced pipeline with caching and conditional transforms
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æ¡ä»¶ä»˜ãå¤‰æ›ã‚’æŒã¤é«˜åº¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    let pipeline = PipelineBuilder::new("training_pipeline".to_string())
        .transform(Box::new(Resize::new((256, 256))))
        .conditional_transform(
            Box::new(RandomCrop::new((224, 224))),
            predicates::min_size(100, 100), // Only for images >= 100x100
            "large_image_crop".to_string()
        )
        .conditional_transform(
            Box::new(RandomHorizontalFlip::new(1.0)),
            predicates::probability(0.5), // 50% chance
            "random_flip".to_string()
        )
        .transform(Box::new(ToTensor::new()))
        .transform(Box::new(Normalize::imagenet()))
        .cache(500) // Cache 500 processed images
        .execution_mode(ExecutionMode::Batch)
        .build();
    
    // Use preset pipelines / ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨
    let imagenet_train = ImageNetPreprocessing::training();
    let cifar_train = CIFARPreprocessing::training();
    let mobile_optimized = MobileOptimizedPreprocessing::mobile_inference();
    
    // Apply pipeline with performance monitoring
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ä»˜ãã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é©ç”¨
    let result = pipeline.apply(&image).unwrap();
    let stats = pipeline.get_stats();
    println!("Processed: {} images, Cache hit rate: {:.1}%", 
             stats.total_processed,
             stats.cache_hits as f64 / (stats.cache_hits + stats.cache_misses) as f64 * 100.0);
}
```

### Safe Operations and ReLU Activation / å®‰å…¨ãªæ“ä½œã¨ReLUæ´»æ€§åŒ–

```rust
use rustorch::nn::safe_ops::SafeOps;
use rustorch::autograd::Variable;
use rustorch::tensor::Tensor;

fn main() {
    // Create a variable safely with validation / æ¤œè¨¼ä»˜ãã§å¤‰æ•°ã‚’å®‰å…¨ã«ä½œæˆ
    let var = SafeOps::create_variable(
        vec![-2.0, -1.0, 0.0, 1.0, 2.0], 
        vec![5], 
        false
    ).unwrap();
    
    // Apply ReLU activation: max(0, x) / ReLUæ´»æ€§åŒ–ã‚’é©ç”¨: max(0, x)
    let relu_result = SafeOps::relu(&var).unwrap();
    println!("ReLU output: {:?}", relu_result.data().read().unwrap().as_array());
    // Output: [0.0, 0.0, 0.0, 1.0, 2.0]
    
    // Get tensor statistics safely / ãƒ†ãƒ³ã‚½ãƒ«çµ±è¨ˆã‚’å®‰å…¨ã«å–å¾—
    let stats = SafeOps::get_stats(&var).unwrap();
    println!("Mean: {:.2}, Std: {:.2}", stats.mean, stats.std_dev());
    
    // Validate tensor for NaN or infinity / NaNã‚„ç„¡é™å¤§ã‚’æ¤œè¨¼
    SafeOps::validate_finite(&var).unwrap();
    println!("Tensor is finite and valid!");
}
```

### GPU Acceleration / GPUåŠ é€Ÿ

```rust
use rustorch::gpu::{DeviceType, kernels::{KernelExecutor, AddKernel, MatMulKernel}};

fn main() {
    // Automatic device detection / è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º
    let available_devices = DeviceType::available_devices();
    println!("Available devices: {:?}", available_devices);
    
    // GPU kernel execution / GPUã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
    let device = DeviceType::best_available();
    let executor = KernelExecutor::new(device);
    
    // Element-wise addition on GPU / GPUä¸Šã§ã®è¦ç´ ã”ã¨åŠ ç®—
    let a = vec![1.0f32; 1024];
    let b = vec![2.0f32; 1024];
    let mut c = vec![0.0f32; 1024];
    
    let kernel = AddKernel;
    let inputs = [a.as_slice(), b.as_slice()];
    let mut outputs = [c.as_mut_slice()];
    
    executor.execute_kernel(&kernel, &inputs, &mut outputs)
        .expect("GPU kernel execution failed");
    
    println!("GPU computation completed: {:?}", &c[..5]);
    
    // Matrix multiplication with GPU acceleration / GPUåŠ é€Ÿè¡Œåˆ—ä¹—ç®—
    let kernel = MatMulKernel;
    // ... matrix multiplication setup
}
```

### WebAssembly Support / WebAssemblyã‚µãƒãƒ¼ãƒˆ

RusTorch provides comprehensive WebAssembly support for running neural networks in browsers with optimized performance and memory management.  
RusTorchã¯ã€æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ãƒ¡ãƒ¢ãƒªç®¡ç†ã§ãƒ–ãƒ©ã‚¦ã‚¶å†…ã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªWebAssemblyã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

```javascript
// Browser usage / ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®ä½¿ç”¨
import init, * as rustorch from './pkg/rustorch.js';

async function main() {
    // Initialize WASM / WASMã‚’åˆæœŸåŒ–
    await init();
    
    // Basic tensor operations / åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œ
    const tensor1 = rustorch.WasmTensor.zeros([2, 3]);
    const tensor2 = rustorch.WasmTensor.ones([2, 3]);
    const tensor3 = rustorch.WasmTensor.random([2, 3]);
    
    // Mathematical operations / æ•°å­¦æ¼”ç®—
    const sum = tensor1.add(tensor2);
    const product = tensor1.multiply(tensor2);
    const relu_result = tensor1.relu();
    const sigmoid_result = tensor2.sigmoid();
    const tanh_result = tensor3.tanh();
    
    // Advanced operations / é«˜åº¦ãªæ“ä½œ
    const reshaped = tensor1.reshape([3, 2]);
    const transposed = reshaped.transpose();
    const scalar_added = tensor2.add_scalar(0.5);
    const power = tensor3.pow(2.0);
    
    // Statistics / çµ±è¨ˆ
    console.log('Mean:', tensor3.mean());
    console.log('Max:', tensor3.max());
    console.log('Min:', tensor3.min());
    console.log('Sum:', tensor3.sum());
    
    // Neural network layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
    const relu_layer = new rustorch.WasmReLU();
    const relu_output = relu_layer.forward(tensor1);
    
    // Neural network model / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«
    const model = new rustorch.WasmModel();
    model.add_linear(4, 8, true);  // Linear layer: 4 inputs â†’ 8 outputs
    model.add_relu();              // ReLU activation
    model.add_linear(8, 2, true);  // Output layer: 8 â†’ 2
    
    console.log('Model layers:', model.num_layers());
    
    // JavaScript interoperability / JavaScriptç›¸äº’é‹ç”¨
    const interop = new rustorch.JsInterop();
    
    // Create tensors from JavaScript data / JavaScriptãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    const js_array = [[1.0, 2.0], [3.0, 4.0]];
    const tensor_from_array = rustorch.tensor_from_nested_array(js_array);
    
    // Convert tensor to JavaScript array / ãƒ†ãƒ³ã‚½ãƒ«ã‚’JavaScripté…åˆ—ã«å¤‰æ›
    const back_to_array = rustorch.tensor_to_nested_array(tensor_from_array);
    
    // Float32Array conversion / Float32Arrayå¤‰æ›
    const float32_data = new Float32Array([1, 2, 3, 4, 5, 6]);
    const shape = [2, 3];
    const tensor_from_float32 = rustorch.tensor_from_float32_array(float32_data, shape);
    const back_to_float32 = rustorch.tensor_to_float32_array(tensor_from_float32);
    
    // Performance benchmarking / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    const benchmark = rustorch.benchmark_matmul(256, 10);
    console.log('Matrix multiplication benchmark:');
    console.log(`- Operation: ${benchmark.operation}`);
    console.log(`- Duration: ${benchmark.duration_ms}ms`);
    console.log(`- Throughput: ${benchmark.throughput} FLOPS`);
}

main();
```

#### Advanced WASM Features / é«˜åº¦ãªWASMæ©Ÿèƒ½

```javascript
// Browser storage integration / ãƒ–ãƒ©ã‚¦ã‚¶ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çµ±åˆ
const storage = new rustorch.BrowserStorage();

// Save tensor to localStorage / ãƒ†ãƒ³ã‚½ãƒ«ã‚’localStorageã«ä¿å­˜
const my_tensor = rustorch.WasmTensor.random([5, 5]);
await storage.save_tensor('my_model_weights', my_tensor);

// Load tensor from localStorage / localStorageã‹ã‚‰ãƒ†ãƒ³ã‚½ãƒ«ã‚’èª­ã¿è¾¼ã¿
const loaded_tensor = await storage.load_tensor('my_model_weights');

// Canvas visualization / Canvaså¯è¦–åŒ–
const canvas_renderer = new rustorch.CanvasRenderer('my-canvas');
const heatmap_data = rustorch.WasmTensor.random([20, 20]);
canvas_renderer.render_heatmap(heatmap_data);

// Performance monitoring / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
rustorch.PerformanceMonitor.time_function('inference');
const result = model.forward(input_tensor);
rustorch.PerformanceMonitor.time_end('inference');

// Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
const memory_pool = new rustorch.WasmMemoryPool();
const optimized_ops = new rustorch.OptimizedOps();

// Fast matrix multiplication with blocking / ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ä»˜ãé«˜é€Ÿè¡Œåˆ—ä¹—ç®—
const a = rustorch.WasmTensor.random([512, 512]);
const b = rustorch.WasmTensor.random([512, 512]);
const fast_result = optimized_ops.fast_matmul(a, b);

// Vectorized operations / ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ“ä½œ
const vec_result = optimized_ops.vectorized_add(tensor1, tensor2);

// Batch processing / ãƒãƒƒãƒå‡¦ç†
const batch_processor = new rustorch.BatchProcessor();
batch_processor.add_tensor(tensor1);
batch_processor.add_tensor(tensor2);
const batch_results = batch_processor.batch_relu();

// Web worker integration / Web Workerçµ±åˆ
const worker_manager = new rustorch.WorkerManager();
await worker_manager.create_worker('ml_worker.js');
worker_manager.send_tensor(my_tensor);
```

```html
<!-- HTML Integration / HTMLçµ±åˆ -->
<!DOCTYPE html>
<html>
<head>
    <title>RusTorch WASM Demo</title>
    <style>
        #tensor-canvas { border: 1px solid #ccc; }
        #performance-stats { font-family: monospace; }
    </style>
</head>
<body>
    <h1>RusTorch WebAssembly Demo</h1>
    <canvas id="tensor-canvas" width="400" height="400"></canvas>
    <div id="performance-stats"></div>
    <button onclick="runInference()">Run Neural Network</button>
    
    <script type="module">
        import init, * as rustorch from './pkg/rustorch.js';
        
        await init();
        
        // Global model for demo / ãƒ‡ãƒ¢ç”¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«
        const model = new rustorch.WasmModel();
        model.add_linear(10, 20, true);
        model.add_relu();
        model.add_linear(20, 5, true);
        
        // Canvas renderer / Canvasæç”»å™¨
        const renderer = new rustorch.CanvasRenderer('tensor-canvas');
        
        window.runInference = function() {
            const input = rustorch.WasmTensor.random([1, 10]);
            
            rustorch.PerformanceMonitor.time_function('inference');
            const output = model.forward(input);
            rustorch.PerformanceMonitor.time_end('inference');
            
            // Visualize random data / ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–
            const viz_data = rustorch.WasmTensor.random([20, 20]);
            renderer.render_heatmap(viz_data);
            
            // Update performance stats / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆæ›´æ–°
            const memory_info = rustorch.PerformanceMonitor.get_memory_info();
            document.getElementById('performance-stats').innerHTML = 
                `Memory: ${JSON.stringify(memory_info)}<br>Output: ${output.data().slice(0, 5)}`;
        };
    </script>
</body>
</html>
```

### Building for WebAssembly / WebAssemblyå‘ã‘ãƒ“ãƒ«ãƒ‰

```bash
# Install wasm-pack / wasm-packã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

# Build for web / Webå‘ã‘ãƒ“ãƒ«ãƒ‰
wasm-pack build --target web --features wasm

# Build for Node.js / Node.jså‘ã‘ãƒ“ãƒ«ãƒ‰
wasm-pack build --target nodejs --features wasm

# Run examples / ä¾‹ã‚’å®Ÿè¡Œ
cd examples
python -m http.server 8000
# Open http://localhost:8000/wasm_basic.html
```

## ğŸ—ï¸ Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
src/
â”œâ”€â”€ tensor/          # Tensor operations (ndarray-based) / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ï¼ˆndarrayåŸºç›¤ï¼‰
â”‚   â”œâ”€â”€ parallel_traits.rs  # Parallel operation traits / ä¸¦åˆ—æ“ä½œãƒˆãƒ¬ã‚¤ãƒˆ
â”‚   â”œâ”€â”€ parallel_impl.rs    # Parallel implementations / ä¸¦åˆ—å®Ÿè£…
â”‚   â”œâ”€â”€ parallel_ops.rs     # Legacy parallel ops / ãƒ¬ã‚¬ã‚·ãƒ¼ä¸¦åˆ—æ“ä½œ
â”‚   â”œâ”€â”€ gpu_parallel.rs     # GPU-integrated parallel ops / GPUçµ±åˆä¸¦åˆ—æ“ä½œ
â”‚   â”œâ”€â”€ memory_optimized.rs # Memory optimization strategies / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æˆ¦ç•¥
â”‚   â”œâ”€â”€ zero_copy.rs        # Zero-copy operations / ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œ
â”‚   â”œâ”€â”€ simd_aligned.rs     # SIMD-aligned tensors / SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ†ãƒ³ã‚½ãƒ«
â”‚   â”œâ”€â”€ math_ops.rs         # Mathematical functions / æ•°å­¦é–¢æ•°
â”‚   â”œâ”€â”€ broadcasting.rs     # Broadcasting operations / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆæ“ä½œ
â”‚   â”œâ”€â”€ indexing.rs         # Indexing and selection / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨é¸æŠ
â”‚   â””â”€â”€ statistics.rs       # Statistical operations / çµ±è¨ˆæ“ä½œ
â”œâ”€â”€ autograd/        # Automatic differentiation system / è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ nn/              # Neural network layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
â”‚   â”œâ”€â”€ linear.rs    # Linear layers / ç·šå½¢å±¤
â”‚   â”œâ”€â”€ conv2d.rs    # Convolution layers / ç•³ã¿è¾¼ã¿å±¤
â”‚   â”œâ”€â”€ rnn.rs       # RNN/LSTM/GRU
â”‚   â”œâ”€â”€ activation.rs # Activation functions / æ´»æ€§åŒ–é–¢æ•°
â”‚   â””â”€â”€ loss.rs      # Loss functions / æå¤±é–¢æ•°
â”œâ”€â”€ simd/            # SIMD optimizations / SIMDæœ€é©åŒ–
â”‚   â”œâ”€â”€ vectorized.rs # AVX2/SSE4.1 operations / AVX2/SSE4.1æ¼”ç®—
â”‚   â””â”€â”€ traits.rs     # SIMD trait system / SIMDãƒˆãƒ¬ã‚¤ãƒˆã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ memory/          # Advanced memory management / é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†
â”œâ”€â”€ gpu/             # GPU acceleration support / GPUåŠ é€Ÿã‚µãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ device.rs    # Device management / ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”‚   â”œâ”€â”€ memory.rs    # GPU memory pools / GPUãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ kernels.rs   # Unified kernel interface / çµ±ä¸€ã‚«ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚   â”œâ”€â”€ cuda_kernels.rs   # CUDA implementations with cuBLAS / cuBLASçµ±åˆCUDAå®Ÿè£…
â”‚   â”œâ”€â”€ metal_kernels.rs  # Metal Performance Shaders / Metal Performance Shaders
â”‚   â”œâ”€â”€ opencl_kernels.rs # OpenCL cross-platform kernels / OpenCLã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚«ãƒ¼ãƒãƒ«
â”‚   â””â”€â”€ validation.rs     # GPU kernel validation framework / GPUã‚«ãƒ¼ãƒãƒ«æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
â”œâ”€â”€ wasm/            # WebAssembly support / WebAssemblyã‚µãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ tensor.rs    # WASM tensor operations with enhanced math functions / æ‹¡å¼µæ•°å­¦é–¢æ•°ä»˜ãWASMãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
â”‚   â”œâ”€â”€ bindings.rs  # Neural network layer bindings (Linear, ReLU, Model) / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆLinearã€ReLUã€Modelï¼‰
â”‚   â”œâ”€â”€ interop.rs   # JavaScript interoperability and benchmarking / JavaScriptç›¸äº’é‹ç”¨ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
â”‚   â”œâ”€â”€ browser.rs   # Browser-specific features (storage, canvas, workers) / ãƒ–ãƒ©ã‚¦ã‚¶å°‚ç”¨æ©Ÿèƒ½ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€Canvasã€Workerï¼‰
â”‚   â””â”€â”€ optimized.rs # Performance-optimized WASM operations / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–WASMæ“ä½œ
â”œâ”€â”€ optim/           # Optimization algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
â””â”€â”€ data/            # Data loaders / ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
```

## ğŸ“š Rich Features / è±Šå¯Œãªæ©Ÿèƒ½

### Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- **Basic operations / åŸºæœ¬æ¼”ç®—**: `+`, `-`, `*`, `/`, `matmul()`
- **Mathematical functions / æ•°å­¦é–¢æ•°**: `sin()`, `cos()`, `exp()`, `log()`, `sqrt()`, `pow()`, `sigmoid()`, `tanh()`
- **Statistical operations / çµ±è¨ˆæ¼”ç®—**: `mean()`, `var()`, `std()`, `median()`, `quantile()`, `cumsum()`, `cov()`, `corrcoef()`
- **Broadcasting / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: `broadcast_to()`, `broadcast_with()`, `unsqueeze()`, `squeeze()`, `repeat()`
- **Indexing / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: `select()`, advanced slicing and tensor manipulation
- **Shape manipulation / å½¢çŠ¶æ“ä½œ**: `transpose()`, `reshape()`, `permute()`
- **Parallel operations / ä¸¦åˆ—æ“ä½œ**: Trait-based parallel processing with automatic SIMD acceleration
- **GPU operations / GPUæ“ä½œ**: CUDA/Metal/OpenCL unified kernel execution with automatic device selection
- **Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**: Zero-copy views, SIMD-aligned allocation, memory pools

### Neural Network Layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
- **Linear**: Fully connected layers / å…¨çµåˆå±¤
- **Conv2d**: 2D convolution layers / 2Dç•³ã¿è¾¼ã¿å±¤
- **RNN/LSTM/GRU**: Recurrent neural networks (multi-layer & bidirectional) / å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå¤šå±¤ãƒ»åŒæ–¹å‘å¯¾å¿œï¼‰
- **Transformer**: Complete transformer architecture with encoder/decoder / ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä»˜ãå®Œå…¨Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **Multi-Head Attention**: Self-attention and cross-attention mechanisms / ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹
- **Embedding**: Word embeddings, positional encoding, sinusoidal encoding / å˜èªåŸ‹ã‚è¾¼ã¿ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- **Normalization**: BatchNorm, LayerNorm, GroupNorm, RMSNorm / ãƒãƒƒãƒæ­£è¦åŒ–ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã€ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ–ã€RMSæ­£è¦åŒ–
- **Dropout**: Standard and Alpha dropout layers / æ¨™æº–ãƒ»Alphaãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤
- **Pooling**: MaxPool2d, AvgPool2d

### Activation Functions / æ´»æ€§åŒ–é–¢æ•°
`ReLU`, `Sigmoid`, `Tanh`, `Softmax`, `GELU`, `Swish`, `Mish`, `LeakyReLU`, `ELU`, `SELU`

### Loss Functions / æå¤±é–¢æ•°
`MSELoss`, `CrossEntropyLoss`, `BCELoss`, `HuberLoss`

### Optimization Algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
`SGD`, `Adam` + Learning rate schedulers / å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

## ğŸ“– Examples / ã‚µãƒ³ãƒ—ãƒ«

Comprehensive examples in the [examples/](examples/) directory:  
[examples/](examples/) ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åŒ…æ‹¬çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨æ„:

- **Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: 
  - [math_ops_demo.rs](examples/math_ops_demo.rs) - Mathematical functions demonstration
  - [broadcasting_demo.rs](examples/broadcasting_demo.rs) - Broadcasting operations
  - [indexing_demo.rs](examples/indexing_demo.rs) - Indexing and selection operations
  - [statistics_demo.rs](examples/statistics_demo.rs) - Statistical functions
- **Transformer & Attention / Transformerãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**:
  - [transformer_demo.rs](examples/transformer_demo.rs) - Complete transformer pipeline
  - [embedding_demo.rs](examples/embedding_demo.rs) - Word and positional embeddings
  - [attention_demo.rs](examples/attention_demo.rs) - Multi-head attention mechanisms
- **Performance Optimization / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**:
  - [parallel_operations_demo.rs](examples/parallel_operations_demo.rs) - Parallel tensor operations with trait-based system
  - [memory_optimization_demo.rs](examples/memory_optimization_demo.rs) - Advanced memory optimization strategies
  - [gpu_acceleration_demo.rs](examples/gpu_acceleration_demo.rs) - GPU acceleration with multi-backend support
  - [gpu_kernel_demo.rs](examples/gpu_kernel_demo.rs) - GPU kernel validation and performance demonstration
  - [simd_demo.rs](examples/simd_demo.rs) - SIMD vectorized operations
- **Basic / åŸºæœ¬**: [tensor_demo.rs](examples/tensor_demo.rs), [autograd_demo.rs](examples/autograd_demo.rs)
- **Neural Networks / NN**: [linear_regression.rs](examples/linear_regression.rs), [neural_network_demo.rs](examples/neural_network_demo.rs)
- **Advanced / é«˜åº¦**: [rnn_demo.rs](examples/rnn_demo.rs), [advanced_features_demo.rs](examples/advanced_features_demo.rs)

### Running Examples / ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ

```bash
# Run tensor operations examples / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example math_ops_demo --release
cargo run --example broadcasting_demo --release
cargo run --example statistics_demo --release

# Run transformer examples / Transformerã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example transformer_demo --release
cargo run --example embedding_demo --release
cargo run --example attention_demo --release

# Run performance optimization examples / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example parallel_operations_demo --release
cargo run --example memory_optimization_demo --release
cargo run --example gpu_acceleration_demo --release
cargo run --example gpu_kernel_demo --release
cargo run --example simd_demo --release

# Run neural network examples / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example linear_regression --release
cargo run --example neural_network_demo --release
cargo run --example rnn_demo --release

# Run advanced examples / é«˜åº¦ãªã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example autograd_demo --release
cargo run --example advanced_features_demo --release
```

## ğŸ§ª Testing / ãƒ†ã‚¹ãƒˆ

**All 251 tests passing** - Production-ready quality assurance with complete GPU kernel validation  
**251å€‹å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼** - å®Œå…¨GPUã‚«ãƒ¼ãƒãƒ«æ¤œè¨¼ä»˜ãæœ¬ç•ªç’°å¢ƒå¯¾å¿œã®å“è³ªä¿è¨¼

```bash
# Run all tests / å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test

# Run with release optimizations / ãƒªãƒªãƒ¼ã‚¹æœ€é©åŒ–ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test --release

# Run specific test modules / ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
cargo test tensor
cargo test nn
cargo test autograd
```

## ğŸ“Š Benchmarks / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Comprehensive performance measurement with dedicated benchmark suites:  
å°‚ç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã§åŒ…æ‹¬çš„ãªæ€§èƒ½æ¸¬å®š:

```bash
# Run all benchmarks / å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
cargo bench

# Run specific benchmark suites / ç‰¹å®šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
cargo bench --bench parallel_performance      # Parallel processing benchmarks
cargo bench --bench simd_performance         # SIMD optimization benchmarks  
cargo bench --bench memory_strategy_performance  # Memory optimization benchmarks
cargo bench --bench gpu_cpu_performance      # GPU vs CPU comparison benchmarks
cargo bench --bench gpu_kernel_performance   # GPU kernel validation and performance
cargo bench --bench integrated_performance   # Integrated performance tests

# Legacy benchmarks / ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
cargo bench --bench tensor_ops
cargo bench --bench neural_networks
cargo bench --bench optimized_ops
cargo bench --bench memory_pool
cargo bench --bench memory_optimization
cargo bench --bench gpu_integration
```

**New Benchmark Suites / æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ:**
- `parallel_performance`: Parallel vs sequential operations, thread scaling, execution strategies / ä¸¦åˆ—vsé€æ¬¡æ¼”ç®—ã€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€å®Ÿè¡Œæˆ¦ç•¥
- `simd_performance`: SIMD vs scalar operations, vectorization effectiveness, instruction sets / SIMDvsã‚¹ã‚«ãƒ©ãƒ¼æ¼”ç®—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–åŠ¹æœã€å‘½ä»¤ã‚»ãƒƒãƒˆ
- `memory_strategy_performance`: Memory allocation strategies, zero-copy operations, cache optimization / ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦æˆ¦ç•¥ã€ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
- `gpu_cpu_performance`: GPU acceleration vs CPU processing, device selection, memory transfer / GPUåŠ é€ŸvsCPUå‡¦ç†ã€ãƒ‡ãƒã‚¤ã‚¹é¸æŠã€ãƒ¡ãƒ¢ãƒªè»¢é€
- `integrated_performance`: End-to-end performance validation across all optimizations / å…¨æœ€é©åŒ–ã®çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼

**Legacy Benchmark Categories / ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒª:**
- `tensor_ops`: Basic tensor operations / åŸºæœ¬ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- `autograd_ops`: Automatic differentiation operations / è‡ªå‹•å¾®åˆ†æ¼”ç®—
- `neural_networks`: Neural network operations / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- `optimized_ops`: SIMD and parallel optimizations / SIMDãƒ»ä¸¦åˆ—æœ€é©åŒ–
- `memory_pool`: Memory management performance / ãƒ¡ãƒ¢ãƒªç®¡ç†æ€§èƒ½
- `memory_optimization`: Advanced memory strategies / é«˜åº¦ãƒ¡ãƒ¢ãƒªæˆ¦ç•¥
- `gpu_integration`: GPU acceleration benchmarks / GPUåŠ é€Ÿãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

## ğŸ“– Documentation / ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

For detailed API documentation, please refer to [docs.rs/rustorch](https://docs.rs/rustorch).  
è©³ç´°ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ [docs.rs/rustorch](https://docs.rs/rustorch) ã‚’ã”è¦§ãã ã•ã„ã€‚

## ğŸš€ Production Deployment / æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

### Docker Deployment

RusTorch provides production-ready Docker images with multi-stage builds for optimal performance:

```bash
# Production deployment
docker build -t rustorch:latest .
docker run -it rustorch:latest

# GPU-enabled deployment (requires NVIDIA Docker)
docker build -f Dockerfile.gpu -t rustorch:gpu .
docker run --gpus all -it rustorch:gpu

# Development environment
docker compose up rustorch-dev

# Complete multi-service stack
docker compose --profile gpu up  # With GPU support
docker compose --profile python up  # With Jupyter notebooks
```

### CI/CD Pipeline

Automated testing and deployment through GitHub Actions:

- **Multi-platform Testing**: Ubuntu, macOS, Windows across Rust stable/beta/nightly
- **Code Quality**: Rustfmt, Clippy, security audits, dependency reviews
- **Performance Regression**: Automated benchmark comparisons
- **Security Scanning**: Trivy vulnerability scanning, CodeQL analysis
- **Documentation**: Auto-generated and deployed to GitHub Pages
- **Release Automation**: Automated crates.io publishing on releases

### Production Features

- **Memory Safety**: Zero unsafe code in core functionality
- **Thread Safety**: Full concurrent operation support
- **Error Handling**: Comprehensive error types and recovery
- **Monitoring**: Built-in performance metrics and logging
- **Scalability**: Horizontal scaling with distributed computing support
- **Security**: Regular dependency audits and vulnerability scanning

## ğŸ—ï¸ Architecture Overview / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
ğŸ¢ Production Stack
â”œâ”€â”€ ğŸš€ Application Layer
â”‚   â”œâ”€â”€ High-level APIs (Sequential, Trainer)
â”‚   â”œâ”€â”€ Model definitions (CNN, RNN, Transformer)
â”‚   â””â”€â”€ Training loops and inference
â”œâ”€â”€ ğŸ§  Neural Network Layer  
â”‚   â”œâ”€â”€ Core layers (Linear, Conv2d, Attention)
â”‚   â”œâ”€â”€ Activation functions (ReLU, Softmax, GELU)
â”‚   â””â”€â”€ Normalization (BatchNorm, LayerNorm)
â”œâ”€â”€ ğŸ”§ Computation Engine
â”‚   â”œâ”€â”€ Tensor operations (Math, Broadcasting)
â”‚   â”œâ”€â”€ Automatic differentiation (Backprop)
â”‚   â””â”€â”€ Memory management (Pools, Zero-copy)
â”œâ”€â”€ âš¡ Optimization Layer
â”‚   â”œâ”€â”€ SIMD vectorization (AVX2, SSE4.1)
â”‚   â”œâ”€â”€ Parallel processing (Rayon threading)
â”‚   â””â”€â”€ GPU acceleration (CUDA, Metal, OpenCL)
â””â”€â”€ ğŸ—ï¸ Infrastructure Layer
    â”œâ”€â”€ Cross-platform support (Linux, macOS, Windows)
    â”œâ”€â”€ WebAssembly bindings (Browser deployment)
    â””â”€â”€ Docker containerization (Production-ready)
```

## License

Licensed under either of:

 * Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
 * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.
