# RusTorch ğŸš€

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-519%20passing-brightgreen.svg)](#testing)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](#testing)
[![Matrix](https://img.shields.io/badge/matrix%20decomposition-SVD%2FQIR%2FLU%2FEig-blueviolet.svg)](#matrix-decomposition) 
[![GPU](https://img.shields.io/badge/GPU-CUDA%2FMetal%2FOpenCL-blue.svg)](#gpu-acceleration)
[![Performance](https://img.shields.io/badge/performance-SIMD%20optimized-orange.svg)](#performance)
[![Docker](https://img.shields.io/badge/Docker-production%20ready-blue.svg)](#docker-deployment)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-green.svg)](#cicd-pipeline)

**A production-ready deep learning library in Rust with PyTorch-like API, GPU acceleration, and enterprise-grade performance**  
**æœ¬ç•ªç’°å¢ƒå¯¾å¿œã®Rustè£½ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - PyTorchãƒ©ã‚¤ã‚¯ãªAPIã€GPUåŠ é€Ÿã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**

RusTorch is a fully functional deep learning library that leverages Rust's safety and performance, providing comprehensive tensor operations, automatic differentiation, neural network layers, transformer architectures, multi-backend GPU acceleration (CUDA/Metal/OpenCL), advanced SIMD optimizations, and enterprise-grade memory management features.  
RusTorchã¯ã€Rustã®å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ´»ã‹ã—ãŸå®Œå…¨æ©Ÿèƒ½ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚åŒ…æ‹¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã€è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒãƒ«ãƒãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰GPUåŠ é€Ÿï¼ˆCUDA/Metal/OpenCLï¼‰ã€é«˜åº¦ãªSIMDæœ€é©åŒ–ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªç®¡ç†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

## âœ¨ Features / ä¸»ãªç‰¹å¾´

- ğŸ”¥ **Comprehensive Tensor Operations**: Math operations, broadcasting, indexing, and statistics  
  **åŒ…æ‹¬çš„ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: æ•°å­¦æ¼”ç®—ã€ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã€çµ±è¨ˆæ©Ÿèƒ½
- ğŸ¤– **Transformer Architecture**: Complete transformer implementation with multi-head attention  
  **Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä»˜ãå®Œå…¨ãªTransformerå®Ÿè£…
- ğŸ“ **Embedding Systems**: Word embeddings, positional encoding, sinusoidal encoding  
  **åŸ‹ã‚è¾¼ã¿ã‚·ã‚¹ãƒ†ãƒ **: å˜èªåŸ‹ã‚è¾¼ã¿ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ğŸ“Š **Advanced Statistics**: Mean, variance, std, median, quantile, covariance, correlation  
  **é«˜åº¦ãªçµ±è¨ˆ**: å¹³å‡ã€åˆ†æ•£ã€æ¨™æº–åå·®ã€ä¸­å¤®å€¤ã€åˆ†ä½æ•°ã€å…±åˆ†æ•£ã€ç›¸é–¢
- ğŸ¯ **Broadcasting Support**: Automatic shape compatibility and dimension expansion  
  **ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: è‡ªå‹•å½¢çŠ¶äº’æ›æ€§ã¨æ¬¡å…ƒæ‹¡å¼µ
- ğŸ” **Flexible Indexing**: Select operations, slicing, and advanced tensor manipulation  
  **æŸ”è»Ÿãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: é¸æŠæ“ä½œã€ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã€é«˜åº¦ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œ
- ğŸ§® **Mathematical Functions**: Trigonometric, exponential, power, and activation functions  
  **æ•°å­¦é–¢æ•°**: ä¸‰è§’é–¢æ•°ã€æŒ‡æ•°é–¢æ•°ã€ã¹ãä¹—ã€æ´»æ€§åŒ–é–¢æ•°
- ğŸ² **Special Mathematical Functions**: Gamma, Bessel, error functions with PyTorch compatibility  
  **ç‰¹æ®Šæ•°å­¦é–¢æ•°**: PyTorchäº’æ›ã®ã‚¬ãƒ³ãƒã€ãƒ™ãƒƒã‚»ãƒ«ã€èª¤å·®é–¢æ•°
- ğŸ“Š **Statistical Distributions**: Complete probability distributions (Normal, Gamma, Beta, etc.)  
  **çµ±è¨ˆåˆ†å¸ƒ**: å®Œå…¨ãªç¢ºç‡åˆ†å¸ƒï¼ˆæ­£è¦ã€ã‚¬ãƒ³ãƒã€ãƒ™ãƒ¼ã‚¿ç­‰ï¼‰
- ğŸ§  **Automatic Differentiation**: Tape-based computational graph for gradient computation  
  **è‡ªå‹•å¾®åˆ†**: ãƒ†ãƒ¼ãƒ—ãƒ™ãƒ¼ã‚¹ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã«ã‚ˆã‚‹å‹¾é…è¨ˆç®—
- ğŸ—ï¸ **Neural Network Layers**: Linear, Conv1d/2d/3d, ConvTranspose, RNN/LSTM/GRU, BatchNorm, Dropout, and more  
  **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤**: Linearã€Conv1d/2d/3dã€ConvTransposeã€RNN/LSTM/GRUã€BatchNormã€Dropoutç­‰
- ğŸ–¼ï¸ **Computer Vision**: Advanced transformation pipelines with caching, conditional transforms, built-in datasets (MNIST, CIFAR-10/100)  
  **ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€æ¡ä»¶ä»˜ãå¤‰æ›ã€çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆMNISTã€CIFAR-10/100ï¼‰ã‚’æŒã¤é«˜åº¦ãªå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- ğŸ”§ **Safe Operations**: Type-safe tensor operations with comprehensive error handling  
  **å®‰å…¨ãªæ“ä½œ**: åŒ…æ‹¬çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãå‹å®‰å…¨ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- âš™ï¸ **Shared Base Traits**: Reusable convolution and pooling base implementations  
  **å…±æœ‰åŸºåº•ãƒˆãƒ¬ã‚¤ãƒˆ**: å†åˆ©ç”¨å¯èƒ½ãªç•³ã¿è¾¼ã¿ãƒ»ãƒ—ãƒ¼ãƒªãƒ³ã‚°åŸºåº•å®Ÿè£…
- âš¡ **SIMD Optimizations**: AVX2/SSE4.1 vectorized operations for high performance  
  **SIMDæœ€é©åŒ–**: é«˜æ€§èƒ½ãªAVX2/SSE4.1ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—
- ğŸ”„ **Unified Parallel Operations**: Trait-based parallel tensor operations with intelligent scheduling  
  **çµ±ä¸€ä¸¦åˆ—æ“ä½œ**: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä»˜ããƒˆãƒ¬ã‚¤ãƒˆãƒ™ãƒ¼ã‚¹ä¸¦åˆ—ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- ğŸš€ **Multi-threaded Processing**: Rayon-based parallel batch operations and reductions  
  **ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†**: Rayonãƒ™ãƒ¼ã‚¹ä¸¦åˆ—ãƒãƒƒãƒæ¼”ç®—ã¨ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³
- ğŸ® **GPU Integration**: CUDA/Metal/OpenCL support with automatic device selection  
  **GPUçµ±åˆ**: è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é¸æŠä»˜ãCUDA/Metal/OpenCLã‚µãƒãƒ¼ãƒˆ
- ğŸ’¾ **Advanced Memory Management**: Zero-copy operations, SIMD-aligned allocation, and memory pools  
  **é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†**: ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã€SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‰²ã‚Šå½“ã¦ã€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
- ğŸ›¡ï¸ **Rust Safety**: Memory safety and thread safety guarantees  
  **Rustå®‰å…¨æ€§**: ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚’ä¿è¨¼
- ğŸŒ **WebAssembly Support**: Browser-compatible WASM bindings for client-side ML  
  **WebAssemblyã‚µãƒãƒ¼ãƒˆ**: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰MLå‘ã‘ãƒ–ãƒ©ã‚¦ã‚¶äº’æ›WASMãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
- ğŸ§® **Matrix Decomposition**: Complete SVD, QR, LU decomposition and eigenvalue solver with PyTorch compatibility  
  **è¡Œåˆ—åˆ†è§£**: PyTorchäº’æ›ã®å®Œå…¨ãªSVDã€QRã€LUåˆ†è§£ã¨å›ºæœ‰å€¤æ±‚è§£
- âœ… **Production Ready**: All 519 tests passing, fully functional library with broadcasting support  
  **æœ¬ç•ªç’°å¢ƒå¯¾å¿œ**: 519å€‹å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼ã€ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¯¾å¿œå®Œå…¨æ©Ÿèƒ½ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

## Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
rustorch = "0.3.20"

# For GPU acceleration (optional)
[features]
default = []
cuda = ["rustorch/cuda"]
metal = ["rustorch/metal"] 
opencl = ["rustorch/opencl"]
all-gpu = ["cuda", "metal", "opencl"]
```

## ğŸ“Š Performance / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**Latest benchmark results (å®Ÿæ¸¬å€¤):**  
**æœ€æ–°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆå®Ÿæ¸¬å€¤ï¼‰:**

### ğŸ”¥ Core Performance Metrics / ã‚³ã‚¢æ€§èƒ½æŒ‡æ¨™

| Operation / æ¼”ç®— | Performance / æ€§èƒ½ | Details / è©³ç´° |
|------------------|-------------------|---------------|
| **Tensor Addition** / ãƒ†ãƒ³ã‚½ãƒ«åŠ ç®— | 34K - 2.3M ops/sec | âœ… Broadcasting support / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¯¾å¿œ |
| **Tensor Sum** / ãƒ†ãƒ³ã‚½ãƒ«åˆè¨ˆ | 52M+ ops/sec | âœ… Consistently high performance / ä¸€è²«ã—ãŸé«˜æ€§èƒ½ |
| **Matrix Multiplication** / è¡Œåˆ—ä¹—ç®— | 0.71 - 0.77 GFLOPS | âœ… Stable scaling / å®‰å®šã—ãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| **Neural Network Inference** / NNæ¨è«– | 15 - 60 inferences/sec | âœ… Batch processing / ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ |

### ğŸ§® Matrix Decomposition Performance / è¡Œåˆ—åˆ†è§£ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**Pure Rust implementation benchmarks (100 iterations) / ç´”Rustå®Ÿè£…ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (100å›åå¾©):**

| Matrix Size / è¡Œåˆ—ã‚µã‚¤ã‚º | SVD | QR | LU | Symeig | Eig |
|--------------------------|-----|----|----|---------|-----|
| **4Ã—4** | 0.96 Î¼s | 0.56 Î¼s | 1.12 Î¼s | 0.51 Î¼s | 0.70 Î¼s |
| **8Ã—8** | 1.38 Î¼s | 1.17 Î¼s | 1.65 Î¼s | 0.47 Î¼s | 0.71 Î¼s |
| **16Ã—16** | 3.02 Î¼s | 4.98 Î¼s | 3.60 Î¼s | 0.43 Î¼s | 0.71 Î¼s |
| **32Ã—32** | 9.92 Î¼s | 33.41 Î¼s | 11.81 Î¼s | 0.54 Î¼s | 0.78 Î¼s |

**âœ… 100% Success Rate**: All matrix sizes and algorithms achieve 100/100 successful decompositions  
**âœ… 100% æˆåŠŸç‡**: å…¨ã¦ã®è¡Œåˆ—ã‚µã‚¤ã‚ºãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§100/100ã®åˆ†è§£æˆåŠŸ

### âš¡ Detailed Performance Breakdown / è©³ç´°æ€§èƒ½å†…è¨³

| Matrix Size | MatMul Performance | Batch Size | NN Inference Rate |
|-------------|-------------------|------------|------------------|
| 64Ã—64 | 0.77 GFLOPS | 32 | 59.86 inferences/sec |
| 128Ã—128 | 0.76 GFLOPS | 64 | 29.35 inferences/sec |
| 256Ã—256 | 0.76 GFLOPS | 128 | 15.09 inferences/sec |
| 512Ã—512 | 0.71 GFLOPS | - | - |

### ğŸš€ System Status / ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
- âœ… **519 Tests Passing** / 519å€‹å…¨ãƒ†ã‚¹ãƒˆé€šé
- âœ… **Zero Compilation Errors** / ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ã‚¼ãƒ­  
- âœ… **Broadcasting Support** / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¯¾å¿œ
- âœ… **Matrix Decomposition** / è¡Œåˆ—åˆ†è§£å¯¾å¿œ
- âœ… **Production Ready** / æœ¬ç•ªç’°å¢ƒå¯¾å¿œ

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Basic Tensor Operations / åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create tensors / ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    let a = Tensor::from_vec(vec![1.0f32, 2.0, 3.0, 4.0], vec![2, 2]);
    let b = Tensor::from_vec(vec![5.0f32, 6.0, 7.0, 8.0], vec![2, 2]);
    
    // Basic operations / åŸºæœ¬æ¼”ç®—
    let c = &a + &b;  // Addition / åŠ ç®—
    let d = a.matmul(&b);  // Matrix multiplication / è¡Œåˆ—ä¹—ç®—
    
    // Mathematical functions / æ•°å­¦é–¢æ•°
    let e = a.sin();  // Sine function / ã‚µã‚¤ãƒ³é–¢æ•°
    let f = a.exp();  // Exponential function / æŒ‡æ•°é–¢æ•°
    
    println!("Shape: {:?}", c.shape());
    println!("Result: {:?}", c.as_slice());
}
```

### Broadcasting Support / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå¯¾å¿œ

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Broadcasting: (batch, features) + (1, features)
    // ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ: (ãƒãƒƒãƒ, ç‰¹å¾´é‡) + (1, ç‰¹å¾´é‡)
    let batch_data = Tensor::from_vec(
        (0..64).map(|i| i as f32 * 0.01).collect(),
        vec![32, 2]  // 32 samples, 2 features
    );
    
    let bias = Tensor::from_vec(
        vec![0.1, 0.2],
        vec![1, 2]  // Broadcast shape
    );
    
    // Automatic broadcasting / è‡ªå‹•ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
    let result = batch_data.add(&bias).unwrap();
    println!("Result shape: {:?}", result.shape()); // [32, 2]
    
    // Neural network bias addition / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒã‚¤ã‚¢ã‚¹åŠ ç®—
    use rustorch::nn::{Linear, Module};
    use rustorch::autograd::Variable;
    
    let linear = Linear::<f32>::new(256, 128);
    let input = Variable::new(
        Tensor::from_vec((0..32*256).map(|i| i as f32 * 0.01).collect(), vec![32, 256]),
        false
    );
    
    let output = linear.forward(&input); // Automatic bias broadcasting
    println!("Linear output: {:?}", output.data().read().unwrap().shape());
}
```

### Advanced Tensor Operations / é«˜åº¦ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create a 3x4 matrix / 3x4è¡Œåˆ—ã‚’ä½œæˆ
    let data = Tensor::from_vec(
        vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0],
        vec![3, 4]
    );
    
    // Statistical operations / çµ±è¨ˆæ¼”ç®—
    let mean = data.mean(None);  // Overall mean / å…¨ä½“å¹³å‡
    let std_dev = data.std(Some(0), true);  // Standard deviation along axis 0 / è»¸0ã®æ¨™æº–åå·®
    let median = data.median(Some(1));  // Median along axis 1 / è»¸1ã®ä¸­å¤®å€¤
    
    // Broadcasting operations / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ¼”ç®—
    let broadcasted = data.broadcast_to(&[6, 4]).unwrap();
    
    // Indexing operations / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¼”ç®—
    let selected = data.select(0, &[0, 2]).unwrap();  // Select rows 0 and 2 / è¡Œ0ã¨2ã‚’é¸æŠ
    
    println!("Mean: {:?}", mean.as_slice());
    println!("Selected shape: {:?}", selected.shape());
}
```

## ğŸ§® Matrix Decomposition / è¡Œåˆ—åˆ†è§£

### SVD, QR, LU Decomposition and Eigenvalue Decomposition / SVDã€QRã€LUåˆ†è§£ã¨å›ºæœ‰å€¤åˆ†è§£

```rust
use rustorch::tensor::Tensor;

fn main() {
    // Create a 3x3 matrix / 3x3è¡Œåˆ—ã‚’ä½œæˆ
    let matrix = Tensor::from_vec(
        vec![1.0f32, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0],
        vec![3, 3]
    );
    
    // Singular Value Decomposition (torch.svd compatible) / ç‰¹ç•°å€¤åˆ†è§£ï¼ˆtorch.svdäº’æ›ï¼‰
    let (u, s, v) = matrix.svd(false).unwrap();
    println!("SVD - U: {:?}, S: {:?}, V: {:?}", u.shape(), s.shape(), v.shape());
    
    // QR decomposition / QRåˆ†è§£
    let (q, r) = matrix.qr().unwrap();
    println!("QR - Q: {:?}, R: {:?}", q.shape(), r.shape());
    
    // LU decomposition with partial pivoting / éƒ¨åˆ†ãƒ”ãƒœãƒƒãƒˆé¸æŠä»˜ãLUåˆ†è§£
    let (l, u, p) = matrix.lu().unwrap();
    println!("LU - L: {:?}, U: {:?}, P: {:?}", l.shape(), u.shape(), p.shape());
    
    // Create symmetric matrix for eigenvalue decomposition / å›ºæœ‰å€¤åˆ†è§£ç”¨å¯¾ç§°è¡Œåˆ—ä½œæˆ
    let sym_data = vec![4.0f32, 2.0, 1.0, 2.0, 3.0, 0.5, 1.0, 0.5, 1.0];
    let sym_matrix = Tensor::from_vec(sym_data, vec![3, 3]);
    
    // Symmetric eigenvalue decomposition (torch.symeig compatible) / å¯¾ç§°å›ºæœ‰å€¤åˆ†è§£ï¼ˆtorch.symeigäº’æ›ï¼‰
    let (eigenvals, eigenvecs) = sym_matrix.symeig(true, true).unwrap();
    println!("Symeig - Values: {:?}, Vectors: {:?}", eigenvals.shape(), eigenvecs.shape());
    
    // General eigenvalue decomposition (torch.eig compatible) / ä¸€èˆ¬å›ºæœ‰å€¤åˆ†è§£ï¼ˆtorch.eigäº’æ›ï¼‰
    let (eig_vals, eig_vecs) = matrix.eig(true).unwrap();
    println!("Eig - Values: {:?}, Vectors: {:?}", eig_vals.shape(), eig_vecs.unwrap().shape());
    
    // Performance: All operations complete in microseconds / æ€§èƒ½: å…¨æ¼”ç®—ãŒãƒã‚¤ã‚¯ãƒ­ç§’ã§å®Œäº†
    // 4x4 matrices: SVD ~0.96Î¼s, QR ~0.56Î¼s, LU ~1.12Î¼s, Symeig ~0.51Î¼s
    // 32x32 matrices: SVD ~9.92Î¼s, QR ~33.41Î¼s, LU ~11.81Î¼s, Symeig ~0.54Î¼s
}

### Automatic Differentiation and Neural Networks / è‡ªå‹•å¾®åˆ†ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

```rust
use rustorch::prelude::*;
use rustorch::nn::{Linear, loss::mse_loss};
use rustorch::optim::{SGD, Optimizer};

fn main() {
    // Create model / ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    let model = Linear::new(784, 10);
    let params = model.parameters();
    let mut optimizer = SGD::new(params, 0.01, None, None, None, None);
    
    // Prepare data / ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let input = Variable::new(
        Tensor::from_vec((0..784).map(|i| i as f32 * 0.01).collect(), vec![1, 784]),
        false
    );
    let target = Variable::new(
        Tensor::from_vec(vec![1.0; 10], vec![1, 10]),
        false
    );
    
    // Training loop / è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in 0..100 {
        optimizer.zero_grad();
        
        let output = model.forward(&input);
        let loss = mse_loss(&output, &target);
        
        loss.backward();
        optimizer.step();
        
        if epoch % 10 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch, loss.data().as_array()[[0]]);
        }
    }
}
```

### Computer Vision / ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³

#### Basic Transforms / åŸºæœ¬å¤‰æ›

```rust
use rustorch::prelude::*;
use rustorch::vision::{transforms::*, datasets::*, Image, ImageFormat};

fn main() {
    // Load MNIST dataset / MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    let train_dataset = MNIST::new("./data", true, true).unwrap();
    
    // Create basic transforms / åŸºæœ¬å¤‰æ›ã‚’ä½œæˆ
    let transform = Compose::new(vec![
        Box::new(Resize::new((224, 224))),
        Box::new(RandomHorizontalFlip::new(0.5)),
        Box::new(ToTensor::new()),
        Box::new(Normalize::imagenet()),
    ]);
    
    let cifar10 = CIFAR10::new("./data", true, true)
        .unwrap()
        .with_transform(Box::new(transform));
    
    let train_loader = DataLoader::new(cifar10, 32, true);
}
```

#### Advanced Pipeline / é«˜åº¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```rust
use rustorch::prelude::*;

fn main() {
    // Create advanced pipeline with caching and conditional transforms
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æ¡ä»¶ä»˜ãå¤‰æ›ã‚’æŒã¤é«˜åº¦ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    let pipeline = PipelineBuilder::new("training_pipeline".to_string())
        .transform(Box::new(Resize::new((256, 256))))
        .conditional_transform(
            Box::new(RandomCrop::new((224, 224))),
            predicates::min_size(100, 100), // Only for images >= 100x100
            "large_image_crop".to_string()
        )
        .conditional_transform(
            Box::new(RandomHorizontalFlip::new(1.0)),
            predicates::probability(0.5), // 50% chance
            "random_flip".to_string()
        )
        .transform(Box::new(ToTensor::new()))
        .transform(Box::new(Normalize::imagenet()))
        .cache(500) // Cache 500 processed images
        .execution_mode(ExecutionMode::Batch)
        .build();
    
    // Use preset pipelines / ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨
    let imagenet_train = ImageNetPreprocessing::training();
    let cifar_train = CIFARPreprocessing::training();
    let mobile_optimized = MobileOptimizedPreprocessing::mobile_inference();
    
    // Apply pipeline with performance monitoring
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ä»˜ãã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é©ç”¨
    let result = pipeline.apply(&image).unwrap();
    let stats = pipeline.get_stats();
    println!("Processed: {} images, Cache hit rate: {:.1}%", 
             stats.total_processed,
             stats.cache_hits as f64 / (stats.cache_hits + stats.cache_misses) as f64 * 100.0);
}
```

### Safe Operations and ReLU Activation / å®‰å…¨ãªæ“ä½œã¨ReLUæ´»æ€§åŒ–

```rust
use rustorch::nn::safe_ops::SafeOps;
use rustorch::autograd::Variable;
use rustorch::tensor::Tensor;

fn main() {
    // Create a variable safely with validation / æ¤œè¨¼ä»˜ãã§å¤‰æ•°ã‚’å®‰å…¨ã«ä½œæˆ
    let var = SafeOps::create_variable(
        vec![-2.0, -1.0, 0.0, 1.0, 2.0], 
        vec![5], 
        false
    ).unwrap();
    
    // Apply ReLU activation: max(0, x) / ReLUæ´»æ€§åŒ–ã‚’é©ç”¨: max(0, x)
    let relu_result = SafeOps::relu(&var).unwrap();
    println!("ReLU output: {:?}", relu_result.data().read().unwrap().as_array());
    // Output: [0.0, 0.0, 0.0, 1.0, 2.0]
    
    // Get tensor statistics safely / ãƒ†ãƒ³ã‚½ãƒ«çµ±è¨ˆã‚’å®‰å…¨ã«å–å¾—
    let stats = SafeOps::get_stats(&var).unwrap();
    println!("Mean: {:.2}, Std: {:.2}", stats.mean, stats.std_dev());
    
    // Validate tensor for NaN or infinity / NaNã‚„ç„¡é™å¤§ã‚’æ¤œè¨¼
    SafeOps::validate_finite(&var).unwrap();
    println!("Tensor is finite and valid!");
}
```

### GPU Acceleration / GPUåŠ é€Ÿ

```rust
use rustorch::gpu::{DeviceType, kernels::{KernelExecutor, AddKernel, MatMulKernel}};

fn main() {
    // Automatic device detection / è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º
    let available_devices = DeviceType::available_devices();
    println!("Available devices: {:?}", available_devices);
    
    // GPU kernel execution / GPUã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
    let device = DeviceType::best_available();
    let executor = KernelExecutor::new(device);
    
    // Element-wise addition on GPU / GPUä¸Šã§ã®è¦ç´ ã”ã¨åŠ ç®—
    let a = vec![1.0f32; 1024];
    let b = vec![2.0f32; 1024];
    let mut c = vec![0.0f32; 1024];
    
    let kernel = AddKernel;
    let inputs = [a.as_slice(), b.as_slice()];
    let mut outputs = [c.as_mut_slice()];
    
    executor.execute_kernel(&kernel, &inputs, &mut outputs)
        .expect("GPU kernel execution failed");
    
    println!("GPU computation completed: {:?}", &c[..5]);
    
    // Matrix multiplication with GPU acceleration / GPUåŠ é€Ÿè¡Œåˆ—ä¹—ç®—
    let kernel = MatMulKernel;
    // ... matrix multiplication setup
}
```

### WebAssembly Support / WebAssemblyã‚µãƒãƒ¼ãƒˆ

RusTorch provides comprehensive WebAssembly support for running neural networks in browsers with optimized performance and memory management.  
RusTorchã¯ã€æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ãƒ¡ãƒ¢ãƒªç®¡ç†ã§ãƒ–ãƒ©ã‚¦ã‚¶å†…ã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªWebAssemblyã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚

```javascript
// Browser usage / ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®ä½¿ç”¨
import init, * as rustorch from './pkg/rustorch.js';

async function main() {
    // Initialize WASM / WASMã‚’åˆæœŸåŒ–
    await init();
    
    // Basic tensor operations / åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œ
    const tensor1 = rustorch.WasmTensor.zeros([2, 3]);
    const tensor2 = rustorch.WasmTensor.ones([2, 3]);
    const tensor3 = rustorch.WasmTensor.random([2, 3]);
    
    // Mathematical operations / æ•°å­¦æ¼”ç®—
    const sum = tensor1.add(tensor2);
    const product = tensor1.multiply(tensor2);
    const relu_result = tensor1.relu();
    const sigmoid_result = tensor2.sigmoid();
    const tanh_result = tensor3.tanh();
    
    // Advanced operations / é«˜åº¦ãªæ“ä½œ
    const reshaped = tensor1.reshape([3, 2]);
    const transposed = reshaped.transpose();
    const scalar_added = tensor2.add_scalar(0.5);
    const power = tensor3.pow(2.0);
    
    // Statistics / çµ±è¨ˆ
    console.log('Mean:', tensor3.mean());
    console.log('Max:', tensor3.max());
    console.log('Min:', tensor3.min());
    console.log('Sum:', tensor3.sum());
    
    // Neural network layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
    const relu_layer = new rustorch.WasmReLU();
    const relu_output = relu_layer.forward(tensor1);
    
    // Neural network model / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«
    const model = new rustorch.WasmModel();
    model.add_linear(4, 8, true);  // Linear layer: 4 inputs â†’ 8 outputs
    model.add_relu();              // ReLU activation
    model.add_linear(8, 2, true);  // Output layer: 8 â†’ 2
    
    console.log('Model layers:', model.num_layers());
    
    // JavaScript interoperability / JavaScriptç›¸äº’é‹ç”¨
    const interop = new rustorch.JsInterop();
    
    // Create tensors from JavaScript data / JavaScriptãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    const js_array = [[1.0, 2.0], [3.0, 4.0]];
    const tensor_from_array = rustorch.tensor_from_nested_array(js_array);
    
    // Convert tensor to JavaScript array / ãƒ†ãƒ³ã‚½ãƒ«ã‚’JavaScripté…åˆ—ã«å¤‰æ›
    const back_to_array = rustorch.tensor_to_nested_array(tensor_from_array);
    
    // Float32Array conversion / Float32Arrayå¤‰æ›
    const float32_data = new Float32Array([1, 2, 3, 4, 5, 6]);
    const shape = [2, 3];
    const tensor_from_float32 = rustorch.tensor_from_float32_array(float32_data, shape);
    const back_to_float32 = rustorch.tensor_to_float32_array(tensor_from_float32);
    
    // Performance benchmarking / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    const benchmark = rustorch.benchmark_matmul(256, 10);
    console.log('Matrix multiplication benchmark:');
    console.log(`- Operation: ${benchmark.operation}`);
    console.log(`- Duration: ${benchmark.duration_ms}ms`);
    console.log(`- Throughput: ${benchmark.throughput} FLOPS`);
}

main();
```

#### Advanced WASM Features / é«˜åº¦ãªWASMæ©Ÿèƒ½

```javascript
// Browser storage integration / ãƒ–ãƒ©ã‚¦ã‚¶ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çµ±åˆ
const storage = new rustorch.BrowserStorage();

// Save tensor to localStorage / ãƒ†ãƒ³ã‚½ãƒ«ã‚’localStorageã«ä¿å­˜
const my_tensor = rustorch.WasmTensor.random([5, 5]);
await storage.save_tensor('my_model_weights', my_tensor);

// Load tensor from localStorage / localStorageã‹ã‚‰ãƒ†ãƒ³ã‚½ãƒ«ã‚’èª­ã¿è¾¼ã¿
const loaded_tensor = await storage.load_tensor('my_model_weights');

// Canvas visualization / Canvaså¯è¦–åŒ–
const canvas_renderer = new rustorch.CanvasRenderer('my-canvas');
const heatmap_data = rustorch.WasmTensor.random([20, 20]);
canvas_renderer.render_heatmap(heatmap_data);

// Performance monitoring / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
rustorch.PerformanceMonitor.time_function('inference');
const result = model.forward(input_tensor);
rustorch.PerformanceMonitor.time_end('inference');

// Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
const memory_pool = new rustorch.WasmMemoryPool();
const optimized_ops = new rustorch.OptimizedOps();

// Fast matrix multiplication with blocking / ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ä»˜ãé«˜é€Ÿè¡Œåˆ—ä¹—ç®—
const a = rustorch.WasmTensor.random([512, 512]);
const b = rustorch.WasmTensor.random([512, 512]);
const fast_result = optimized_ops.fast_matmul(a, b);

// Vectorized operations / ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ“ä½œ
const vec_result = optimized_ops.vectorized_add(tensor1, tensor2);

// Batch processing / ãƒãƒƒãƒå‡¦ç†
const batch_processor = new rustorch.BatchProcessor();
batch_processor.add_tensor(tensor1);
batch_processor.add_tensor(tensor2);
const batch_results = batch_processor.batch_relu();

// Web worker integration / Web Workerçµ±åˆ
const worker_manager = new rustorch.WorkerManager();
await worker_manager.create_worker('ml_worker.js');
worker_manager.send_tensor(my_tensor);
```

```html
<!-- HTML Integration / HTMLçµ±åˆ -->
<!DOCTYPE html>
<html>
<head>
    <title>RusTorch WASM Demo</title>
    <style>
        #tensor-canvas { border: 1px solid #ccc; }
        #performance-stats { font-family: monospace; }
    </style>
</head>
<body>
    <h1>RusTorch WebAssembly Demo</h1>
    <canvas id="tensor-canvas" width="400" height="400"></canvas>
    <div id="performance-stats"></div>
    <button onclick="runInference()">Run Neural Network</button>
    
    <script type="module">
        import init, * as rustorch from './pkg/rustorch.js';
        
        await init();
        
        // Global model for demo / ãƒ‡ãƒ¢ç”¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«
        const model = new rustorch.WasmModel();
        model.add_linear(10, 20, true);
        model.add_relu();
        model.add_linear(20, 5, true);
        
        // Canvas renderer / Canvasæç”»å™¨
        const renderer = new rustorch.CanvasRenderer('tensor-canvas');
        
        window.runInference = function() {
            const input = rustorch.WasmTensor.random([1, 10]);
            
            rustorch.PerformanceMonitor.time_function('inference');
            const output = model.forward(input);
            rustorch.PerformanceMonitor.time_end('inference');
            
            // Visualize random data / ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–
            const viz_data = rustorch.WasmTensor.random([20, 20]);
            renderer.render_heatmap(viz_data);
            
            // Update performance stats / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆæ›´æ–°
            const memory_info = rustorch.PerformanceMonitor.get_memory_info();
            document.getElementById('performance-stats').innerHTML = 
                `Memory: ${JSON.stringify(memory_info)}<br>Output: ${output.data().slice(0, 5)}`;
        };
    </script>
</body>
</html>
```

### Building for WebAssembly / WebAssemblyå‘ã‘ãƒ“ãƒ«ãƒ‰

```bash
# Install wasm-pack / wasm-packã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

# Build for web / Webå‘ã‘ãƒ“ãƒ«ãƒ‰
wasm-pack build --target web --features wasm

# Build for Node.js / Node.jså‘ã‘ãƒ“ãƒ«ãƒ‰
wasm-pack build --target nodejs --features wasm

# Run examples / ä¾‹ã‚’å®Ÿè¡Œ
cd examples
python -m http.server 8000
# Open http://localhost:8000/wasm_basic.html
```

## ğŸ—ï¸ Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
src/
â”œâ”€â”€ tensor/          # Tensor operations (ndarray-based) / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ï¼ˆndarrayåŸºç›¤ï¼‰
â”‚   â”œâ”€â”€ operations.rs       # Matrix decomposition (SVD, QR, LU, eigenvalue) / è¡Œåˆ—åˆ†è§£ï¼ˆSVDã€QRã€LUã€å›ºæœ‰å€¤ï¼‰
â”‚   â”œâ”€â”€ parallel_traits.rs  # Parallel operation traits / ä¸¦åˆ—æ“ä½œãƒˆãƒ¬ã‚¤ãƒˆ
â”‚   â”œâ”€â”€ parallel_impl.rs    # Parallel implementations / ä¸¦åˆ—å®Ÿè£…
â”‚   â”œâ”€â”€ parallel_ops.rs     # Legacy parallel ops / ãƒ¬ã‚¬ã‚·ãƒ¼ä¸¦åˆ—æ“ä½œ
â”‚   â”œâ”€â”€ gpu_parallel.rs     # GPU-integrated parallel ops / GPUçµ±åˆä¸¦åˆ—æ“ä½œ
â”‚   â”œâ”€â”€ memory_optimized.rs # Memory optimization strategies / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æˆ¦ç•¥
â”‚   â”œâ”€â”€ zero_copy.rs        # Zero-copy operations / ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œ
â”‚   â”œâ”€â”€ simd_aligned.rs     # SIMD-aligned tensors / SIMDã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ†ãƒ³ã‚½ãƒ«
â”‚   â”œâ”€â”€ math_ops.rs         # Mathematical functions / æ•°å­¦é–¢æ•°
â”‚   â”œâ”€â”€ broadcasting.rs     # Broadcasting operations / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆæ“ä½œ
â”‚   â”œâ”€â”€ indexing.rs         # Indexing and selection / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨é¸æŠ
â”‚   â””â”€â”€ statistics.rs       # Statistical operations / çµ±è¨ˆæ“ä½œ
â”œâ”€â”€ autograd/        # Automatic differentiation system / è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ nn/              # Neural network layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
â”‚   â”œâ”€â”€ linear.rs    # Linear layers / ç·šå½¢å±¤
â”‚   â”œâ”€â”€ conv2d.rs    # Convolution layers / ç•³ã¿è¾¼ã¿å±¤
â”‚   â”œâ”€â”€ rnn.rs       # RNN/LSTM/GRU
â”‚   â”œâ”€â”€ activation.rs # Activation functions / æ´»æ€§åŒ–é–¢æ•°
â”‚   â””â”€â”€ loss.rs      # Loss functions / æå¤±é–¢æ•°
â”œâ”€â”€ simd/            # SIMD optimizations / SIMDæœ€é©åŒ–
â”‚   â”œâ”€â”€ vectorized.rs # AVX2/SSE4.1 operations / AVX2/SSE4.1æ¼”ç®—
â”‚   â””â”€â”€ traits.rs     # SIMD trait system / SIMDãƒˆãƒ¬ã‚¤ãƒˆã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ memory/          # Advanced memory management / é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†
â”œâ”€â”€ gpu/             # GPU acceleration support / GPUåŠ é€Ÿã‚µãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ device.rs    # Device management / ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”‚   â”œâ”€â”€ memory.rs    # GPU memory pools / GPUãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ kernels.rs   # Unified kernel interface / çµ±ä¸€ã‚«ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚   â”œâ”€â”€ cuda_kernels.rs   # CUDA implementations with cuBLAS / cuBLASçµ±åˆCUDAå®Ÿè£…
â”‚   â”œâ”€â”€ metal_kernels.rs  # Metal Performance Shaders / Metal Performance Shaders
â”‚   â”œâ”€â”€ opencl_kernels.rs # OpenCL cross-platform kernels / OpenCLã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚«ãƒ¼ãƒãƒ«
â”‚   â””â”€â”€ validation.rs     # GPU kernel validation framework / GPUã‚«ãƒ¼ãƒãƒ«æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
â”œâ”€â”€ wasm/            # WebAssembly support / WebAssemblyã‚µãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ tensor.rs    # WASM tensor operations with enhanced math functions / æ‹¡å¼µæ•°å­¦é–¢æ•°ä»˜ãWASMãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
â”‚   â”œâ”€â”€ bindings.rs  # Neural network layer bindings (Linear, ReLU, Model) / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆLinearã€ReLUã€Modelï¼‰
â”‚   â”œâ”€â”€ interop.rs   # JavaScript interoperability and benchmarking / JavaScriptç›¸äº’é‹ç”¨ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
â”‚   â”œâ”€â”€ browser.rs   # Browser-specific features (storage, canvas, workers) / ãƒ–ãƒ©ã‚¦ã‚¶å°‚ç”¨æ©Ÿèƒ½ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã€Canvasã€Workerï¼‰
â”‚   â””â”€â”€ optimized.rs # Performance-optimized WASM operations / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–WASMæ“ä½œ
â”œâ”€â”€ special/         # Special mathematical functions / ç‰¹æ®Šæ•°å­¦é–¢æ•°
â”‚   â”œâ”€â”€ gamma.rs     # Gamma functions (Î“, ln Î“, Ïˆ, B) / ã‚¬ãƒ³ãƒé–¢æ•°ï¼ˆÎ“ã€ln Î“ã€Ïˆã€Bï¼‰
â”‚   â”œâ”€â”€ bessel.rs    # Bessel functions (J, Y, I, K) / ãƒ™ãƒƒã‚»ãƒ«é–¢æ•°ï¼ˆJã€Yã€Iã€Kï¼‰
â”‚   â”œâ”€â”€ error.rs     # Error functions (erf, erfc, erfinv) / èª¤å·®é–¢æ•°ï¼ˆerfã€erfcã€erfinvï¼‰
â”‚   â””â”€â”€ utils.rs     # Utility functions / ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
â”œâ”€â”€ distributions/   # Statistical distributions / çµ±è¨ˆåˆ†å¸ƒ
â”‚   â”œâ”€â”€ normal.rs    # Normal distribution / æ­£è¦åˆ†å¸ƒ
â”‚   â”œâ”€â”€ gamma.rs     # Gamma distribution / ã‚¬ãƒ³ãƒåˆ†å¸ƒ
â”‚   â”œâ”€â”€ beta.rs      # Beta distribution / ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ
â”‚   â””â”€â”€ ...         # Other distributions / ãã®ä»–ã®åˆ†å¸ƒ
â”œâ”€â”€ optim/           # Optimization algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
â””â”€â”€ data/            # Data loaders / ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
```

## ğŸ“š Rich Features / è±Šå¯Œãªæ©Ÿèƒ½

### Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- **Basic operations / åŸºæœ¬æ¼”ç®—**: `+`, `-`, `*`, `/`, `matmul()`
- **Mathematical functions / æ•°å­¦é–¢æ•°**: `sin()`, `cos()`, `exp()`, `log()`, `sqrt()`, `pow()`, `sigmoid()`, `tanh()`
- **Special functions / ç‰¹æ®Šé–¢æ•°**: `gamma()`, `lgamma()`, `digamma()`, `erf()`, `erfc()`, `bessel_j()`, `bessel_y()`, `bessel_i()`, `bessel_k()`
- **Statistical operations / çµ±è¨ˆæ¼”ç®—**: `mean()`, `var()`, `std()`, `median()`, `quantile()`, `cumsum()`, `cov()`, `corrcoef()`
- **Matrix decomposition / è¡Œåˆ—åˆ†è§£**: `svd()`, `qr()`, `lu()`, `eig()`, `symeig()` with PyTorch compatibility
- **Broadcasting / ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: `broadcast_to()`, `broadcast_with()`, `unsqueeze()`, `squeeze()`, `repeat()`
- **Indexing / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: `select()`, advanced slicing and tensor manipulation
- **Shape manipulation / å½¢çŠ¶æ“ä½œ**: `transpose()`, `reshape()`, `permute()`
- **Parallel operations / ä¸¦åˆ—æ“ä½œ**: Trait-based parallel processing with automatic SIMD acceleration
- **GPU operations / GPUæ“ä½œ**: CUDA/Metal/OpenCL unified kernel execution with automatic device selection
- **Memory optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**: Zero-copy views, SIMD-aligned allocation, memory pools

### Neural Network Layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
- **Linear**: Fully connected layers / å…¨çµåˆå±¤
- **Conv2d**: 2D convolution layers / 2Dç•³ã¿è¾¼ã¿å±¤
- **RNN/LSTM/GRU**: Recurrent neural networks (multi-layer & bidirectional) / å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå¤šå±¤ãƒ»åŒæ–¹å‘å¯¾å¿œï¼‰
- **Transformer**: Complete transformer architecture with encoder/decoder / ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ä»˜ãå®Œå…¨Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **Multi-Head Attention**: Self-attention and cross-attention mechanisms / ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹
- **Embedding**: Word embeddings, positional encoding, sinusoidal encoding / å˜èªåŸ‹ã‚è¾¼ã¿ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- **Normalization**: BatchNorm, LayerNorm, GroupNorm, RMSNorm / ãƒãƒƒãƒæ­£è¦åŒ–ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã€ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ–ã€RMSæ­£è¦åŒ–
- **Dropout**: Standard and Alpha dropout layers / æ¨™æº–ãƒ»Alphaãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤
- **Pooling**: MaxPool2d, AvgPool2d

### Activation Functions / æ´»æ€§åŒ–é–¢æ•°
`ReLU`, `Sigmoid`, `Tanh`, `Softmax`, `GELU`, `Swish`, `Mish`, `LeakyReLU`, `ELU`, `SELU`

### Loss Functions / æå¤±é–¢æ•°
`MSELoss`, `CrossEntropyLoss`, `BCELoss`, `HuberLoss`

### Optimization Algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
`SGD`, `Adam` + Learning rate schedulers / å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

## ğŸ“– Examples / ã‚µãƒ³ãƒ—ãƒ«

Comprehensive examples in the [examples/](examples/) directory:  
[examples/](examples/) ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åŒ…æ‹¬çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨æ„:

- **Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: 
  - [math_ops_demo.rs](examples/math_ops_demo.rs) - Mathematical functions demonstration
  - [broadcasting_demo.rs](examples/broadcasting_demo.rs) - Broadcasting operations
  - [indexing_demo.rs](examples/indexing_demo.rs) - Indexing and selection operations
  - [statistics_demo.rs](examples/statistics_demo.rs) - Statistical functions
- **Matrix Decomposition / è¡Œåˆ—åˆ†è§£**:
  - [svd_demo.rs](examples/svd_demo.rs) - SVD demonstration with verification and edge cases
  - [eigenvalue_demo.rs](examples/eigenvalue_demo.rs) - Eigenvalue decomposition with PCA examples
  - [matrix_decomposition_demo.rs](examples/matrix_decomposition_demo.rs) - QR/LU demonstrations with linear system solving
- **Transformer & Attention / Transformerãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**:
  - [transformer_demo.rs](examples/transformer_demo.rs) - Complete transformer pipeline
  - [embedding_demo.rs](examples/embedding_demo.rs) - Word and positional embeddings
  - [attention_demo.rs](examples/attention_demo.rs) - Multi-head attention mechanisms
- **Special Functions / ç‰¹æ®Šé–¢æ•°**:
  - [special_functions_demo.rs](examples/special_functions_demo.rs) - Gamma, Bessel, error functions demonstration
- **Performance Optimization / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**:
  - [parallel_operations_demo.rs](examples/parallel_operations_demo.rs) - Parallel tensor operations with trait-based system
  - [memory_optimization_demo.rs](examples/memory_optimization_demo.rs) - Advanced memory optimization strategies
  - [gpu_acceleration_demo.rs](examples/gpu_acceleration_demo.rs) - GPU acceleration with multi-backend support
  - [gpu_kernel_demo.rs](examples/gpu_kernel_demo.rs) - GPU kernel validation and performance demonstration
  - [simd_demo.rs](examples/simd_demo.rs) - SIMD vectorized operations
- **Basic / åŸºæœ¬**: [tensor_demo.rs](examples/tensor_demo.rs), [autograd_demo.rs](examples/autograd_demo.rs)
- **Neural Networks / NN**: [linear_regression.rs](examples/linear_regression.rs), [neural_network_demo.rs](examples/neural_network_demo.rs)
- **Advanced / é«˜åº¦**: [rnn_demo.rs](examples/rnn_demo.rs), [advanced_features_demo.rs](examples/advanced_features_demo.rs)

### Running Examples / ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ

```bash
# Run tensor operations examples / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example math_ops_demo --release
cargo run --example broadcasting_demo --release
cargo run --example statistics_demo --release

# Run special functions examples / ç‰¹æ®Šé–¢æ•°ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example special_functions_demo --release

# Run matrix decomposition examples / è¡Œåˆ—åˆ†è§£ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example svd_demo --release
cargo run --example eigenvalue_demo --release
cargo run --example matrix_decomposition_demo --release

# Run transformer examples / Transformerã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example transformer_demo --release
cargo run --example embedding_demo --release
cargo run --example attention_demo --release

# Run performance optimization examples / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example parallel_operations_demo --release
cargo run --example memory_optimization_demo --release
cargo run --example gpu_acceleration_demo --release
cargo run --example gpu_kernel_demo --release
cargo run --example simd_demo --release

# Run neural network examples / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example linear_regression --release
cargo run --example neural_network_demo --release
cargo run --example rnn_demo --release

# Run advanced examples / é«˜åº¦ãªã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
cargo run --example autograd_demo --release
cargo run --example advanced_features_demo --release
```

## ğŸ§ª Testing / ãƒ†ã‚¹ãƒˆ

**All 519 tests passing** - Production-ready quality assurance with complete functionality validation  
**519å€‹å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼** - å®Œå…¨æ©Ÿèƒ½æ¤œè¨¼ä»˜ãæœ¬ç•ªç’°å¢ƒå¯¾å¿œã®å“è³ªä¿è¨¼

### ğŸ§® Matrix Decomposition Tests / è¡Œåˆ—åˆ†è§£ãƒ†ã‚¹ãƒˆ

**25 comprehensive matrix decomposition tests** including:  
**åŒ…æ‹¬çš„ãª25å€‹ã®è¡Œåˆ—åˆ†è§£ãƒ†ã‚¹ãƒˆ**å«ã‚€:

- **SVD (9 tests)**: Square matrices, rectangular matrices, identity matrices, rank-deficient cases, orthogonality verification
- **Eigenvalue (8 tests)**: General matrices, symmetric matrices, eigenvector computation, identity and zero matrices  
- **QR (4 tests)**: Basic decomposition, rectangular matrices, identity matrices, error handling
- **LU (4 tests)**: Basic decomposition, identity matrices, rectangular matrices, partial pivoting

All matrix decomposition algorithms achieve **100% success rate** across all test cases.  
å…¨ã¦ã®è¡Œåˆ—åˆ†è§£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§**100%æˆåŠŸç‡**ã‚’é”æˆã€‚

```bash
# Run all tests / å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test

# Run with release optimizations / ãƒªãƒªãƒ¼ã‚¹æœ€é©åŒ–ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test --release

# Run specific test modules / ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
cargo test tensor
cargo test nn
cargo test autograd
```

## ğŸ“Š Benchmarks / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Comprehensive performance measurement with dedicated benchmark suites:  
å°‚ç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã§åŒ…æ‹¬çš„ãªæ€§èƒ½æ¸¬å®š:

```bash
# Run all benchmarks / å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
cargo bench

# Run specific benchmark suites / ç‰¹å®šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
cargo bench --bench parallel_performance      # Parallel processing benchmarks
cargo bench --bench simd_performance         # SIMD optimization benchmarks  
cargo bench --bench memory_strategy_performance  # Memory optimization benchmarks
cargo bench --bench gpu_cpu_performance      # GPU vs CPU comparison benchmarks
cargo bench --bench gpu_kernel_performance   # GPU kernel validation and performance
cargo bench --bench integrated_performance   # Integrated performance tests

# Matrix decomposition benchmarks / è¡Œåˆ—åˆ†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
cargo bench --bench matrix_decomposition_benchmark  # Comprehensive matrix decomposition
cargo bench --bench optimized_matrix_benchmark      # Timeout-optimized matrix benchmarks
cargo bench --bench quick_matrix_benchmark         # Quick matrix performance tests

# Legacy benchmarks / ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
cargo bench --bench tensor_ops
cargo bench --bench neural_networks
cargo bench --bench optimized_ops
cargo bench --bench memory_pool
cargo bench --bench memory_optimization
cargo bench --bench gpu_integration
```

**New Benchmark Suites / æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ:**
- `parallel_performance`: Parallel vs sequential operations, thread scaling, execution strategies / ä¸¦åˆ—vsé€æ¬¡æ¼”ç®—ã€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€å®Ÿè¡Œæˆ¦ç•¥
- `simd_performance`: SIMD vs scalar operations, vectorization effectiveness, instruction sets / SIMDvsã‚¹ã‚«ãƒ©ãƒ¼æ¼”ç®—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–åŠ¹æœã€å‘½ä»¤ã‚»ãƒƒãƒˆ
- `memory_strategy_performance`: Memory allocation strategies, zero-copy operations, cache optimization / ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦æˆ¦ç•¥ã€ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼æ“ä½œã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
- `gpu_cpu_performance`: GPU acceleration vs CPU processing, device selection, memory transfer / GPUåŠ é€ŸvsCPUå‡¦ç†ã€ãƒ‡ãƒã‚¤ã‚¹é¸æŠã€ãƒ¡ãƒ¢ãƒªè»¢é€
- `integrated_performance`: End-to-end performance validation across all optimizations / å…¨æœ€é©åŒ–ã®çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼

**Matrix Decomposition Benchmarks / è¡Œåˆ—åˆ†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:**
- `matrix_decomposition_benchmark`: Comprehensive SVD, QR, LU, eigenvalue benchmarks with scaling analysis / SVDã€QRã€LUã€å›ºæœ‰å€¤ã®åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£æ
- `optimized_matrix_benchmark`: Timeout-resistant benchmarks with conservative settings / ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè€æ€§ã®ä¿å®ˆçš„è¨­å®šãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- `quick_matrix_benchmark`: Fast matrix operation benchmarks for development / é–‹ç™ºç”¨é«˜é€Ÿè¡Œåˆ—æ¼”ç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**Legacy Benchmark Categories / ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒª:**
- `tensor_ops`: Basic tensor operations / åŸºæœ¬ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- `autograd_ops`: Automatic differentiation operations / è‡ªå‹•å¾®åˆ†æ¼”ç®—
- `neural_networks`: Neural network operations / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- `optimized_ops`: SIMD and parallel optimizations / SIMDãƒ»ä¸¦åˆ—æœ€é©åŒ–
- `memory_pool`: Memory management performance / ãƒ¡ãƒ¢ãƒªç®¡ç†æ€§èƒ½
- `memory_optimization`: Advanced memory strategies / é«˜åº¦ãƒ¡ãƒ¢ãƒªæˆ¦ç•¥
- `gpu_integration`: GPU acceleration benchmarks / GPUåŠ é€Ÿãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

## ğŸ“– Documentation / ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

For detailed API documentation, please refer to [docs.rs/rustorch](https://docs.rs/rustorch).  
è©³ç´°ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ [docs.rs/rustorch](https://docs.rs/rustorch) ã‚’ã”è¦§ãã ã•ã„ã€‚

## ğŸš€ Production Deployment / æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

### Docker Deployment

RusTorch provides production-ready Docker images with multi-stage builds for optimal performance:

```bash
# Production deployment
docker build -t rustorch:latest .
docker run -it rustorch:latest

# GPU-enabled deployment (requires NVIDIA Docker)
docker build -f Dockerfile.gpu -t rustorch:gpu .
docker run --gpus all -it rustorch:gpu

# Development environment
docker compose up rustorch-dev

# Complete multi-service stack
docker compose --profile gpu up  # With GPU support
docker compose --profile python up  # With Jupyter notebooks
```

### CI/CD Pipeline

Automated testing and deployment through GitHub Actions:

- **Multi-platform Testing**: Ubuntu, macOS, Windows across Rust stable/beta/nightly
- **Code Quality**: Rustfmt, Clippy, security audits, dependency reviews
- **Performance Regression**: Automated benchmark comparisons
- **Security Scanning**: Trivy vulnerability scanning, CodeQL analysis
- **Documentation**: Auto-generated and deployed to GitHub Pages
- **Release Automation**: Automated crates.io publishing on releases

### Production Features

- **Memory Safety**: Zero unsafe code in core functionality
- **Thread Safety**: Full concurrent operation support
- **Error Handling**: Comprehensive error types and recovery
- **Monitoring**: Built-in performance metrics and logging
- **Scalability**: Horizontal scaling with distributed computing support
- **Security**: Regular dependency audits and vulnerability scanning

## ğŸ—ï¸ Architecture Overview / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
ğŸ¢ Production Stack
â”œâ”€â”€ ğŸš€ Application Layer
â”‚   â”œâ”€â”€ High-level APIs (Sequential, Trainer)
â”‚   â”œâ”€â”€ Model definitions (CNN, RNN, Transformer)
â”‚   â””â”€â”€ Training loops and inference
â”œâ”€â”€ ğŸ§  Neural Network Layer  
â”‚   â”œâ”€â”€ Core layers (Linear, Conv2d, Attention)
â”‚   â”œâ”€â”€ Activation functions (ReLU, Softmax, GELU)
â”‚   â””â”€â”€ Normalization (BatchNorm, LayerNorm)
â”œâ”€â”€ ğŸ”§ Computation Engine
â”‚   â”œâ”€â”€ Tensor operations (Math, Broadcasting)
â”‚   â”œâ”€â”€ Automatic differentiation (Backprop)
â”‚   â””â”€â”€ Memory management (Pools, Zero-copy)
â”œâ”€â”€ âš¡ Optimization Layer
â”‚   â”œâ”€â”€ SIMD vectorization (AVX2, SSE4.1)
â”‚   â”œâ”€â”€ Parallel processing (Rayon threading)
â”‚   â””â”€â”€ GPU acceleration (CUDA, Metal, OpenCL)
â””â”€â”€ ğŸ—ï¸ Infrastructure Layer
    â”œâ”€â”€ Cross-platform support (Linux, macOS, Windows)
    â”œâ”€â”€ WebAssembly bindings (Browser deployment)
    â””â”€â”€ Docker containerization (Production-ready)
```

## License

Licensed under either of:

 * Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
 * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.
