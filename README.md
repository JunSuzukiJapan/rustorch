# RusTorch ğŸš€

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-76%20passing-green.svg)](#testing)

**A high-performance deep learning library in Rust with PyTorch-like API, combining safety and speed**  
**é«˜æ€§èƒ½ãªRustè£½ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª - PyTorchãƒ©ã‚¤ã‚¯ãªAPIã§å®‰å…¨æ€§ã¨é€Ÿåº¦ã‚’ä¸¡ç«‹**

RusTorch is a deep learning library that leverages Rust's safety and performance, providing automatic differentiation, rich neural network layers, and optimized tensor operations.  
RusTorchã¯ã€Rustã®å®‰å…¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ´»ã‹ã—ãŸãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ ã€è±Šå¯Œãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã€æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚’æä¾›ã—ã¾ã™ã€‚

## âœ¨ Features / ä¸»ãªç‰¹å¾´

- ğŸ”¥ **High-Performance Tensor Operations**: 3-9% performance improvements with optimized ndarray backend  
  **é«˜æ€§èƒ½ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—**: æœ€é©åŒ–ã•ã‚ŒãŸndarrayåŸºç›¤ã§3-9%ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾
- ğŸ§  **Complete Automatic Differentiation**: Tape-based computational graph for automatic gradient computation  
  **å®Œå…¨ãªè‡ªå‹•å¾®åˆ†**: ãƒ†ãƒ¼ãƒ—ãƒ™ãƒ¼ã‚¹ã®è¨ˆç®—ã‚°ãƒ©ãƒ•ã«ã‚ˆã‚‹è‡ªå‹•å‹¾é…è¨ˆç®—
- ğŸ—ï¸ **Rich Neural Network Layers**: Linear, Conv2d, RNN/LSTM/GRU, BatchNorm, and more  
  **è±Šå¯ŒãªNNå±¤**: Linearã€Conv2dã€RNN/LSTM/GRUã€BatchNormç­‰ã‚’å®Œå‚™
- âš¡ **In-place Operations**: Memory-efficient `add_inplace()`, `mul_inplace()`, etc.  
  **In-placeæ¼”ç®—**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãª`add_inplace()`, `mul_inplace()`ç­‰
- ğŸ¯ **PyTorch-like API**: Familiar interface for PyTorch users  
  **PyTorchãƒ©ã‚¤ã‚¯API**: è¦ªã—ã¿ã‚„ã™ã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- ğŸ›¡ï¸ **Rust Safety**: Memory safety and thread safety guarantees  
  **Rustå®‰å…¨æ€§**: ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚’ä¿è¨¼
- ğŸ“Š **Comprehensive Testing**: 76 tests ensuring stability  
  **åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ**: 76å€‹ã®ãƒ†ã‚¹ãƒˆã§å®‰å®šæ€§ã‚’ç¢ºä¿

## Installation

Add this to your `Cargo.toml`:

```toml
[dependencies]
rustorch = "0.1.0"
```

## ğŸ“Š Performance / ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

Latest benchmark results (post-optimization):  
æœ€æ–°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆæœ€é©åŒ–å¾Œï¼‰:

| Operation / æ¼”ç®— | Execution Time / å®Ÿè¡Œæ™‚é–“ | Improvement / æ”¹å–„ç‡ |
|------------------|---------------------------|---------------------|
| 100x100 Matrix Multiplication / 100x100è¡Œåˆ—ä¹—ç®— | 69Âµs | 9.2% improvement / 9.2%å‘ä¸Š |
| Tensor Addition / ãƒ†ãƒ³ã‚½ãƒ«åŠ ç®— | 1.93Âµs | 3.0% improvement / 3.0%å‘ä¸Š |
| Transpose / è»¢ç½®æ¼”ç®— | 1.30Âµs | 1.5% improvement / 1.5%å‘ä¸Š |
| 1000x1000 Matrix Multiplication / 1000x1000è¡Œåˆ—ä¹—ç®— | 32.5ms | 1.5% improvement / 1.5%å‘ä¸Š |
| Batch Processing / ãƒãƒƒãƒå‡¦ç† | 268Âµs | New feature / æ–°æ©Ÿèƒ½ |

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Basic Tensor Operations / åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—

```rust
use rustorch::prelude::*;

fn main() {
    // Create tensors / ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    let a = Tensor::from_vec(vec![1.0, 2.0, 3.0, 4.0], vec![2, 2]);
    let b = Tensor::from_vec(vec![5.0, 6.0, 7.0, 8.0], vec![2, 2]);
    
    // Basic operations / åŸºæœ¬æ¼”ç®—
    let c = &a + &b;  // Addition / åŠ ç®—
    let d = a.matmul(&b);  // Matrix multiplication / è¡Œåˆ—ä¹—ç®—
    
    // In-place operations (memory efficient) / In-placeæ¼”ç®—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
    let mut e = a.clone();
    e.add_inplace(&b);
    e.mul_scalar_inplace(2.0);
    
    println!("Result: {:?}", e.size());
}
```

### Automatic Differentiation and Neural Networks / è‡ªå‹•å¾®åˆ†ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

```rust
use rustorch::prelude::*;
use rustorch::nn::{Linear, loss::mse_loss};
use rustorch::optim::{SGD, Optimizer};

fn main() {
    // Create model / ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    let model = Linear::new(784, 10);
    let params = model.parameters();
    let mut optimizer = SGD::new(params, 0.01, None, None, None, None);
    
    // Prepare data / ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let input = Variable::new(
        Tensor::from_vec((0..784).map(|i| i as f32 * 0.01).collect(), vec![1, 784]),
        false
    );
    let target = Variable::new(
        Tensor::from_vec(vec![1.0; 10], vec![1, 10]),
        false
    );
    
    // Training loop / è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in 0..100 {
        optimizer.zero_grad();
        
        let output = model.forward(&input);
        let loss = mse_loss(&output, &target);
        
        loss.backward();
        optimizer.step();
        
        if epoch % 10 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch, loss.data().as_array()[[0]]);
        }
    }
}
```

## ğŸ—ï¸ Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
src/
â”œâ”€â”€ tensor/          # Tensor operations (ndarray-based) / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ï¼ˆndarrayåŸºç›¤ï¼‰
â”œâ”€â”€ autograd/        # Automatic differentiation system / è‡ªå‹•å¾®åˆ†ã‚·ã‚¹ãƒ†ãƒ 
â”œâ”€â”€ nn/              # Neural network layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
â”‚   â”œâ”€â”€ linear.rs    # Linear layers / ç·šå½¢å±¤
â”‚   â”œâ”€â”€ conv2d.rs    # Convolution layers / ç•³ã¿è¾¼ã¿å±¤
â”‚   â”œâ”€â”€ rnn.rs       # RNN/LSTM/GRU
â”‚   â”œâ”€â”€ activation.rs # Activation functions / æ´»æ€§åŒ–é–¢æ•°
â”‚   â””â”€â”€ loss.rs      # Loss functions / æå¤±é–¢æ•°
â”œâ”€â”€ optim/           # Optimization algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
â””â”€â”€ data/            # Data loaders / ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
```

## ğŸ“š Rich Features / è±Šå¯Œãªæ©Ÿèƒ½

### Tensor Operations / ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- Basic operations / åŸºæœ¬æ¼”ç®—: `+`, `-`, `*`, `/`, `matmul()`
- In-place operations / In-placeæ¼”ç®—: `add_inplace()`, `mul_inplace()`, `sub_inplace()`
- Reductions / ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³: `sum()`, `mean()`, `sum_axis()`
- Shape manipulation / å½¢çŠ¶æ“ä½œ: `transpose()`, `reshape()`, `permute()`

### Neural Network Layers / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤
- **Linear**: Fully connected layers / å…¨çµåˆå±¤
- **Conv2d**: 2D convolution layers / 2Dç•³ã¿è¾¼ã¿å±¤
- **RNN/LSTM/GRU**: Recurrent neural networks (multi-layer & bidirectional) / å†å¸°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå¤šå±¤ãƒ»åŒæ–¹å‘å¯¾å¿œï¼‰
- **BatchNorm**: Batch normalization / ãƒãƒƒãƒæ­£è¦åŒ–
- **Dropout**: Dropout layers / ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
- **Pooling**: MaxPool2d, AvgPool2d

### Activation Functions / æ´»æ€§åŒ–é–¢æ•°
`ReLU`, `Sigmoid`, `Tanh`, `Softmax`, `GELU`, `Swish`, `Mish`, `LeakyReLU`, `ELU`, `SELU`

### Loss Functions / æå¤±é–¢æ•°
`MSELoss`, `CrossEntropyLoss`, `BCELoss`, `HuberLoss`

### Optimization Algorithms / æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
`SGD`, `Adam` + Learning rate schedulers / å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

## ğŸ“– Examples / ã‚µãƒ³ãƒ—ãƒ«

20 practical examples in the [examples/](examples/) directory:  
[examples/](examples/) ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«20å€‹ã®å®Ÿç”¨çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨æ„:

- **Basic / åŸºæœ¬**: [tensor_demo.rs](examples/tensor_demo.rs), [autograd_demo.rs](examples/autograd_demo.rs)
- **Neural Networks / NN**: [linear_regression.rs](examples/linear_regression.rs), [neural_network_demo.rs](examples/neural_network_demo.rs)
- **Advanced / é«˜åº¦**: [rnn_demo.rs](examples/rnn_demo.rs), [advanced_features_demo.rs](examples/advanced_features_demo.rs)

## ğŸ§ª Testing / ãƒ†ã‚¹ãƒˆ

```bash
# Run all tests / å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cargo test

# Run benchmarks / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
cargo bench

# Run specific benchmarks / ç‰¹å®šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
cargo bench --bench tensor_ops
cargo bench --bench optimized_ops
```

## ğŸ“Š Benchmarks / ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Continuous performance measurement with 4 dedicated benchmark suites:  
4ã¤ã®å°‚ç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã§æ€§èƒ½ã‚’ç¶™ç¶šçš„ã«æ¸¬å®š:
- `tensor_ops`: Basic tensor operations / åŸºæœ¬ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—
- `autograd_ops`: Automatic differentiation operations / è‡ªå‹•å¾®åˆ†æ¼”ç®—
- `neural_networks`: Neural network operations / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- `optimized_ops`: Optimized operations / æœ€é©åŒ–ã•ã‚ŒãŸæ¼”ç®—

## ğŸ“– Documentation / ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

For detailed API documentation, please refer to [docs.rs/rustorch](https://docs.rs/rustorch).  
è©³ç´°ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ [docs.rs/rustorch](https://docs.rs/rustorch) ã‚’ã”è¦§ãã ã•ã„ã€‚

## License

Licensed under either of:

 * Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
 * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.
