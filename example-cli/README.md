# RusTorch CLI

Interactive command-line interface for running local Large Language Models using RusTorch.

## üìã Overview

RusTorch CLI provides a user-friendly REPL (Read-Eval-Print Loop) interface for interacting with local LLMs, with support for multiple computational backends including CPU, CUDA, Metal, OpenCL, and Apple's Neural Engine.

## ‚ú® Features

- üñ•Ô∏è  **Interactive REPL Interface** - Claude Code-inspired command-line interface
- üöÄ **Multiple Backends** - CPU, CUDA, Metal, OpenCL, CoreML, and hybrid modes
- üíæ **Session Management** - Save and load conversation history
- ‚öôÔ∏è  **Flexible Configuration** - Command-line arguments and TOML config files
- üìä **Progress Indicators** - Visual feedback during model inference
- üéØ **Special Commands** - Built-in commands for session control

## üöÄ Quick Start

### Installation

```bash
# Build from source
cargo build --release --bin rustorch-cli

# The binary will be in target/release/rustorch-cli
```

### Basic Usage

```bash
# Start with default CPU backend
rustorch-cli

# Specify a model file
rustorch-cli --model path/to/model.gguf

# Use Metal GPU backend (macOS)
rustorch-cli --backend metal

# Use CUDA GPU backend (NVIDIA)
rustorch-cli --backend cuda --features cuda

# Use hybrid mode with f32 precision
rustorch-cli --backend hybrid-f32 --features hybrid-f32
```

## üéÆ Interactive Commands

Once inside the REPL, you can use the following commands:

| Command | Description |
|---------|-------------|
| `/exit`, `/quit`, `/q` | Exit the application |
| `/help`, `/h`, `/?` | Show help message |
| `/clear`, `/cls` | Clear conversation history |
| `/save [FILE]` | Save conversation history |
| `/load [FILE]` | Load conversation history |
| `/model [FILE]` | Reload or switch model |
| `/backend <TYPE>` | Switch computational backend |
| `/stats`, `/status` | Display statistics |
| `/system <PROMPT>` | Set system prompt |
| `/config`, `/cfg` | Display current configuration |
| `/config save [FILE]` | Save configuration to file |

## üõ†Ô∏è Development Helper Script

The `run-cli.sh` script provides a convenient way to build and run the CLI during development:

```bash
# Quick start with CPU backend (release mode)
./example-cli/run-cli.sh

# Debug mode for faster compilation
./example-cli/run-cli.sh --debug

# With Metal GPU backend
./example-cli/run-cli.sh --metal

# With Mac hybrid mode (Metal + CoreML)
./example-cli/run-cli.sh --mac-hybrid

# With specific CLI commands
./example-cli/run-cli.sh --metal -- chat
./example-cli/run-cli.sh --debug -- download --source ollama llama2

# Show help
./example-cli/run-cli.sh --help
```

### Script Options

| Option | Description |
|--------|-------------|
| `--debug` | Build in debug mode (faster compilation) |
| `--metal` | Enable Metal GPU backend |
| `--coreml` | Enable CoreML backend |
| `--mac-hybrid` | Enable Mac hybrid backend (Metal + CoreML) |
| `--hybrid-f32` | Enable f32 hybrid mode |
| `--cuda` | Enable CUDA backend |
| `--help` | Show help message |

Arguments after `--` are passed directly to the CLI application.

## ‚öôÔ∏è Configuration

### Command-Line Options

```bash
rustorch-cli --help
```

Key options:
- `--model <FILE>` - Path to model file
- `--backend <BACKEND>` - Computation backend (cpu, cuda, metal, opencl, hybrid, hybrid-f32)
- `--config <FILE>` - Path to configuration file
- `--max-tokens <N>` - Maximum tokens to generate (default: 2048)
- `--temperature <F>` - Sampling temperature (default: 0.7)
- `--top-p <F>` - Top-p sampling (default: 0.9)
- `--top-k <N>` - Top-k sampling (default: 40)
- `--log-level <LEVEL>` - Log level (trace, debug, info, warn, error)

### Configuration File

Create a `config.toml` file (default location: `~/.rustorch/config.toml`):

```toml
[model]
path = "~/.rustorch/models/llama-7b.gguf"
format = "gguf"

[backend]
default = "metal"
fallback = ["cpu"]

[generation]
max_tokens = 2048
temperature = 0.7
top_p = 0.9
top_k = 40

[interface]
prompt_user = "You> "
prompt_assistant = "Assistant> "
show_progress = true

[logging]
level = "info"
file = "~/.rustorch/logs/cli.log"

[history]
auto_save = true
file = "~/.rustorch/history/latest.json"
max_entries = 1000
```

## üèóÔ∏è Build Features

The CLI supports various backend features that can be enabled during compilation:

```bash
# CPU only (default)
cargo build --release

# With Metal GPU support (macOS)
cargo build --release --features metal

# With CUDA support (NVIDIA GPUs)
cargo build --release --features cuda

# With OpenCL support
cargo build --release --features opencl

# With CoreML support (Apple devices)
cargo build --release --features coreml

# With Mac Hybrid mode (Metal + CoreML)
cargo build --release --features mac-hybrid

# With Hybrid F32 mode
cargo build --release --features hybrid-f32
```

## üìö Model Formats

Supported model formats (planned):
- **GGUF** - llama.cpp compatible format (priority)
- **Safetensors** - Hugging Face standard format
- **ONNX** - ONNX Runtime format
- **MLX** - Apple Silicon optimized format

## üîß Development

### Project Structure

```
example-cli/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/          # CLI argument parsing and REPL
‚îÇ   ‚îú‚îÄ‚îÄ session/      # Session and history management
‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Utilities (logging, progress, errors)
‚îÇ   ‚îú‚îÄ‚îÄ lib.rs        # Library exports
‚îÇ   ‚îî‚îÄ‚îÄ main.rs       # Entry point
‚îú‚îÄ‚îÄ docs/             # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ REQUIREMENTS.md
‚îÇ   ‚îú‚îÄ‚îÄ IMPLEMENTATION_PLAN.md
‚îÇ   ‚îú‚îÄ‚îÄ CONFIG.md           # Configuration guide (Phase 7)
‚îÇ   ‚îú‚îÄ‚îÄ EXAMPLES.md
‚îÇ   ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md
‚îú‚îÄ‚îÄ tests/            # Integration tests
‚îú‚îÄ‚îÄ examples/         # Usage examples
‚îî‚îÄ‚îÄ Cargo.toml        # Project configuration
```

### Running Tests

```bash
# Run all tests
cargo test

# Run with specific backend feature
cargo test --features metal
```

### Development Mode

```bash
# Run with debug logging
cargo run -- --log-level debug

# Run with specific backend
cargo run --features metal -- --backend metal
```

## ü§ù Contributing

This is part of the RusTorch project. For contribution guidelines, please see the main RusTorch repository.

## üìÑ License

MIT OR Apache-2.0

## üîó Links

- [RusTorch Main Repository](https://github.com/JunSuzukiJapan/rustorch)
- [Documentation](docs/)
- [Requirements](docs/REQUIREMENTS.md)
- [Implementation Plan](docs/IMPLEMENTATION_PLAN.md)
- [Configuration Guide](docs/CONFIG.md)
- [Examples](docs/EXAMPLES.md)
- [Troubleshooting](docs/TROUBLESHOOTING.md)

## üéØ Roadmap

### Phase 1: Foundation ‚úÖ COMPLETE
- ‚úÖ Basic REPL interface
- ‚úÖ Command-line argument parsing
- ‚úÖ Session management
- ‚úÖ Special commands

### Phase 2: Model Support ‚úÖ COMPLETE
- ‚úÖ Tokenizer integration
- ‚úÖ Model loader framework
- ‚úÖ Transformer inference engine

### Phase 3: Backend Integration ‚úÖ COMPLETE
- ‚úÖ CPU implementation
- ‚úÖ Metal GPU support (macOS)
- ‚úÖ Multi-backend architecture

### Phase 4: REPL Enhancement ‚úÖ COMPLETE
- ‚úÖ Token streaming display
- ‚úÖ Colored output
- ‚úÖ Multiline input support
- ‚úÖ Progress indicators
- ‚úÖ Model/backend switching

### Phase 5: Multi-Format Support ‚úÖ COMPLETE
- ‚úÖ Safetensors format loader
- ‚úÖ ONNX format support
- ‚úÖ Format auto-detection
- ‚úÖ ModelLoader integration

### Phase 6: Quality & Docs ‚úÖ COMPLETE
- ‚úÖ 102 unit tests passing
- ‚úÖ Code refactoring
- ‚úÖ Clippy compliance
- ‚úÖ Comprehensive documentation

### Phase 7: Configuration Management ‚úÖ COMPLETE
- ‚úÖ TOML configuration file support
- ‚úÖ `~/.rustorch/config.toml` auto-loading
- ‚úÖ `/config save` command
- ‚úÖ CLI argument priority override
- ‚úÖ Comprehensive CONFIG.md guide

### Phase 8: CLI Arguments Complete ‚úÖ COMPLETE
- ‚úÖ `--save-history` flag implementation
- ‚úÖ `--load-history` flag implementation
- ‚úÖ Auto-save on exit with Ctrl+C handling
- ‚úÖ Full CLI argument coverage

### Phase 9-12: Advanced Features (Planned)
- ‚è≥ MLX and PyTorch format support
- ‚è≥ Real model inference with RusTorch API
- ‚è≥ Full backend optimization (Metal/CUDA/OpenCL)
- ‚è≥ Performance measurement and benchmarks

## üí° Examples

### Basic Conversation

```bash
$ rustorch-cli
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           RusTorch CLI - Local LLM Chat                   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Backend: cpu
Model: dummy-model

Type '/help' for available commands, '/exit' to quit.

You> Hello, how are you?
Assistant> [Echo] Hello, how are you?

You> /stats
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                     Statistics                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

  Messages:        2
  Total tokens:    10
  Backend:         cpu
  Model:           dummy-model

You> /exit
Saving session...
Goodbye!
```

## ‚ö†Ô∏è Current Status

**Phase 8 Complete** - Production-ready CLI with complete CLI argument support:

‚úÖ **Implemented:**
- Full REPL interface with colored output
- Token streaming display
- Multiline input support
- **TOML configuration file support** (Phase 7)
- **Persistent settings with `~/.rustorch/config.toml`** (Phase 7)
- **`/config save` command** (Phase 7)
- **`--save-history` / `--load-history` flags** (Phase 8)
- **Auto-save on exit** (Phase 8)
- Multiple model format loaders (GGUF, Safetensors, ONNX)
- Multi-backend support (CPU, Metal, CUDA)
- Session management with save/load
- Model and backend hot-swapping
- 108 unit tests, zero clippy warnings

‚è≥ **Limitations:**
- Full model inference requires actual model weights
- GGUF parsing is metadata-only (full implementation pending)
- ONNX requires ONNX Runtime for inference
- Production models need additional configuration

This is a **fully functional CLI framework** ready for integration with production LLM models.

## üôè Acknowledgments

- Inspired by [Claude Code](https://claude.com/code) CLI interface
- Built on [RusTorch](https://github.com/JunSuzukiJapan/rustorch) deep learning library
- Uses [rustyline](https://github.com/kkawakam/rustyline) for REPL functionality
- Uses [clap](https://github.com/clap-rs/clap) for CLI argument parsing
