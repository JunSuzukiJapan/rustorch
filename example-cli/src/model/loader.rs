use anyhow::Result;
use rustorch::prelude::Tensor;
use std::collections::HashMap;
use std::path::{Path, PathBuf};

use super::format_loader::FormatLoader;
use super::loaders::{GGUFFormatLoader, MLXFormatLoader, SafetensorsFormatLoader};
use super::{ModelFormat, ModelMetadata};
use crate::tokenizer::{Tokenizer, TokenizerWrapper, GGUFTokenizer};

pub struct ModelLoader {
    path: PathBuf,
    metadata: ModelMetadata,
    weights: HashMap<String, Tensor<f64>>,
    tokenizer: Box<dyn Tokenizer>,
}

impl ModelLoader {
    /// Load a model from file using the refactored loader architecture
    /// ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    pub fn from_file(path: impl AsRef<Path>) -> Result<Self> {
        let path = path.as_ref();

        if !path.exists() {
            anyhow::bail!("Model file not found: {}", path.display());
        }

        tracing::info!("Loading model from: {}", path.display());

        // Load metadata using appropriate format loader
        let metadata = Self::load_metadata_with_format_detection(path)?;

        tracing::info!("Detected format: {}", metadata.format.as_str());

        // Try to find tokenizer
        let tokenizer = Self::load_tokenizer(path, &metadata.format)?;

        Ok(Self {
            path: path.to_path_buf(),
            metadata,
            weights: HashMap::new(),
            tokenizer,
        })
    }

    /// Create a dummy model for testing only
    /// ãƒ†ã‚¹ãƒˆå°‚ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    #[cfg(test)]
    pub fn dummy() -> Self {
        Self::load_dummy().expect("Failed to create dummy model")
    }

    /// Load metadata with automatic format detection
    /// è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œå‡ºã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    fn load_metadata_with_format_detection(path: &Path) -> Result<ModelMetadata> {
        // Try each format loader in order of preference
        if GGUFFormatLoader::can_load(path) {
            return GGUFFormatLoader::load_metadata(path);
        }

        if SafetensorsFormatLoader::can_load(path) {
            return SafetensorsFormatLoader::load_metadata(path);
        }

        if MLXFormatLoader::can_load(path) {
            return MLXFormatLoader::load_metadata(path);
        }

        // Fall back to format detection by extension
        let format = ModelFormat::from_path(path)?;
        match format {
            ModelFormat::PyTorch => Self::load_pytorch_metadata(path),
            ModelFormat::ONNX => Self::load_onnx_metadata(path),
            _ => anyhow::bail!("Unsupported model format for path: {}", path.display()),
        }
    }

    /// Load PyTorch metadata (legacy support)
    /// PyTorchãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ã‚µãƒãƒ¼ãƒˆï¼‰
    fn load_pytorch_metadata(path: &Path) -> Result<ModelMetadata> {
        use super::formats::PyTorchLoader;

        tracing::info!("Loading PyTorch model from: {}", path.display());
        let (_state_dict, metadata) = PyTorchLoader::load(path)?;
        tracing::info!(
            "Successfully loaded PyTorch model with {} tensors",
            _state_dict.len()
        );

        Ok(metadata)
    }

    /// Load ONNX metadata (legacy support)
    /// ONNXãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ã‚µãƒãƒ¼ãƒˆï¼‰
    fn load_onnx_metadata(path: &Path) -> Result<ModelMetadata> {
        use super::formats::ONNXLoader;

        tracing::info!("Loading ONNX model from: {}", path.display());
        let loader = ONNXLoader::from_file(path)?;
        let meta = loader.metadata();
        tracing::info!("ONNX model metadata: {:?}", meta);

        Ok(ModelMetadata {
            name: path
                .file_name()
                .and_then(|n| n.to_str())
                .unwrap_or("unknown")
                .to_string(),
            format: ModelFormat::ONNX,
            vocab_size: 32000,
            hidden_size: 512,
            num_layers: 6,
            num_heads: 8,
            context_length: 2048,
        })
    }

    /// Load tokenizer for the model
    /// ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    fn load_tokenizer(path: &Path, format: &ModelFormat) -> Result<Box<dyn Tokenizer>> {
        // For GGUF format, try to extract tokenizer from file first
        if matches!(format, ModelFormat::GGUF) {
            tracing::info!("ğŸ” Attempting to extract tokenizer from GGUF file...");
            match rustorch::formats::gguf::GGUFLoader::from_file(path) {
                Ok(gguf) => {
                    match gguf.extract_tokenizer_vocab() {
                        Ok(vocab) => {
                            tracing::info!("âœ… Extracted {} tokens from GGUF file", vocab.len());
                            let tokenizer_model = gguf.get_tokenizer_model();
                            if let Some(model_type) = tokenizer_model {
                                tracing::info!("ğŸ“ Tokenizer model type: {}", model_type);
                            }
                            return Ok(Box::new(GGUFTokenizer::new(vocab)));
                        }
                        Err(e) => {
                            tracing::warn!("âš ï¸  Failed to extract tokenizer from GGUF: {}", e);
                        }
                    }
                }
                Err(e) => {
                    tracing::warn!("âš ï¸  Failed to load GGUF file for tokenizer extraction: {}", e);
                }
            }
        }

        // Try format-specific tokenizer path
        let tokenizer_path = match format {
            ModelFormat::GGUF => GGUFFormatLoader::default_tokenizer_path(path),
            ModelFormat::Safetensors => SafetensorsFormatLoader::default_tokenizer_path(path),
            ModelFormat::MLX => MLXFormatLoader::default_tokenizer_path(path),
            _ => None,
        };

        if let Some(tokenizer_path) = tokenizer_path {
            tracing::info!("Loading tokenizer from: {}", tokenizer_path.display());
            return Ok(Box::new(TokenizerWrapper::from_file(&tokenizer_path)?));
        }

        tracing::warn!("Tokenizer file not found, using dummy tokenizer");
        Ok(Box::new(TokenizerWrapper::dummy()?))
    }

    #[cfg(test)]
    fn load_dummy() -> Result<Self> {
        tracing::info!("Creating dummy model for testing");

        let metadata = ModelMetadata {
            name: "dummy-model".to_string(),
            format: ModelFormat::Dummy,
            vocab_size: 32000,
            hidden_size: 512,
            num_layers: 6,
            num_heads: 8,
            context_length: 2048,
        };

        let tokenizer = Box::new(TokenizerWrapper::dummy()?);

        Ok(Self {
            path: PathBuf::from("dummy.model"),
            metadata,
            weights: HashMap::new(),
            tokenizer,
        })
    }

    /// Get model path
    /// ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
    pub fn path(&self) -> &Path {
        &self.path
    }

    /// Get model metadata
    /// ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    pub fn metadata(&self) -> &ModelMetadata {
        &self.metadata
    }

    /// Get model weights
    pub fn weights(&self) -> &HashMap<String, Tensor<f64>> {
        &self.weights
    }

    /// Get a specific weight by name
    pub fn weight(&self, name: &str) -> Option<&Tensor<f64>> {
        self.weights.get(name)
    }

    /// Get tokenizer reference
    pub fn tokenizer(&self) -> &dyn Tokenizer {
        self.tokenizer.as_ref()
    }

    /// Encode text to token IDs
    pub fn encode(&self, text: &str) -> Result<Vec<u32>> {
        self.tokenizer.encode(text, true)
    }

    /// Decode token IDs to text
    pub fn decode(&self, ids: &[u32]) -> Result<String> {
        self.tokenizer.decode(ids, true)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_dummy_model_creation() {
        let loader = ModelLoader::dummy();
        assert_eq!(loader.metadata().name, "dummy-model");
        assert_eq!(loader.metadata().format, ModelFormat::Dummy);
        assert_eq!(loader.metadata().vocab_size, 32000);
    }

    #[test]
    fn test_load_nonexistent_file() {
        let result = ModelLoader::from_file("nonexistent.gguf");
        assert!(result.is_err());
    }
}
