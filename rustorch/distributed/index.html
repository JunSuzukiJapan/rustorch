<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Distributed training support for multi-GPU and multi-machine training マルチGPUおよびマルチマシン学習用分散学習サポート Distributed training support for RusTorch RusTorchの分散学習サポート"><title>rustorch::distributed - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../../static.files/rustdoc-aa0817cf.css"><meta name="rustdoc-vars" data-root-path="../../" data-static-root-path="../../static.files/" data-current-crate="rustorch" data-themes="" data-resource-suffix="" data-rustdoc-version="1.90.0 (1159e78c4 2025-09-14)" data-channel="1.90.0" data-search-js="search-fa3e91e5.js" data-settings-js="settings-5514c975.js" ><script src="../../static.files/storage-68b7e25d.js"></script><script defer src="../sidebar-items.js"></script><script defer src="../../static.files/main-eebb9057.js"></script><noscript><link rel="stylesheet" href="../../static.files/noscript-32bb7600.css"></noscript><link rel="alternate icon" type="image/png" href="../../static.files/favicon-32x32-6580c154.png"><link rel="icon" type="image/svg+xml" href="../../static.files/favicon-044be391.svg"></head><body class="rustdoc mod"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle" title="show sidebar"></button></nav><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../../rustorch/index.html">rustorch</a><span class="version">0.6.26</span></h2></div><div class="sidebar-elems"><section id="rustdoc-toc"><h2 class="location"><a href="#">Module distributed</a></h2><h3><a href="#reexports">Module Items</a></h3><ul class="block"><li><a href="#reexports" title="Re-exports">Re-exports</a></li><li><a href="#modules" title="Modules">Modules</a></li><li><a href="#structs" title="Structs">Structs</a></li><li><a href="#enums" title="Enums">Enums</a></li><li><a href="#traits" title="Traits">Traits</a></li><li><a href="#functions" title="Functions">Functions</a></li></ul></section><div id="rustdoc-modnav"><h2 class="in-crate"><a href="../index.html">In crate rustorch</a></h2></div></div></nav><div class="sidebar-resizer" title="Drag to resize sidebar"></div><main><div class="width-limiter"><rustdoc-search></rustdoc-search><section id="main-content" class="content"><div class="main-heading"><div class="rustdoc-breadcrumbs"><a href="../index.html">rustorch</a></div><h1>Module <span>distributed</span><button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../../src/rustorch/distributed/mod.rs.html#1-441">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Distributed training support for multi-GPU and multi-machine training
マルチGPUおよびマルチマシン学習用分散学習サポート
Distributed training support for RusTorch
RusTorchの分散学習サポート</p>
<p>This module provides comprehensive distributed training capabilities including:</p>
<ul>
<li>Data parallel training across multiple GPUs</li>
<li>Model parallel training for large models</li>
<li>Multi-machine cluster support</li>
<li>Communication backends (NCCL, Gloo, MPI)</li>
<li>Gradient synchronization and aggregation</li>
</ul>
<p>このモジュールは包括的な分散学習機能を提供します：</p>
<ul>
<li>複数GPU間でのデータ並列学習</li>
<li>大規模モデル向けのモデル並列学習</li>
<li>複数マシンクラスターサポート</li>
<li>通信バックエンド（NCCL、Gloo、MPI）</li>
<li>勾配同期と集約</li>
</ul>
</div></details><h2 id="reexports" class="section-header">Re-exports<a href="#reexports" class="anchor">§</a></h2><dl class="item-table reexports"><dt id="reexport.BenchmarkResults"><code>pub use multi_gpu_validation::<a class="struct" href="multi_gpu_validation/struct.BenchmarkResults.html" title="struct rustorch::distributed::multi_gpu_validation::BenchmarkResults">BenchmarkResults</a>;</code></dt><dt id="reexport.GpuDeviceInfo"><code>pub use multi_gpu_validation::<a class="struct" href="multi_gpu_validation/struct.GpuDeviceInfo.html" title="struct rustorch::distributed::multi_gpu_validation::GpuDeviceInfo">GpuDeviceInfo</a>;</code></dt><dt id="reexport.MemoryUsage"><code>pub use multi_gpu_validation::<a class="struct" href="multi_gpu_validation/struct.MemoryUsage.html" title="struct rustorch::distributed::multi_gpu_validation::MemoryUsage">MemoryUsage</a>;</code></dt><dt id="reexport.MultiGpuValidator"><code>pub use multi_gpu_validation::<a class="struct" href="multi_gpu_validation/struct.MultiGpuValidator.html" title="struct rustorch::distributed::multi_gpu_validation::MultiGpuValidator">MultiGpuValidator</a>;</code></dt><dt id="reexport.ValidationMetrics"><code>pub use multi_gpu_validation::<a class="struct" href="multi_gpu_validation/struct.ValidationMetrics.html" title="struct rustorch::distributed::multi_gpu_validation::ValidationMetrics">ValidationMetrics</a>;</code></dt><dt id="reexport.wrap_module"><code>pub use ddp::<a class="fn" href="ddp/fn.wrap_module.html" title="fn rustorch::distributed::ddp::wrap_module">wrap_module</a>;</code></dt><dt id="reexport.DistributedDataParallel"><code>pub use ddp::<a class="struct" href="ddp/struct.DistributedDataParallel.html" title="struct rustorch::distributed::ddp::DistributedDataParallel">DistributedDataParallel</a>;</code></dt><dt id="reexport.wrap_simple"><code>pub use simple_ddp::<a class="fn" href="simple_ddp/fn.wrap_simple.html" title="fn rustorch::distributed::simple_ddp::wrap_simple">wrap_simple</a>;</code></dt><dt id="reexport.SimpleDistributedDataParallel"><code>pub use simple_ddp::<a class="struct" href="simple_ddp/struct.SimpleDistributedDataParallel.html" title="struct rustorch::distributed::simple_ddp::SimpleDistributedDataParallel">SimpleDistributedDataParallel</a>;</code></dt><dt id="reexport.AsyncConfig"><code>pub use async_gradient::<a class="struct" href="async_gradient/struct.AsyncConfig.html" title="struct rustorch::distributed::async_gradient::AsyncConfig">AsyncConfig</a>;</code></dt><dt id="reexport.AsyncGradientSynchronizer"><code>pub use async_gradient::<a class="struct" href="async_gradient/struct.AsyncGradientSynchronizer.html" title="struct rustorch::distributed::async_gradient::AsyncGradientSynchronizer">AsyncGradientSynchronizer</a>;</code></dt><dt id="reexport.Priority"><code>pub use async_gradient::<a class="enum" href="async_gradient/enum.Priority.html" title="enum rustorch::distributed::async_gradient::Priority">Priority</a>;</code></dt><dt><code>pub use <a class="mod" href="api/index.html" title="mod rustorch::distributed::api">api</a>::*;</code></dt></dl><h2 id="modules" class="section-header">Modules<a href="#modules" class="anchor">§</a></h2><dl class="item-table"><dt><a class="mod" href="api/index.html" title="mod rustorch::distributed::api">api</a></dt><dd>Data parallel training module
データ並列学習モジュール
PyTorch-compatible distributed training API
PyTorch互換分散学習API</dd><dt><a class="mod" href="async_gradient/index.html" title="mod rustorch::distributed::async_gradient">async_<wbr>gradient</a></dt><dd>Asynchronous gradient synchronization system for distributed training
分散学習用非同期勾配同期システム</dd><dt><a class="mod" href="backends/index.html" title="mod rustorch::distributed::backends">backends</a></dt><dd>Communication backends for distributed training
分散学習用通信バックエンド</dd><dt><a class="mod" href="cluster/index.html" title="mod rustorch::distributed::cluster">cluster</a></dt><dd>Multi-machine cluster support for distributed training
分散学習用マルチマシンクラスターサポート</dd><dt><a class="mod" href="common/index.html" title="mod rustorch::distributed::common">common</a></dt><dd>Common utilities for distributed operations
分散操作用共通ユーティリティ</dd><dt><a class="mod" href="data_parallel/index.html" title="mod rustorch::distributed::data_parallel">data_<wbr>parallel</a></dt><dd>Data parallel training implementation
データ並列学習実装</dd><dt><a class="mod" href="ddp/index.html" title="mod rustorch::distributed::ddp">ddp</a></dt><dd>DistributedDataParallel (DDP) implementation for RusTorch
RusTorch用DistributedDataParallel（DDP）実装</dd><dt><a class="mod" href="model_parallel/index.html" title="mod rustorch::distributed::model_parallel">model_<wbr>parallel</a></dt><dd>Model parallel training implementation
モデル並列学習実装</dd><dt><a class="mod" href="multi_gpu_validation/index.html" title="mod rustorch::distributed::multi_gpu_validation">multi_<wbr>gpu_<wbr>validation</a></dt><dd>Multi-GPU validation and benchmarking for distributed training
分散学習用マルチGPU検証とベンチマーキング</dd><dt><a class="mod" href="nccl_integration/index.html" title="mod rustorch::distributed::nccl_integration">nccl_<wbr>integration</a></dt><dd>Advanced NCCL integration for high-performance distributed training
高性能分散学習のための高度NCCL統合</dd><dt><a class="mod" href="optimizer/index.html" title="mod rustorch::distributed::optimizer">optimizer</a></dt><dd>Distributed optimizers for distributed training
分散学習用分散オプティマイザー</dd><dt><a class="mod" href="performance/index.html" title="mod rustorch::distributed::performance">performance</a></dt><dd>Performance optimizations for distributed learning
分散学習のパフォーマンス最適化</dd><dt><a class="mod" href="simple_ddp/index.html" title="mod rustorch::distributed::simple_ddp">simple_<wbr>ddp</a></dt><dd>Simplified DistributedDataParallel implementation
簡略化DistributedDataParallel実装</dd></dl><h2 id="structs" class="section-header">Structs<a href="#structs" class="anchor">§</a></h2><dl class="item-table"><dt><a class="struct" href="struct.DistributedState.html" title="struct rustorch::distributed::DistributedState">Distributed<wbr>State</a></dt><dd>Distributed state management
分散状態管理</dd><dt><a class="struct" href="struct.ProcessGroup.html" title="struct rustorch::distributed::ProcessGroup">Process<wbr>Group</a></dt><dd>Process group for distributed training
分散学習用プロセスグループ</dd></dl><h2 id="enums" class="section-header">Enums<a href="#enums" class="anchor">§</a></h2><dl class="item-table"><dt><a class="enum" href="enum.DistributedBackend.html" title="enum rustorch::distributed::DistributedBackend">Distributed<wbr>Backend</a></dt><dd>Distributed backend types
分散バックエンドタイプ</dd><dt><a class="enum" href="enum.ReduceOp.html" title="enum rustorch::distributed::ReduceOp">Reduce<wbr>Op</a></dt><dd>Reduction operations for collective communications
集合通信用リダクション操作</dd></dl><h2 id="traits" class="section-header">Traits<a href="#traits" class="anchor">§</a></h2><dl class="item-table"><dt><a class="trait" href="trait.DistributedDataParallelTrait.html" title="trait rustorch::distributed::DistributedDataParallelTrait">Distributed<wbr>Data<wbr>Parallel<wbr>Trait</a></dt><dd>Common trait for distributed data parallel implementations
分散データ並列実装の共通トレイト</dd><dt><a class="trait" href="trait.DistributedOps.html" title="trait rustorch::distributed::DistributedOps">Distributed<wbr>Ops</a></dt><dd>Communication operations for distributed training
分散学習用通信操作</dd><dt><a class="trait" href="trait.DistributedScalar.html" title="trait rustorch::distributed::DistributedScalar">Distributed<wbr>Scalar</a></dt><dd>Type alias for distributed-compatible float types
分散互換フロート型の型エイリアス</dd></dl><h2 id="functions" class="section-header">Functions<a href="#functions" class="anchor">§</a></h2><dl class="item-table"><dt><a class="fn" href="fn.finalize.html" title="fn rustorch::distributed::finalize">finalize</a></dt><dd>Finalize distributed training
分散学習を終了</dd><dt><a class="fn" href="fn.get_distributed_state.html" title="fn rustorch::distributed::get_distributed_state">get_<wbr>distributed_<wbr>state</a></dt><dd>Get global distributed state
グローバル分散状態を取得</dd><dt><a class="fn" href="fn.get_rank.html" title="fn rustorch::distributed::get_rank">get_<wbr>rank</a></dt><dd>Get current rank in distributed training
分散学習での現在のランクを取得</dd><dt><a class="fn" href="fn.get_world_size.html" title="fn rustorch::distributed::get_world_size">get_<wbr>world_<wbr>size</a></dt><dd>Get world size in distributed training
分散学習でのワールドサイズを取得</dd><dt><a class="fn" href="fn.init_distributed.html" title="fn rustorch::distributed::init_distributed">init_<wbr>distributed</a></dt><dd>Initialize distributed training
分散学習を初期化</dd><dt><a class="fn" href="fn.is_available.html" title="fn rustorch::distributed::is_available">is_<wbr>available</a></dt><dd>Check if distributed training is available
分散学習が利用可能かチェック</dd></dl></section></div></main></body></html>