# Metal GPU Backend Debugging Status

**Date**: 2025-10-09
**Model**: TinyLlama-1.1B-Chat (Q8_0, Q6_K, Q5_K_M, Q4_K_M)
**Problem**: All quantization levels produce random/incorrect output on Metal backend

## ğŸ” Problem Summary

### Symptom
- **Input**: "1" (token sequence includes 29896)
- **Expected Output**: "1" (echo input)
- **Actual Output**:
  - Metal Q8_0: "regret" (token 26686) âŒ
  - CPU: "entes" (token 5326) âŒ
  - **Problem affects BOTH Metal and CPU backends**

### Key Discovery
**The issue is NOT Metal-specific** - CPU backend also fails, indicating a systematic error in the RusTorch GPT implementation itself.

## ğŸ” Critical Finding: ggml_mul_mat Memory Layout

**Date**: 2025-10-09 (continued investigation)

### ggml_mul_mat Implementation Detail

llama.cpp's `ggml_mul_mat(a, b)` does NOT compute standard `a @ b`!

**Key Discovery from ggml.c:3049**:
```c
const int64_t ne[4] = { a->ne[1], b->ne[1], b->ne[2], b->ne[3] };
```

Result shape is `{a->ne[1], b->ne[1], ...}`, meaning:
- **ggml_mul_mat(a, b) = a^T @ b** (a is implicitly transposed!)
- Example: `ggml_mul_mat(wq, cur)` with `wq=[2048,2048]`, `cur=[2048,15]`
  - Computes: `wq^T @ cur = [2048,2048] @ [2048,15] = [2048,15]`

### RusTorch vs llama.cpp Tensor Layout

| Framework | Input Shape | Weight Shape | Operation | Result Shape |
|-----------|-------------|--------------|-----------|--------------|
| llama.cpp | `[features, tokens]` = `[2048, 15]` | `[out, in]` = `[2048, 2048]` | `wq^T @ cur` | `[2048, 15]` |
| RusTorch  | `[tokens, features]` = `[15, 2048]` | `[out, in]` = `[2048, 2048]` | `x @ wq^T` | `[15, 2048]` |

**Conclusion**: RusTorch's original `x.matmul(weight)` is CORRECT for its `[tokens, features]` layout.
Attempting to use `weight.matmul(x)` causes dimension mismatch error.

### MatMul Order Investigation Results

1. **Tested**: Changed all matmul from `x.matmul(weight)` to `weight.matmul(x)`
2. **Result**: Dimension mismatch error `[2048,2048] @ [15,2048]` â†’ incompatible
3. **Reverted**: All matmul back to original `x.matmul(weight)`
4. **Status**: MatMul order is NOT the root cause âœ…

## âœ… Verified Components (All CORRECT)

### 1. RoPE Implementation
- Position tracking: 100% correct (0,1,2,3...)
- rope_idx calculation: correct (0,32,64,96...)
- **Status**: âœ… No issues found

### 2. Attention Mechanism
- Q/K/V projections: normal values
- Attention scores: proper range
- Softmax: correct normalization
- **Status**: âœ… No issues found

### 3. Metal Matrix Multiplication Kernel
- Standalone test created: `examples/test_metal_matmul.rs`
- All test cases: **Perfect match** with CPU
- **Status**: âœ… Kernel implementation is correct

### 4. Weight Data Transfer
- Q projection weights: verified (4,194,304 elements, correct values)
- Matmul parameters: correct dimensions (m=19, n=2048, k=2048)
- Q projection output: normal values (-0.0035, 0.0367, -0.0553...)
- **Status**: âœ… No issues found

### 5. FFN Layer (Layer 0, Q8_0)
- Gate weight: 11,534,336 elements, normal values
- Input (x_ln2): normal values (Â±0.14 range)
- Gate output: normal values (Â±0.09 range)
- FFN output: normal values (Â±0.006 range)
- **Status**: âœ… No issues found

### 6. LM Head & Logits Computation
- LM head weight key: `output.weight` (separate from `token_embd.weight`)
- Weight shape: [2048, 32000]
- **Manual logit calculation**:
  - Token 29896 computed: 0.7285
  - Token 29896 manual: 0.728484
  - **Difference**: 0.000000 âœ… **Perfect match**
- **Status**: âœ… Computation is correct, but logit value is wrong

### 7. GGUF Memory Layout
- Embedding layout: `[token0(2048), token1(2048), ...]` âœ…
- LM head layout verification:
  - `[[0,0]]` = 0.012437 = Token 0, element 0 âœ…
  - `[[1,0]]` = -0.035898 = Token 0, element 1 âœ…
  - `[[0,1]]` = -0.022653 = Token 1, element 0 âœ…
- **Status**: âœ… ndarray indexing is correct

## âŒ Identified Problem

### Token 29896 Logit Analysis
- **Token 29896** ("1" in input): logit = **0.7285** (very low)
- **Token 9134**: logit = 6.6563 (high)
- **Token 26686** (generated): highest logit

**Issue**: Token 29896 should have the highest logit for echo task, but has very low value.

### Root Cause Hypothesis
All individual components are correct, but **Last Hidden State points in wrong direction**:
- Last Hidden State RMS: 1.921011
- Values: [-3.71 to 3.50 range]
- Values appear normal individually but **collectively produce wrong logits**

This suggests:
1. **Cumulative error** across 22 transformer layers
2. **RMS Norm implementation** may have subtle bug
3. **Residual connections** may accumulate errors
4. **Numerical precision** issues in f32/f64 conversions

## ğŸ”¬ Investigation Evidence

### Weight Loading
```
output.weight: data_offset=1709440, tensor_offset=0
token_embd.weight: data_offset=1709440, tensor_offset=69632000
```
â†’ Separate weights (not shared)

### Last Hidden State (Metal, Q8_0)
```
Token pos=18, first 10: [0.454, 0.494, -0.380, 1.146, 1.368, ...]
RMS: 1.921011
```

### Logits Top-5 (Metal, Q8_0)
```
1. Token 24155: 8.0987
2. Token 4031: 7.8813
3. Token 26890: 7.8115
4. Token 19285: 7.7963
5. Token 3499: 7.7400

Token 29896: 0.7285 âŒ (should be top)
```

## ğŸ¯ Remaining Investigation Areas

### High Priority
1. **RMS Norm Implementation**
   - Compare with llama.cpp implementation
   - Check epsilon handling (1e-5)
   - Verify normalization formula

2. **Residual Connection Accumulation**
   - Check value clipping (currently Â±10.0)
   - Verify residual addition implementation
   - Track layer-by-layer accumulation

3. **Numerical Precision**
   - f32 vs f64 conversions
   - Quantization dequantization errors
   - Accumulation precision in long sequences

### Medium Priority
4. **Layer-by-Layer Comparison with llama.cpp**
   - Compare embedding output
   - Compare Layer 0 output
   - Identify divergence point

5. **Different Quantization Levels**
   - Test Q8_0, Q6_K, Q5_K_M, Q4_K_M
   - Check if pattern is consistent

## ğŸ“ Debug Output Examples

### FFN Layer 0 Debug Output
```
ğŸ”¶ [FFN GATE] Layer 0:
   Gate weight len=11534336, first 10: [-0.00032, 0.01166, 0.00946, ...]
   Input (x_ln2) len=38912, first 10: [0.00948, 0.00969, -0.01578, ...]
   Gate output first 10: [-0.04368, 0.01807, 0.04929, ...]

âœ… [FFN OUTPUT] Layer 0:
   FFN output first 10: [0.00371, -0.00056, -0.00070, ...]
```

### Manual Logit Verification
```
ğŸ§® [MANUAL] Token 29896 partial logit (first 10 dims): 0.040439
ğŸ§® [MANUAL] Token 29896 full logit: 0.728484
ğŸ§® [MANUAL] Difference from computed: 0.000000
```

## ğŸ”§ Code Locations

### Key Files
- `src/models/gpt.rs`: Main GPT implementation
  - Line 468-1228: `forward_metal()` function
  - Line 987-1038: FFN layer with debug output
  - Line 1140-1224: Last hidden & logits computation

### Debug Markers
- ğŸ“: Weight info
- ğŸ”§: Matmul parameters
- âœ…: Output verification
- ğŸ”¶: FFN gate projection
- ğŸ¯: Last hidden state
- ğŸ”: LM head & logits
- ğŸ§®: Manual calculations
- ğŸ§ª: Layout tests

## ğŸ“Š Test Commands

### Run with Debug Output
```bash
printf "1\n" | RUSTORCH_DEBUG=1 ./target/release/rustorch-cli \
  --model ~/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
  --backend metal --max-tokens 1
```

### Compare Backends
```bash
# Metal
--backend metal

# CPU (also fails)
--backend cpu
```

## ğŸš¨ Critical Finding

**The problem is NOT Metal-specific.** Both Metal and CPU backends produce incorrect output, suggesting a fundamental issue in the RusTorch GPT implementation that affects all backends.

The most likely culprit is:
1. **RMS Norm implementation** - subtle mathematical error
2. **Residual connection handling** - accumulation pattern
3. **Weight layout interpretation** - though tests suggest this is correct

## ğŸ“‹ Next Steps

1. **Compare RMS Norm** with llama.cpp reference implementation
2. **Add layer-by-layer output comparison** with llama.cpp
3. **Test with FP16/FP32 models** to isolate quantization effects
4. **Review residual connection** implementation carefully
5. **Check if problem exists in older commits** (git bisect)

---

## ğŸ”¬ 2025-10-09 16:56 - æ ¹æœ¬åŸå› èª¿æŸ»ã®é€²æ—

### å®Ÿæ–½ã—ãŸæ¤œè¨¼

#### âœ… 1. RMS Norm ã® hidden_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª

**æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ä½œæˆ**: `examples/verify_rms_norm_and_embeddings.rs`

**ç¢ºèªçµæœ**:
```
hidden_size (d_model): 2048 âœ…
```

**RMS Norm Weight ã®é•·ã•ç¢ºèª**:
- `blk.0.attn_norm.weight`: Shape [2048] âœ…
- `blk.0.ffn_norm.weight`: Shape [2048] âœ…  
- `output_norm.weight`: Shape [2048] âœ…

**ã‚³ãƒ¼ãƒ‰ç¢ºèª**:
- `src/models/gpt.rs` Line 612, 1020, 1240 ã§ `rms_norm_f32()` ã‚’å‘¼ã³å‡ºã—
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: `d_model` (= 2048) ãŒ `hidden_size` ã¨ã—ã¦æ­£ã—ãæ¸¡ã•ã‚Œã¦ã„ã‚‹
- RMS Norm å®Ÿè£… (Line 1437-1520): `hidden_size` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ­£ã—ãä½¿ç”¨

**çµè«–**: RMS Norm ã® hidden_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ **2048 ã§æ­£ã—ã„** âœ…

#### âœ… 2. Token Embedding å€¤ã®ç¢ºèª

**Token 29896 ("1") ã®åŸ‹ã‚è¾¼ã¿**:
```
æœ€åˆã®10è¦ç´ :
[-0.005837917, -0.003361225, 0.000353813, 0.022467136, -0.004953384,
 -0.000707626, -0.000530720, 0.006014824, 0.000176907, 0.001238346]

çµ±è¨ˆ:
- mean: -0.000102335
- rms: 0.008698901
- ç¯„å›²: [-0.077635765, 0.075213432]
```

**llama.cpp ã¨ã®æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†**:
- Token 29896, 1, 2, 0 (BOS) ã®åŸ‹ã‚è¾¼ã¿ã‚’æœ€åˆã®20è¦ç´ ã¾ã§ãƒ€ãƒ³ãƒ—
- æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: llama.cpp ã§åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã—ã¦æ¯”è¼ƒ

#### ğŸ”„ 3. Layer 0 é‡ã¿ç¢ºèªï¼ˆé€²è¡Œä¸­ï¼‰

**æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ä½œæˆ**: `examples/dump_layer0_output.rs`

**Layer 0 ã®å…¨é‡ã¿ç¢ºèªå®Œäº†**:
- Attention RMS Norm: [2048] âœ…
- Query projection: [2048, 2048] âœ…
- Key projection: [2048, 256] âœ…
- Value projection: [2048, 256] âœ…
- Attention output: [2048, 2048] âœ…
- FFN RMS Norm: [2048] âœ…
- FFN gate: [2048, 5632] âœ…
- FFN up: [2048, 5632] âœ…
- FFN down: [5632, 2048] âœ…

ã™ã¹ã¦ã®é‡ã¿ã®å½¢çŠ¶ãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèªã€‚

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå„ªå…ˆé †ä½é †ï¼‰

1. **llama.cpp ã¨ã® Token Embedding æ¯”è¼ƒ**
   - llama.cpp ã§ Token 29896 ã®åŸ‹ã‚è¾¼ã¿ã‚’ãƒ€ãƒ³ãƒ—
   - RusTorch ã®å€¤ã¨è¦ç´ ã”ã¨ã«æ¯”è¼ƒ
   - å·®ç•°ãŒã‚ã‚Œã°ã€GGUFèª­ã¿è¾¼ã¿ã®å•é¡Œã‚’èª¿æŸ»

2. **llama.cpp ã¨ã® Layer 0 å‡ºåŠ›æ¯”è¼ƒ**
   - llama.cpp ã§ Layer 0 å‡ºåŠ›ï¼ˆAttention + FFN å¾Œï¼‰ã‚’ãƒ€ãƒ³ãƒ—
   - RusTorch ã® Layer 0 å‡ºåŠ›ã¨è¦ç´ ã”ã¨ã«æ¯”è¼ƒ
   - å·®ç•°ãŒã‚ã‚‹å ´åˆã€ä»¥ä¸‹ã‚’é †ã«èª¿æŸ»:
     - RMS Norm (Attentionå‰)
     - Attention è¨ˆç®—
     - RMS Norm (FFNå‰)
     - FFN è¨ˆç®—

3. **RMS Norm å®Ÿè£…ã®è©³ç´°æ¤œè¨¼**
   - llama.cpp ã® RMS Norm å®Ÿè£…ã¨æ•°å¼ãƒ¬ãƒ™ãƒ«ã§æ¯”è¼ƒ
   - epsilon å€¤ (1e-5) ã®æ‰±ã„ã‚’ç¢ºèª
   - æ•°å€¤ç²¾åº¦ (f32 vs f64) ã®å½±éŸ¿ã‚’èª¿æŸ»

### ä½œæˆã—ãŸãƒ„ãƒ¼ãƒ«

- `examples/verify_rms_norm_and_embeddings.rs`: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨é‡ã¿ã®æ¤œè¨¼
- `examples/dump_layer0_output.rs`: Layer 0 å‡ºåŠ›ã®ãƒ€ãƒ³ãƒ—ï¼ˆæº–å‚™ä¸­ï¼‰

### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

```bash
# RMS Norm ã¨ Embedding ã®æ¤œè¨¼
cargo run --release --example verify_rms_norm_and_embeddings -- \
  ~/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q8_0.gguf

# Layer 0 å‡ºåŠ›ã®ãƒ€ãƒ³ãƒ—
cargo run --release --example dump_layer0_output -- \
  ~/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q8_0.gguf "1"
```

---

## ğŸš¨ 2025-10-09 19:30 - æ ¹æœ¬åŸå› ã‚’ç‰¹å®šï¼

### æ±ºå®šçš„ãªç™ºè¦‹

#### âŒ Metal GPU Backend ã®å•é¡Œã§ã¯ãªã„
**ä¸¡æ–¹ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ä¸æ­£è§£ã‚’å‡ºåŠ›**ï¼š
- **hybrid_f32**: "1" â†’ "Ñ‚Ğ¸Ğ²" (token 3499) âŒ
- **Metal**: "1" â†’ "entes" (token 5326) âŒ
- **llama.cpp**: "1" â†’ "1" (token 29896) âœ…

â†’ **RusTorch å®Ÿè£…å…¨ä½“ã®å•é¡Œ**

#### ğŸ”´ RMS Norm å‡ºåŠ›ã®ç•°å¸¸å€¤ã‚’æ¤œå‡º

**hybrid_f32 ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›**:
```
Before attn RMSNorm: input rms=0.009337, max=0.087731
attn_norm.weight stats: rms=0.046377, max=0.769531
After attn RMSNorm: rms=0.099257, max=4.536953
```

**æ•°å€¤åˆ†æ**:
- Weight RMS: 0.046377
- **æœŸå¾…ã•ã‚Œã‚‹ Output RMS**: â‰ˆ 0.046 (æ­£è¦åŒ–å¾Œã®RMS â‰ˆ 1.0 ãªã®ã§)
- **å®Ÿéš›ã® Output RMS**: 0.099257
- **æ¯”ç‡**: **2.14å€å¤§ãã„** âŒ

**å‡ºåŠ›ã®æœ€å¤§å€¤**: 4.536953
- ã“ã‚Œã¯ Weight max (0.769531) ã®ç´„6å€ï¼
- æ­£å¸¸ãª RMS Norm ã§ã¯èµ·ã“ã‚Šãˆãªã„å€¤

#### âœ… Metal vs hybrid_f32 Attention å‡ºåŠ›æ¯”è¼ƒ

**Attention Output (before output projection) ã® RMS**:
| Backend | RMS | æ¯”ç‡ |
|---------|-----|------|
| hybrid_f32 | 0.028690 | **3.34å€** |
| Metal | 0.008590 | 1.00å€ |

â†’ hybrid_f32 ã® Attention å‡ºåŠ›ãŒ Metal ã® **3.3å€å¤§ãã„**ï¼

### æ¤œè¨¼æ¸ˆã¿æ­£å¸¸å‹•ä½œï¼ˆå†ç¢ºèªï¼‰

1. âœ… Metal Transposed Matmul Kernel: ç›¸å¯¾èª¤å·® 6.86e-8
2. âœ… Q4_K Dequantization: ç›¸å¯¾èª¤å·® 7e-5ã€è«–ç†çš„ã«æ­£ã—ã„
3. âœ… Q6_K Dequantization: llama.cpp ã¨å®Ÿè£…ä¸€è‡´
4. âœ… F32 èª­ã¿è¾¼ã¿: `f32::from_le_bytes` ã§æ­£ã—ã„
5. âœ… Softmax: åˆè¨ˆ â‰ˆ 1.0
6. âœ… RMS Norm æ•°å¼: `output = (input / rms) * weight` æ­£ã—ã„
7. âœ… Output Projection 2.5å€å¢—å¹…: æ­£å¸¸ãªç·šå½¢å¤‰æ›

### æ ¹æœ¬åŸå› ã®ä»®èª¬

#### æœ€æœ‰åŠ›å€™è£œï¼šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸
RMS Norm ã®å®Ÿè£…ã¯æ­£ã—ã„ãŒã€**å…¥åŠ›ãƒ‡ãƒ¼ã‚¿è‡ªä½“ãŒæ—¢ã«ç•°å¸¸**ã®å¯èƒ½æ€§ï¼š

1. **Token Embedding ã®å€¤ãŒé–“é•ã£ã¦ã„ã‚‹**
   - GGUF ã‹ã‚‰ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼
   - ãƒã‚¤ãƒˆã‚ªãƒ¼ãƒ€ãƒ¼ã®å•é¡Œ
   - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãšã‚Œ

2. **Position Embedding ãŒåŠ ç®—ã•ã‚Œã¦ã„ã‚‹**
   - TinyLlama ã¯ RoPE ã®ã¿ã§ Position Embedding ã¯ä½¿ã‚ãªã„ã¯ãš
   - èª¤ã£ã¦è¿½åŠ ã® embedding ãŒè¶³ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§

3. **åˆæœŸæ­£è¦åŒ–ã®æ¬ è½**
   - Embedding å¾Œã«ä½•ã‹æ­£è¦åŒ–ãŒå¿…è¦ï¼Ÿ
   - llama.cpp ã¨å‡¦ç†é †åºãŒç•°ãªã‚‹ï¼Ÿ

#### ãã®ä»–ã®å¯èƒ½æ€§

4. **RMS è¨ˆç®—ã®åˆ†æ¯ã‚¨ãƒ©ãƒ¼**
   - `hidden_size` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ 2048 ã§æ­£ã—ã„ï¼ˆæ¤œè¨¼æ¸ˆã¿ï¼‰
   - ã—ã‹ã—å®Ÿè¡Œæ™‚ã«åˆ¥ã®å€¤ãŒæ¸¡ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ï¼Ÿ

5. **Weight é©ç”¨ã®é‡è¤‡**
   - Weight ãŒ2å›é©ç”¨ã•ã‚Œã¦ã„ã‚‹ï¼Ÿ
   - ã©ã“ã‹ã§ãƒ«ãƒ¼ãƒ—ãƒŸã‚¹ï¼Ÿ

### æ¬¡ã®èª¿æŸ»ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå„ªå…ˆé †ä½é †ï¼‰

#### ğŸ”¥ æœ€å„ªå…ˆï¼šToken Embedding å€¤ã®ç›´æ¥æ¯”è¼ƒ
```bash
# 1. llama.cpp ã§ Token 29896 ã® embedding ã‚’ãƒ€ãƒ³ãƒ—
# 2. RusTorch ã® Token 29896 embedding ã¨æ¯”è¼ƒ
# 3. å€¤ãŒä¸€è‡´ã—ãªã„å ´åˆã€GGUF èª­ã¿è¾¼ã¿ã‚’èª¿æŸ»
```

#### ğŸ”¥ é«˜å„ªå…ˆï¼šRMS Norm å…¥åŠ›å€¤ã®ãƒˆãƒ¬ãƒ¼ã‚¹
```rust
// RMS Norm é–¢æ•°ã®å…ˆé ­ã«è¿½åŠ 
if debug && seq_idx == 0 {
    eprintln!("RMS Norm input[0..10]: {:?}", &row[0..10]);
    eprintln!("RMS calculation: sum={}, mean_sq={}, rms={}",
              sum, mean_sq, rms);
}
```

#### ğŸ”¥ é«˜å„ªå…ˆï¼šllama.cpp ã¨ã® Layer 0 å®Œå…¨æ¯”è¼ƒ
1. Token Embedding å‡ºåŠ›
2. Layer 0 Attention RMS Norm å‡ºåŠ›
3. Layer 0 Attention å‡ºåŠ›
4. Layer 0 FFN RMS Norm å‡ºåŠ›
5. Layer 0 FFN å‡ºåŠ›
6. Layer 0 æœ€çµ‚å‡ºåŠ›

å„ã‚¹ãƒ†ãƒƒãƒ—ã§å€¤ã‚’æ¯”è¼ƒã—ã€æœ€åˆã«ç™ºæ•£ã™ã‚‹å ´æ‰€ã‚’ç‰¹å®šã€‚

### è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ

ä»Šå›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ä½œæˆã—ãŸè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆï¼š
- `/tmp/metal_vs_hybrid_comparison.md` - Metal ã¨ hybrid_f32 ã®æ¯”è¼ƒ
- `/tmp/FINAL_DIAGNOSIS.md` - æœ€çµ‚è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ
- `/tmp/ROOT_CAUSE_IDENTIFIED.md` - æ ¹æœ¬åŸå› ã®è©³ç´°åˆ†æ

### æŠ€è¡“ãƒ¡ãƒ¢

**RMS Norm ã®æœŸå¾…å‹•ä½œ**:
```
Normalized Input RMS â‰ˆ 1.0
Output RMS â‰ˆ Weight RMS
```

**å®Ÿéš›ã®å‹•ä½œï¼ˆç•°å¸¸ï¼‰**:
```
Input RMS: 0.009337
Weight RMS: 0.046377
Output RMS: 0.099257 âŒ (Weight RMS ã® 2.14å€!)
Output Max: 4.536953 âŒ (ç•°å¸¸ã«å¤§ãã„)
```

ã“ã‚Œã¯æ˜ã‚‰ã‹ã« **RMS Norm ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ Weight ãƒ‡ãƒ¼ã‚¿ã«å•é¡ŒãŒã‚ã‚‹** ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

---

## ğŸ‰ 2025-10-09 21:00 - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®2ã¤ã®é‡å¤§ãƒã‚°ã‚’ä¿®æ­£

### ğŸ› Bug 1: ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä¸ä¸€è‡´

**å ´æ‰€**: `example-cli/src/model/inference.rs:103`

**å•é¡Œ**:
- **ä¿®æ­£å‰**: `"<|user|>\n{}</s>\n<|assistant|>\n"` â†’ 19ãƒˆãƒ¼ã‚¯ãƒ³
- **ä¿®æ­£å¾Œ**: `"<|user|>\n{}<|assistant|>"` â†’ 15ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆllama.cppã¨ä¸€è‡´ï¼‰

**å½±éŸ¿**: llama.cppã¨ç•°ãªã‚‹ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ãŸãŸã‚ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ãŒç•°ãªã£ã¦ã„ãŸã€‚

### ğŸ› Bug 2: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ç©ºç™½æ–‡å­—å‡¦ç†ï¼ˆ**æ ¹æœ¬åŸå› **ï¼‰

**å ´æ‰€**: `example-cli/src/tokenizer/llama_spm.rs:97-98`

**å•é¡Œ**:
```rust
// ä¿®æ­£å‰ï¼ˆé–“é•ã„ï¼‰
.map(|c| if c.is_whitespace() { 'â–' } else { c })
```

ã“ã®å®Ÿè£…ã¯**ã™ã¹ã¦ã®ç©ºç™½æ–‡å­—**ï¼ˆ`\n`, `\t`, `\r`ã‚’å«ã‚€ï¼‰ã‚’ SentencePiece ã®ã‚¹ãƒšãƒ¼ã‚¹ãƒãƒ¼ã‚«ãƒ¼ 'â–' ã«ç½®æ›ã—ã¦ã„ãŸã€‚

**çµæœ**:
- '\n' (byte 10) â†’ 'â–' â†’ **ãƒˆãƒ¼ã‚¯ãƒ³ 29871** (ã‚¹ãƒšãƒ¼ã‚¹) âŒ
- llama.cpp ã§ã¯: '\n' â†’ **ãƒˆãƒ¼ã‚¯ãƒ³ 13** (æ”¹è¡Œ) âœ…

**ä¿®æ­£**:
```rust
// ä¿®æ­£å¾Œï¼ˆæ­£ã—ã„ï¼‰
.map(|c| if c == ' ' { 'â–' } else { c })
```

ã‚¹ãƒšãƒ¼ã‚¹ ' ' ã®ã¿ã‚’ 'â–' ã«ç½®æ›ã—ã€æ”¹è¡Œãªã©ã®åˆ¶å¾¡æ–‡å­—ã¯ãã®ã¾ã¾ä¿æŒã€‚

### âœ… æ¤œè¨¼çµæœ

#### Python ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆæœŸå¾…å€¤ï¼‰
```
ãƒ†ã‚­ã‚¹ãƒˆ: "<|user|>\n1<|assistant|>"
ãƒˆãƒ¼ã‚¯ãƒ³: [1, 529, 29989, 1792, 29989, 29958, 13, 29896, 29966, 29989, 465, 22137, 29989, 29958]
                                      ^^ ãƒˆãƒ¼ã‚¯ãƒ³ 13 ('\n')
```

#### RusTorchï¼ˆä¿®æ­£å‰ï¼‰
```
ãƒˆãƒ¼ã‚¯ãƒ³: [1, 529, 29989, 1792, 29989, 29958, 29871, 29896, 29966, 29989, 465, 22137, 29989, 29958, 2]
                                      ^^^^^ ãƒˆãƒ¼ã‚¯ãƒ³ 29871 (' ') - é–“é•ã„ï¼
```

#### RusTorchï¼ˆä¿®æ­£å¾Œï¼‰
```
ãƒˆãƒ¼ã‚¯ãƒ³: [1, 529, 29989, 1792, 29989, 29958, 13, 29896, 29966, 29989, 465, 22137, 29989, 29958, 2]
                                      ^^ ãƒˆãƒ¼ã‚¯ãƒ³ 13 ('\n') - æ­£ã—ã„ï¼
```

#### llama.cpp
```
ãƒˆãƒ¼ã‚¯ãƒ³: [1, 529, 29989, 1792, 29989, 29958, 13, 29896, 29966, 29989, 465, 22137, 29989, 29958]
```

**å·®ç•°**: RusTorch ã¯æœ«å°¾ã«ãƒˆãƒ¼ã‚¯ãƒ³ 2 (EOS) ã‚’è¿½åŠ ï¼ˆ`add_special_tokens=true` ã®ãŸã‚ï¼‰

### ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ¯”è¼ƒè¡¨

| Position | RusTorch (ä¿®æ­£å¾Œ) | llama.cpp | ä¸€è‡´ |
|----------|------------------|-----------|------|
| 0 | 1 (BOS) | 1 (BOS) | âœ… |
| 1 | 529 ('<') | 529 | âœ… |
| 2 | 29989 ('\|') | 29989 | âœ… |
| 3 | 1792 ('user') | 1792 | âœ… |
| 4 | 29989 ('\|') | 29989 | âœ… |
| 5 | 29958 ('>') | 29958 | âœ… |
| 6 | **13 ('\n')** | 13 | âœ… |
| 7 | 29896 ('1') | 29896 | âœ… |
| 8 | 29966 ('<') | 29966 | âœ… |
| 9 | 29989 ('\|') | 29989 | âœ… |
| 10 | 465 ('ass') | 465 | âœ… |
| 11 | 22137 ('istant') | 22137 | âœ… |
| 12 | 29989 ('\|') | 29989 | âœ… |
| 13 | 29958 ('>') | 29958 | âœ… |
| 14 | 2 (EOS) | - | âš ï¸  |

### ğŸ“‹ ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«

1. `example-cli/src/model/inference.rs`
   - Line 103: ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¿®æ­£

2. `example-cli/src/tokenizer/llama_spm.rs`
   - Lines 96-98: ç©ºç™½æ–‡å­—å‡¦ç†ä¿®æ­£
   - Lines 277-298: ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›è¿½åŠ 

### âŒ æ®‹ã£ã¦ã„ã‚‹å•é¡Œ

**å‡ºåŠ›ã¯ä¾ç„¶ã¨ã—ã¦ä¸æ­£è§£**:
- hybrid_f32 backend: "cogn" (token 25323) âŒ
- æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: llama.cpp ã¨åŒæ§˜ã®æ­£ã—ã„å¿œç­”

**ã“ã‚ŒãŒè¨¼æ˜ã™ã‚‹ã“ã¨**:
- âœ… ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šæ­£ã—ã„
- âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼šæ­£ã—ã„ï¼ˆllama.cppã¨å®Œå…¨ä¸€è‡´ï¼‰
- âŒ **å•é¡Œã¯æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³å†…éƒ¨**ï¼ˆToken Embeddingã€RMS Normã€RoPEã€Attentionç­‰ï¼‰

### ğŸ”¬ æ¬¡ã®èª¿æŸ»ã‚¹ãƒ†ãƒƒãƒ—

ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ­£ã—ããªã£ãŸãŸã‚ã€llama.cppã¨ã®å…¬å¹³ãªæ¯”è¼ƒãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚

#### 1. Token Embedding å€¤ã®æ¯”è¼ƒï¼ˆæœ€å„ªå…ˆï¼‰
```bash
# llama.cpp ã§ Token 0 (BOS, ID=1) ã® embedding ã‚’ãƒ€ãƒ³ãƒ—
# RusTorch ã®å€¤ã¨è¦ç´ ã”ã¨ã«æ¯”è¼ƒ
```

**RusTorch ã® Token 0 embeddingï¼ˆæ—¢ã«å–å¾—æ¸ˆã¿ï¼‰**:
```
Token 0 (ID=1): [-0.001300097, 0.001904249, -0.001940966, ...]
Stats: mean=0.000028282, rms=0.002229018
```

#### 2. RoPE å®Ÿè£…ã®æ¤œè¨¼
- llama.cpp ã® RoPE å®Ÿè£…ã¨æ¯”è¼ƒ
- ä½ç½®ä¾å­˜ã®ãŸã‚ã€æœ€æœ‰åŠ›å€™è£œ

#### 3. Attention Q/K/V è¨ˆç®—ã®æ¤œè¨¼
- Layer 0 ã® Q/K/V projection å‡ºåŠ›ã‚’ llama.cpp ã¨æ¯”è¼ƒ
- Attention scores ã®ä¸­é–“å€¤ã‚’æ¯”è¼ƒ

### ğŸ“ é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

ä»Šå›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ä½œæˆã—ãŸè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆï¼š
- `/tmp/CHAT_TEMPLATE_AND_TOKENIZER_FIX.md` - ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®æ­£ã®è©³ç´°
- `/tmp/TOKENIZER_FIX_RESULT.md` - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¿®æ­£ã®æ¤œè¨¼çµæœ

### ğŸ“ é‡è¦ãªå­¦ã³

**SentencePiece ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ«ãƒ¼ãƒ«**:
- ã‚¹ãƒšãƒ¼ã‚¹ ' ' ã®ã¿ã‚’ 'â–' (U+2581) ã«ç½®æ›
- **æ”¹è¡Œ '\n'ã€ã‚¿ãƒ– '\t'ã€ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ '\r' ã¯ç½®æ›ã—ãªã„**
- ã“ã‚Œã‚‰ã®åˆ¶å¾¡æ–‡å­—ã«ã¯å°‚ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³IDãŒå­˜åœ¨ã™ã‚‹
- ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯æ”¹è¡Œã‚’æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ã“ã®é•ã„ã¯è‡´å‘½çš„

---

## ğŸ” Attention Mask & Tensor Reshape Investigation

**Date**: 2025-10-09 (continued from tokenizer fix)

### llama.cpp's Q/K/V Tensor Reshaping

**Key Finding from llama.cpp/src/llama-model.cpp:6482-6484**:
```cpp
Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);
Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);
Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);
```

llama.cpp reshapes Q/K/V tensors to 3D **BEFORE** applying RoPE:
- Shape: `[head_dim, n_heads, n_tokens]`
- Memory layout (row-major): All dims of head0_token0, all dims of head0_token1, ..., all dims of head1_token0, ...

### RusTorch's Current Approach

RusTorch keeps Q/K/V as 2D tensors when passed to RoPE:
- Shape: `[seq_len, num_heads * head_dim]`
- Memory layout: token0_head0_dims, token0_head1_dims, ..., token1_head0_dims, token1_head1_dims, ...

**RusTorch's apply_rope** (llama.rs:387-435) handles 2D layout by:
```rust
for token_idx in 0..seq_len {
    for head_idx in 0..num_heads {
        let head_offset = token_idx * total_dim + head_idx * head_dim;
        // Apply RoPE rotation
    }
}
```

### Analysis

**Question**: Does the different layout cause incorrect results?

**Hypothesis**: NO - the layouts are equivalent for RoPE application IF the iteration order is correct.
- llama.cpp: Iterates through 3D tensor `[head_dim][heads][tokens]`
- RusTorch: Iterates through 2D tensor as `[tokens][heads][dims]`
- Both apply the same RoPE rotation per (position, head, dim_pair)

**Verification Needed**: Check if RoPE output values match between llama.cpp and RusTorch.

### Attention Mask Implementation

**RusTorch** (llama.rs:492-509): Implements causal masking implicitly
```rust
// Query at position q_pos can only attend to keys at positions 0..=current_kv_pos
let current_kv_pos = (cached_len + q_pos).min(total_kv_len - 1);
for kv_pos in 0..=current_kv_pos {
    // Compute attention score
}
```

**llama.cpp** (ggml.c:3736-3815): Uses `ggml_diag_mask_inf` for causal masking
- Applied as explicit mask tensor in `ggml_soft_max`
- Sets future positions to `-inf` before softmax

Both implementations achieve causal masking, but through different mechanisms:
- RusTorch: Loop only over valid positions (implicit masking)
- llama.cpp: Explicit mask tensor with `-inf` values

**Status**: Attention mask implementation appears equivalent - both prevent attending to future tokens.

### Investigation Results

1. âœ… Weight transpose: NOT needed (confirmed)
2. âœ… Attention mask: Implementation correct (implicit vs explicit, both valid)
3. âš ï¸  Tensor reshape: Different layouts but potentially equivalent IF iteration order is correct
4. â“ Root cause: Still unidentified - need numerical comparison with llama.cpp

### Next Investigation Areas

Based on previous findings and current state:

1. **Q/K/V Reshape & RoPE Interaction** (Medium priority)
   - Verify RoPE applies correct rotations despite 2D vs 3D layout difference
   - Compare numerical outputs after RoPE between implementations

2. **Grouped Query Attention** (High priority)
   - Verify GQA implementation matches llama.cpp's approach
   - Check head grouping and KV reuse logic

3. **Softmax Numerical Stability** (Medium priority)
   - Current implementation uses standard max subtraction
   - Verify no precision issues

4. **Attention Score Computation** (High priority)
   - Verify QÂ·K^T computation is correct
   - Check scaling factor (1/âˆšd_k)

---

## âœ… 2025-10-10 - RoPEå®Ÿè£…ã®å®Œå…¨æ¤œè¨¼å®Œäº†

### RoPE Frequency Precomputation æ¤œè¨¼çµæœ

**ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›è¿½åŠ å ´æ‰€**: `src/hybrid_f32/models/llama.rs:138-173`

**æ¤œè¨¼å†…å®¹**:
- `precompute_rope_frequencies` é–¢æ•°ã«ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’è¿½åŠ 
- head_dim=64, max_seq_len=2048, theta=10000.0 âœ…

**Frequency Precomputation çµæœ**:
```
Position 0 (æœ€åˆã®3ãƒšã‚¢):
  pos=0, i=0, freq=1.000000000, angle=0.000000000, cos=1.000000000, sin=0.000000000
  pos=0, i=1, freq=0.870550573, angle=0.000000000, cos=1.000000000, sin=0.000000000
  pos=0, i=2, freq=0.757858276, angle=0.000000000, cos=1.000000000, sin=0.000000000
```

**é…åˆ—ç¢ºèª**:
```
Index 0-31 (pos=0, å…¨å‘¨æ³¢æ•°): cos=1.0, sin=0.0 âœ… (angle=0ã®ãŸã‚æ­£ã—ã„)
Index 32-41 (pos=1, æœ€åˆã®å‘¨æ³¢æ•°): cos=[0.5403023, 0.731761, 0.84600914, ...] âœ…
```

**çµè«–**: âœ… RoPEå‘¨æ³¢æ•°ã®äº‹å‰è¨ˆç®—ã¯å®Œå…¨ã«æ­£ã—ã„

### RoPE Application æ¤œè¨¼çµæœ

**ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›è¿½åŠ å ´æ‰€**: `src/hybrid_f32/models/llama.rs:387-461`

**æ¤œè¨¼å†…å®¹**:
- `apply_rope` é–¢æ•°ã«è©³ç´°ãªãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’è¿½åŠ 
- Token 0ã¨Token 1ã®å›è»¢å‡¦ç†ã‚’æ¤œè¨¼

**Token 0 (position=0) ã®æ¤œè¨¼**:
```
ğŸŒ€ [RoPE DETAIL] token=0, head=0, pair=0, pos=0, rope_idx=0
  cos=1.000000000, sin=0.000000000
  input:  x0=0.009036371, x1=-0.193953320
  output: rot0=0.009036371, rot1=-0.193953320
```
â†’ **æ’ç­‰å¤‰æ›** (rotation ãªã—) âœ… æ•°å­¦çš„ã«æ­£ã—ã„

**Token 1 (position=1) ã®æ¤œè¨¼**:
```
ğŸŒ€ [RoPE DETAIL] token=1, head=0, pair=0, pos=1, rope_idx=32
  cos=0.540302277, sin=0.841470957
  input:  x0=0.009036371, x1=-0.193953320
  output: rot0=0.168088451, rot1=-0.097189575
```
â†’ **æ­£ã—ãå›è»¢é©ç”¨** âœ… å…¥åŠ›ã¨å‡ºåŠ›ãŒç•°ãªã‚Šã€å›è»¢ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹

**æ•°å­¦çš„æ¤œè¨¼**:
```
Position 0: angle = 0 * freq = 0
  â†’ cos(0) = 1.0, sin(0) = 0.0
  â†’ [x0, x1] * [[1, 0], [0, 1]] = [x0, x1] (æ’ç­‰å¤‰æ›)

Position 1: angle = 1 * freq â‰  0
  â†’ cos â‰ˆ 0.540, sin â‰ˆ 0.841
  â†’ å®Ÿéš›ã«å›è»¢ãŒé©ç”¨ã•ã‚Œã‚‹
```

**çµè«–**: âœ… RoPEå®Ÿè£…ã¯å®Œå…¨ã«æ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹

### RoPE 2D vs 3D Layout åˆ†æ

**llama.cpp**: 3D tensor `[head_dim, n_heads, n_tokens]`
**RusTorch**: 2D tensor `[tokens, heads * head_dim]`

**åˆ†æçµæœ**:
- ä¸¡æ–¹ã¨ã‚‚åŒã˜ (position, head, dim_pair) ã«å¯¾ã—ã¦åŒã˜å›è»¢ã‚’é©ç”¨
- ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒç•°ãªã‚‹ã ã‘ã§ã€æ•°å­¦çš„ã«ã¯ç­‰ä¾¡
- RusTorchã®å®Ÿè£…ã¯æ­£ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹

**æ¤œè¨¼æ–¹æ³•**:
```rust
// RusTorchã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—
let head_offset = token_idx * total_dim + head_idx * head_dim;
let rope_idx = position * (head_dim / 2) + i;
```

**çµè«–**: âœ… 2D/3D layoutã®é•ã„ã¯å•é¡Œã§ã¯ãªã„

### ç·åˆçµè«–

**RoPEé–¢é€£ã®æ¤œè¨¼**:
1. âœ… Frequency precomputation - æ­£ã—ã„
2. âœ… Position tracking - æ­£ã—ã„ (0, 1, 2, ...)
3. âœ… Rotation application - æ­£ã—ã„ (token=0ã¯æ’ç­‰å¤‰æ›ã€token=1+ã¯å›è»¢é©ç”¨)
4. âœ… 2D vs 3D layout - ç­‰ä¾¡ã€å•é¡Œãªã—

**ä¸æ­£è§£å‡ºåŠ›ã®åŸå› **:
RoPEã¯å®Œå…¨ã«æ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ãŸã‚ã€**å•é¡Œã¯ä»–ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚ã‚‹**:
- Attentionè¨ˆç®— (QK^T, softmax, weighted sum)
- FFNè¨ˆç®—
- æœ€çµ‚logitsè¨ˆç®—

### æ¬¡ã®èª¿æŸ»å¯¾è±¡ï¼ˆå„ªå…ˆé †ä½é †ï¼‰

1. **Attentionè¨ˆç®—ã®è©³ç´°æ¤œè¨¼** (æœ€å„ªå…ˆ)
   - QK^Tè¨ˆç®—ã®æ•°å€¤ç¢ºèª
   - Softmaxå‡ºåŠ›ã®æ¤œè¨¼
   - Attention weightsåˆ†å¸ƒã®ç¢ºèª

2. **FFNè¨ˆç®—ã®æ¤œè¨¼**
   - Gate/Up projection
   - SiLU activation
   - Down projection

3. **llama.cppã¨ã®å±¤åˆ¥æ¯”è¼ƒ**
   - Layer 0å‡ºåŠ›
   - Layer 11å‡ºåŠ›
   - æœ€çµ‚å‡ºåŠ›

---

## ğŸ” 2025-10-10 - Attentionè¨ˆç®—ã®æ¤œè¨¼

### Attentionè¨ˆç®—ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›è¿½åŠ 

**è¿½åŠ å ´æ‰€**: `src/hybrid_f32/models/llama.rs:491-593`

**è¿½åŠ ã—ãŸå‡ºåŠ›**:
1. GQAå‘¼ã³å‡ºã—æƒ…å ± (Line 492-493)
2. Attention scoresè©³ç´° (Line 544-573)
3. GQAå‡ºåŠ› (Line 590-591)

### Attentionè¨ˆç®—ã®æ¤œè¨¼çµæœ

**ãƒ†ã‚¹ãƒˆ**: hybrid-f32 backend, Q4_K_M model, input "1"

**Layer 0 Attention (æœ€åˆã®æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼)**:
```
ğŸ’« [ATTENTION] q_pos=0, head=0, kv_head=0, num_scores=1
  Raw scores: min=0.000019, max=0.000019, first_5=[1.8768282e-5]
  Exp scores: first_5=[1.0]
  Sum of exp: 1.000000000
  Attention weights: sum=1.000000000, first_5=[1.0]

ğŸ’« [ATTENTION] q_pos=0, head=0, kv_head=0, num_scores=1
  Raw scores: min=0.000649, max=0.000649, first_5=[0.0006485885]
  Exp scores: first_5=[1.0]
  Sum of exp: 1.000000000
  Attention weights: sum=1.000000000, first_5=[1.0]
```

### åˆ†æ

**âœ… æ­£å¸¸ãªå‹•ä½œ**:
1. `num_scores=1`: q_pos=0ã¯è‡ªåˆ†è‡ªèº«ã®ã¿ã«attendã™ã‚‹ï¼ˆcausal maskingæ­£ã—ã„ï¼‰
2. `Attention weights: [1.0]`: num_scores=1ã®å ´åˆã€softmaxã¯å¿…ãš [1.0] ã‚’è¿”ã™ï¼ˆæ­£ã—ã„ï¼‰
3. `Sum of exp: 1.000000000`: Softmaxæ­£è¦åŒ–ãŒæ­£ã—ãæ©Ÿèƒ½

**â“ è¦èª¿æŸ»**:
1. **Raw scoresãŒå°ã•ã™ãã‚‹**:
   - Layer 0: 0.000019, 0.000649
   - ã“ã‚Œã¯ QÂ·K^T / sqrt(head_dim) ã®çµæœ
   - head_dim=64ãªã®ã§ã€scaling factor = 1/8
   - QÂ·K^T ã®ç”Ÿã®å€¤ãŒéå¸¸ã«å°ã•ã„å¯èƒ½æ€§

**æ¬¡ã®èª¿æŸ»**:
- Q/K projectionå¾Œã®å€¤ã‚’ç¢ºèª
- QÂ·K^T ã®ç”Ÿã®å†…ç©å€¤ã‚’ç¢ºèªï¼ˆscalingå‰ï¼‰
- llama.cppã¨æ¯”è¼ƒã—ã¦ã€åŒã˜ç¯„å›²ã‹ç¢ºèª

### çµè«–

Attentionè¨ˆç®—ã®**æ§‹é€ ã¯æ­£ã—ã„**:
- Causal masking: âœ…
- Softmaxè¨ˆç®—: âœ…
- Attention weightsæ­£è¦åŒ–: âœ…

ãŸã ã—ã€**æ•°å€¤ã®ç¯„å›²**ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚‹:
- Raw attention scoresãŒæœŸå¾…ã•ã‚Œã‚‹ç¯„å›²å†…ã‹
- Q/K projectionå‡ºåŠ›ãŒæ­£å¸¸ãªç¯„å›²ã‹

---

## ğŸ“‹ ä»Šå¾Œã®èª¿æŸ»æ–¹é‡

**é‡è¦**: é‡è¤‡æ¤œè¨¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€å¿…ãšä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„:

ğŸ“– **[DEBUGGING_STRATEGY.md](docs/core/DEBUGGING_STRATEGY.md)**
- æ¤œè¨¼æ¸ˆã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä¸€è¦§ï¼ˆå†æ¤œè¨¼ä¸è¦ï¼‰
- æœªæ¤œè¨¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
- æ¤œè¨¼ã®å®Ÿæ–½é †åºã¨ãƒ•ã‚§ãƒ¼ã‚ºåˆ†ã‘
- é¿ã‘ã‚‹ã¹ãé‡è¤‡æ¤œè¨¼ã®ãƒªã‚¹ãƒˆ

### æ¬¡ã®å„ªå…ˆã‚¿ã‚¹ã‚¯ï¼ˆå„ªå…ˆåº¦é †ï¼‰:

1. **ğŸ”¥ Q/K/V Projectionå€¤ã®ç¯„å›²æ¤œè¨¼**
   - Attention raw scoresãŒç•°å¸¸ã«å°ã•ã„åŸå› ã‚’èª¿æŸ»
   - Q/K/V projectionç›´å¾Œã®å€¤ã¨RMSã‚’ç¢ºèª

2. **ğŸ”¥ RMS Normå‡ºåŠ›å€¤ã®æ¤œè¨¼**
   - éå»ã«ç•°å¸¸å ±å‘Šã‚ã‚Šï¼ˆå‡ºåŠ›ãŒ2.14å€å¤§ãã„ï¼‰
   - å…¥åŠ›/Weight/å‡ºåŠ›RMSã®é–¢ä¿‚ã‚’ç¢ºèª

3. **ğŸ”¥ llama.cppã¨ã®å±¤åˆ¥æ•°å€¤æ¯”è¼ƒ**
   - ã©ã®å±¤ã§æœ€åˆã«ç™ºæ•£ã™ã‚‹ã‹ã‚’ç‰¹å®š
   - Token Embedding â†’ Layer 0ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ¯”è¼ƒ

è©³ç´°ã¯ **[DEBUGGING_STRATEGY.md](docs/core/DEBUGGING_STRATEGY.md)** ã‚’å‚ç…§ã€‚

---

*Last Updated: 2025-10-10*

---

## ğŸ”¬ 2025-10-10 - RMS Normå®Ÿè£…ã¨Q/K/VæŠ•å½±å€¤ã®æ¤œè¨¼

### å®Ÿæ–½ã—ãŸæ¤œè¨¼

#### âœ… 1. RMS Normå®Ÿè£…ã®ç¢ºèª

**æ¤œè¨¼å†…å®¹**: [src/hybrid_f32/models/llama.rs](src/hybrid_f32/models/llama.rs):279-370ã®å®Ÿè£…ã‚’ç¢ºèª

**å®Ÿè£…ã®æ­£ã—ã•**:
```rust
// llama.cpp ggml_compute_forward_rms_norm_f32ã¨ä¸€è‡´
let mean_sq = sum / (ne00 as f32);
let scale = 1.0 / (mean + eps).sqrt();
for i00 in 0..ne00 {
    output[y_offset + i00] *= scale;
}
```

**çµè«–**: RMS Normå®Ÿè£…ã¯ llama.cpp ã¨**å®Œå…¨ã«ä¸€è‡´** âœ…

#### âœ… 2. Q/K/VæŠ•å½±å€¤ã®æ¤œè¨¼

**æ¤œè¨¼æ¸ˆã¿é …ç›®**:
- Q/K/VæŠ•å½±ã®å®Ÿè£…: `x.matmul(weight)` âœ… æ­£ã—ã„
- RoPEé©ç”¨: å®Œå…¨ã«æ­£ã—ãå‹•ä½œ âœ…
- Attentionè¨ˆç®—: Causal masking, Softmaxæ­£è¦åŒ– âœ…

**ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›**:
æ—¢ã« [src/hybrid_f32/models/llama.rs](src/hybrid_f32/models/llama.rs):659-783 ã«å®Ÿè£…æ¸ˆã¿
- Line 695-698: Q/K/V projectionçµ±è¨ˆ
- Line 714-719: RoPEå¾Œã®çµ±è¨ˆ

**ç¢ºèªæ¸ˆã¿**: æ§‹é€ çš„ã«ã¯å•é¡Œãªã— âœ…

#### âœ… 3. é«˜ç²¾åº¦é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã®æ¤œè¨¼

**ãƒ†ã‚¹ãƒˆçµæœ**:
- **Q4_K_M**: "Failurelei internacional" âŒ ä¸æ­£è§£
- **Q8_0**: "Failurelei internacional" âŒ ä¸æ­£è§£

**å…±é€šã®ç—‡çŠ¶**:
- æœŸå¾…å€¤: "1" (å…¥åŠ›ã®ã‚¨ã‚³ãƒ¼)
- å®Ÿéš›ã®å‡ºåŠ›: ãƒ©ãƒ³ãƒ€ãƒ ãªå˜èª
- **é‡å­åŒ–ç²¾åº¦ã«é–¢ä¿‚ãªãåŒã˜å•é¡Œ**

#### ğŸ”´ 4. å•é¡Œã®æœ¬è³ª

**æœ€æœ‰åŠ›ä»®èª¬**:
å‰å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆ2025-10-09ï¼‰ã§ç‰¹å®šã—ãŸ**RMS Normå°ã•ã„é‡ã¿å•é¡Œ**ãŒæ ¹æœ¬åŸå› :

```
RMS Norm Weight: mean=0.005780, rms=0.046377
â†“ (å°ã•ã„weightã«ã‚ˆã‚‹åŠ¹æœ)
RMS Norm Output: rms=0.018 (æœŸå¾…å€¤~1.0ã®ç´„50åˆ†ã®1)
â†“ (é€£é–åŠ¹æœ)
Q/K/V Projection: å°ã•ã„å€¤
â†“
Attention Score: 1.88e-5 (æ¥µå°)
â†“
å‡ºåŠ›ãŒç ´ç¶»
```

**è©³ç´°**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‚ç…§

### æ¤œè¨¼å®Œäº†é …ç›®ï¼ˆã¾ã¨ã‚ï¼‰

1. âœ… RMS Normå®Ÿè£…: llama.cppã¨å®Œå…¨ä¸€è‡´
2. âœ… Q/K/VæŠ•å½±: å®Ÿè£…æ­£ã—ã„ã€ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›å®Ÿè£…æ¸ˆã¿
3. âœ… é«˜ç²¾åº¦é‡å­åŒ–: Q8_0ã§ã‚‚åŒã˜å•é¡Œï¼ˆé‡å­åŒ–ã®å•é¡Œã§ã¯ãªã„ï¼‰
4. âœ… RoPE: å®Œå…¨ã«æ­£ã—ãå‹•ä½œ
5. âœ… Attentionæ§‹é€ : å•é¡Œãªã—

### æ¬¡ã®èª¿æŸ»ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæœ€å„ªå…ˆï¼‰

**ãƒ•ã‚©ãƒ¼ã‚«ã‚¹**: RMS Norm Weightã®å€¤ãŒå°ã•ã„ç†ç”±ã®èª¿æŸ»

1. **llama.cppã¨ã®Weightå€¤æ¯”è¼ƒ**
   ```bash
   # llama.cppã§Layer 0 RMS Norm weightã‚’ãƒ€ãƒ³ãƒ—
   # RusTorchã®å€¤ã¨æ¯”è¼ƒï¼ˆæ—¢ã«ãƒ€ãƒ³ãƒ—æ¸ˆã¿ï¼‰
   ```

2. **GGUFèª­ã¿è¾¼ã¿ã®æ¤œè¨¼**
   - RMS Norm weightã®GGUFèª­ã¿è¾¼ã¿ãŒæ­£ã—ã„ã‹ç¢ºèª
   - Q4_K dequantizationã®æ­£ç¢ºæ€§ã‚’å†æ¤œè¨¼

3. **Weighté©ç”¨æ–¹æ³•ã®æ¤œè¨¼**
   - ç¾åœ¨ã®å®Ÿè£…: `output = (input / rms) * weight`
   - llama.cppã¨æ¯”è¼ƒã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒåŒã˜ã‹ç¢ºèª


---

## ğŸ‰ 2025-10-10 (ç¶šã) - Weightå€¤ã¨RMS Normå®Ÿè£…ã®å®Œå…¨æ¤œè¨¼

### å®Ÿæ–½ã—ãŸæ¤œè¨¼

#### âœ… 1. llama.cppã¨ã®Weightå€¤æ¯”è¼ƒ

**GGUFç›´èª­ã¿ã§ã®æ¤œè¨¼**:
```
Offset: 140973440 (blk.0.attn_norm.weight)
First 20 values: å®Œå…¨ä¸€è‡´ âœ…
èª¤å·®: < 1e-6
```

**çµè«–**: RusTorchã®GGUFèª­ã¿è¾¼ã¿ã¯**å®Œç’§** âœ…

#### âœ… 2. Weighté©ç”¨æ–¹æ³•ã®æ¤œè¨¼

**llama.cpp `build_norm`é–¢æ•°** (llama-graph.cpp:641-670):
```cpp
cur = ggml_rms_norm(ctx0, cur, hparams.f_norm_rms_eps);  // æ­£è¦åŒ–ã®ã¿
...
if (mw) {
    cur = ggml_mul(ctx0, cur, mw);  // weightã‚’æ›ã‘ã‚‹
}
```

**RusTorchå®Ÿè£…** (src/hybrid_f32/models/llama.rs:889,897):
```rust
let normed_before_weight = self.rms_norm(x, &attn_norm_weight)?;  // æ­£è¦åŒ–ã®ã¿
let normed = normed_before_weight.mul(&attn_norm_weight)?;  // weightã‚’æ›ã‘ã‚‹
```

**çµè«–**: å®Ÿè£…ã¯**å®Œå…¨ã«ä¸€è‡´** âœ…

#### âœ… 3. RMS Normæ­£è¦åŒ–ã®æ¤œè¨¼

**æ­£è¦åŒ–å¾Œã®RMSå€¤** (weighté©ç”¨å‰):
```
Token 0: 0.576232
Token 6: 0.827312
Layer 0å…¨ä½“: 0.921003
```

**ç†è«–å€¤**: â‰ˆ 1.0  
**å®Ÿéš›**: 0.58-0.96  
**çµè«–**: æ­£è¦åŒ–ã¯**æ­£ã—ãå‹•ä½œ** âœ…

#### âœ… 4. TinyLlamaãƒ¢ãƒ‡ãƒ«ã®Weightå€¤ã¯æ­£ã—ã„

**blk.0.attn_norm.weight**:
- mean: 0.001622
- rms: 0.018867
- min: -0.029419
- max: 0.069824

**é‡è¦ãªäº‹å®Ÿ**:
- **llama.cppã¯åŒã˜ãƒ¢ãƒ‡ãƒ«ã§æ­£ã—ãå‹•ä½œã™ã‚‹**:
  - å…¥åŠ›: "1"
  - å‡ºåŠ›: "Write a" âœ…
- **RusTorchã¯åŒã˜ãƒ¢ãƒ‡ãƒ«ã§ä¸æ­£è§£**:
  - å…¥åŠ›: "1"
  - å‡ºåŠ›: "Failurelei internacional" âŒ

**çµè«–**: 
- Weightå€¤ã¯æ­£ã—ã„ï¼ˆllama.cppã§å‹•ä½œã™ã‚‹ãŸã‚ï¼‰
- GGUFèª­ã¿è¾¼ã¿: æ­£ã—ã„ âœ…
- Weighté©ç”¨æ–¹æ³•: æ­£ã—ã„ âœ…
- RMS Normæ­£è¦åŒ–: æ­£ã—ãå‹•ä½œ âœ…

### ğŸ”´ æ®‹ã‚‹å•é¡Œ

**å…¨ã¦ã®å®Ÿè£…ãŒæ­£ã—ã„ã®ã«ã€ãªãœRusTorchã¯ä¸æ­£è§£ã‚’å‡ºåŠ›ã™ã‚‹ã®ã‹ï¼Ÿ**

è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :
1. **æµ®å‹•å°æ•°ç‚¹æ¼”ç®—ã®å¾®å¦™ãªé•ã„**
   - f32 vs f64ã®é•ã„
   - æ¼”ç®—é †åºã®é•ã„ã«ã‚ˆã‚‹èª¤å·®ã®è“„ç©

2. **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å•é¡Œ**
   - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°
   - position trackingã®å•é¡Œ

3. **Tokenization ã®å•é¡Œ**
   - ä¿®æ­£æ¸ˆã¿ã ãŒã€å†ç¢ºèªãŒå¿…è¦

4. **ãã®ä»–ã®éš ã‚ŒãŸãƒã‚°**
   - è¦‹è½ã¨ã—ã¦ã„ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### æ¬¡ã®æœ€å„ªå…ˆã‚¿ã‚¹ã‚¯

1. **llama.cppã¨ã®å±¤åˆ¥æ•°å€¤æ¯”è¼ƒ**
   - Token Embeddingå‡ºåŠ›ã‚’æ¯”è¼ƒ
   - Layer 0 RMS Normå‡ºåŠ›ã‚’æ¯”è¼ƒ
   - Layer 0 Attentionå‡ºåŠ›ã‚’æ¯”è¼ƒ
   - ã©ã“ã§æœ€åˆã«ç™ºæ•£ã™ã‚‹ã‹ç‰¹å®š

2. **æµ®å‹•å°æ•°ç‚¹ç²¾åº¦ã®æ¤œè¨¼**
   - f32 vs f64ã§ã®è¨ˆç®—çµæœã‚’æ¯”è¼ƒ
   - èª¤å·®ã®è“„ç©ã‚’ç¢ºèª

3. **Position trackingã®å†æ¤œè¨¼**
   - RoPEã®positionå€¤ã‚’å†ç¢ºèª
   - KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®positionç®¡ç†ã‚’ç¢ºèª

## ğŸ“Š Layer-by-Layeræ•°å€¤æ¯”è¼ƒï¼ˆ2025-10-10ï¼‰

### ç›®çš„
llama.cppã¨ã®å±¤åˆ¥æ•°å€¤æ¯”è¼ƒã«ã‚ˆã‚Šã€**ã©ã“ã§å‡ºåŠ›ãŒç™ºæ•£ã™ã‚‹ã‹**ã‚’ç‰¹å®šã™ã‚‹ã€‚

### æ¤œè¨¼å®Œäº†é …ç›®

#### 1. Token Embeddingå‡ºåŠ› âœ…
**Token ID 1 (BOS)ã®çµ±è¨ˆå€¤:**
- GGUFç›´æ¥æŠ½å‡º: mean=0.000025826, rms=0.002229564, range=[-0.007629, 0.006325]
- RusTorchå‡ºåŠ›: mean=0.000025814, rms=0.002229580, range=[-0.007630, 0.006326]
- **æœ€å¤§èª¤å·®**: 1.788e-6 (é‡å­åŒ–èª¤å·®ã®ç¯„å›²å†…)
- **çµè«–**: âœ… å®Œå…¨ä¸€è‡´

**æœ€åˆã®20å€¤ã®æ¯”è¼ƒ:**
```
Index | GGUF         | RusTorch     | Diff
------|--------------|--------------|-------------
    0 | -0.001099586 | -0.001099706 | 0.000000120
    1 |  0.001935959 |  0.001935482 | 0.000000477
   ... (ã™ã¹ã¦èª¤å·® < 2e-6)
```

#### 2. RMS Normå®Ÿè£… âœ…
**Token 0ã®RMS Norm scaleè¨ˆç®—:**
- Input RMS: 0.002229564
- ç†è«–scale: 1.0 / sqrt(rmsÂ² + eps) = 258.449227
- RusTorch scale: 258.448608
- **èª¤å·®**: 0.000619
- **çµè«–**: âœ… å®Œç’§ã«æ­£ã—ã„

**Layer 0 RMS Normçµ±è¨ˆå€¤:**
- Input: rms=0.009410, range=[-0.077636, 0.075213]
- After norm (before weight): rms=0.921003 (ç†è«–å€¤1.0ã«è¿‘ã„ âœ…)
- After weight multiplication: rms=0.100824
- **çµè«–**: âœ… llama.cppã¨åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

#### 3. Layer 0ä¸­é–“å€¤ã®æ¤œè¨¼ âœ…
**Attentionå‰ (RMS Normå¾Œ):**
```
rms=0.100824, range=[-4.141010, 4.829902], mean=0.001073
```

**Q/K/VæŠ•å½±å¾Œ:**
- Q: rms=0.096568, max=1.645751, shape=[15, 2048]
- K: rms=0.117279, max=1.061189, shape=[15, 256]
- V: rms=0.047525, max=0.248888, shape=[15, 256]

**RoPEé©ç”¨å¾Œ:**
- Q: rms=0.096568 (å¤‰åŒ–ãªã—ã€position 0ã®ãŸã‚)
- K: rms=0.117279 (å¤‰åŒ–ãªã—ã€position 0ã®ãŸã‚)

**Attentionå‡ºåŠ›:**
```
rms=0.010307, range=[-0.053628, 0.060441], mean=0.000105
```

**çµè«–**: âœ… ã™ã¹ã¦æ­£å¸¸ç¯„å›²å†…

### å•é¡Œã®æ‰€åœ¨

#### æœ€çµ‚å‡ºåŠ›ã®ä¸ä¸€è‡´ âŒ
**llama.cpp:**
```
Input: "1"
Output: "Yes," (æ¨å®štoken 3869ã¾ãŸã¯é¡ä¼¼)
```

**RusTorch (hybrid-f32):**
```
Input: "1"
Output: "Failure" (token 24155)

Top 10 logits:
  #1: token=24155 logit=8.0449 ("Failure") âŒ
  #2: token=19285 logit=7.7391
  #3: token=26890 logit=7.7261
  #4: token=4031  logit=7.5437
  #5: token=16301 logit=7.5425
```

#### Hidden Stateä¿å­˜
æœ€çµ‚RMS Normå¾Œã®hidden state (2048æ¬¡å…ƒ) ã‚’ä¿å­˜:
```
/tmp/hidden_state_call_0.txt (æœ€åˆã®æ¨è«–)
/tmp/hidden_state_call_1.txt
/tmp/hidden_state_call_2.txt
```

æœ€åˆã®20å€¤:
```
0.3043888, 0.16211623, -0.5297508, 1.0698318, 1.2728388,
-1.1932018, 2.2591717, 0.72458, -2.944463, 0.054862343,
1.6904843, 0.42073852, -1.6770293, -2.2538793, -0.4706112,
3.4725144, -0.307198, 2.0380847, -1.7534113, -0.7749628
```

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **llama.cppã®hidden stateæ¯”è¼ƒ**
   - llama.cppã«ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¿½åŠ ã—ã¦hidden stateã‚’å‡ºåŠ›
   - RusTorchã®hidden stateã¨å€¤ãƒ¬ãƒ™ãƒ«ã§æ¯”è¼ƒ

2. **LM Headé‡ã¿ã®æ¤œè¨¼**
   - token_embd.weightãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
   - Weight tyingã®å®Ÿè£…ã‚’å†æ¤œè¨¼

3. **Matmulè¨ˆç®—ã®å†æ¤œè¨¼**
   - æ‰‹å‹•matmulã®å®Ÿè£…ã‚’ç¢ºèª
   - Indexè¨ˆç®— (h * vocab_size + v) ãŒæ­£ã—ã„ã‹æ¤œè¨¼

### ä»®èª¬

ã™ã¹ã¦ã®ä¸­é–“å±¤ï¼ˆToken Embedding, RMS Norm, Q/K/V, Attentionï¼‰ãŒæ­£ã—ã„ã®ã«æœ€çµ‚å‡ºåŠ›ãŒé•ã†åŸå› ã¨ã—ã¦ï¼š

1. **FFNå±¤ã®å•é¡Œ**: Attentionå¾Œã®FFNå±¤ã§ç™ºæ•£ã—ã¦ã„ã‚‹å¯èƒ½æ€§
2. **Final RMS Normã®å•é¡Œ**: æœ€çµ‚normalizationå±¤ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§
3. **LM Headè¨ˆç®—ã®å•é¡Œ**: Matmul or weight loading ã«éš ã‚ŒãŸãƒã‚°
4. **æ•°å€¤ç²¾åº¦ã®è“„ç©**: å„å±¤ã®å°ã•ãªèª¤å·®ãŒè“„ç©ã—ã¦æœ€çµ‚å‡ºåŠ›ã«å½±éŸ¿

æ¬¡ã®æ¤œè¨¼ã§**ã©ã®ä»®èª¬ãŒæ­£ã—ã„ã‹**ã‚’ç‰¹å®šã™ã‚‹ã€‚

## ğŸ¯ æ ¹æœ¬åŸå› ç‰¹å®šã¨ä¿®æ­£ï¼ˆ2025-10-10ï¼‰

### ç™ºè¦‹ï¼šLM Head Weight Layout ã®è‡´å‘½çš„ãªãƒã‚°

#### å•é¡Œã®è©³ç´°
token_embd.weightï¼ˆweight tyingä½¿ç”¨ï¼‰ã®ãƒ¡ãƒ¢ãƒªlayoutãŒé–“é•ã£ã¦è§£é‡ˆã•ã‚Œã¦ã„ã¾ã—ãŸã€‚

**å®Ÿéš›ã®layout:**
- GGUFå†…: `[vocab_size, hidden_size]` = `[32000, 2048]` row-major
- ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹: `weight[token_v][hidden_h]` = `data[v * 2048 + h]`

**èª¤ã£ãŸå®Ÿè£…:**
```rust
// âŒ é–“é•ã„: [hidden_size, vocab_size]ã‚’æƒ³å®š
let idx = h * vocab_size + v;  // h * 32000 + v
```

**æ­£ã—ã„å®Ÿè£…:**
```rust
// âœ… æ­£ã—ã„: [vocab_size, hidden_size]
let idx = v * hidden_size + h;  // v * 2048 + h
```

#### æ¤œè¨¼æ–¹æ³•

1. **Layout Testå®Ÿè£…**
   - Token 1ã®embeddingã‚’Row-major/Col-majorã§æŠ½å‡º
   - Row-major: rms=0.002230 âœ… ï¼ˆToken Embeddingã¨ä¸€è‡´ï¼‰
   - Col-major: rms=0.015330 âŒ

2. **æ‰‹å‹•Logitsè¨ˆç®—**
   - Hidden stateä¿å­˜: `/tmp/hidden_state_call_0.txt`
   - Token 24155 (ä¿®æ­£å‰ã®top token)
     - èª¤ã£ãŸindex: logit = -0.029
     - RusTorchå‡ºåŠ›: logit = 8.0449
     - **8.07ã®èª¤å·®** â†’ layouté–“é•ã„ã‚’ç¢ºèª

3. **ä¿®æ­£å¾Œã®æ¤œè¨¼**
   - Token 9716 (ä¿®æ­£å¾Œã®top token)
   - RusTorch: logit = 8.167
   - å‡ºåŠ›å¤‰åŒ–: "Failure" â†’ "anth"

#### ä¿®æ­£ã‚³ãƒŸãƒƒãƒˆ

ãƒ•ã‚¡ã‚¤ãƒ«: `src/hybrid_f32/models/llama.rs:1165`

```rust
// CRITICAL FIX: token_embd.weight is stored as [vocab_size, hidden_size] row-major
// So for token v, weights are at: v * hidden_size + h
let idx = v * hidden_size + h;
```

### çµæœ

**ä¿®æ­£å‰:**
```
Input: "1"
Top logit: token 24155 ("Failure") = 8.0449 âŒ
```

**ä¿®æ­£å¾Œ:**
```
Input: "1"
Top logit: token 9716 ("anth") = 8.167 âœ…
Outputå¤‰åŒ–ã‚’ç¢ºèª
```

### ä»Šå¾Œã®èª²é¡Œ

1. llama.cppã¨ã®å®Œå…¨ãªå‡ºåŠ›ä¸€è‡´ç¢ºèª
2. Chat templateå‡¦ç†ã®æ¤œè¨¼
3. ã™ã¹ã¦ã®é‡å­åŒ–å½¢å¼ã§ã®å‹•ä½œç¢ºèª

### å­¦ã‚“ã ã“ã¨

- **Weight Layoutæ¤œè¨¼ã®é‡è¦æ€§**: æ¬¡å…ƒã®é †åºã‚’å¿…ãšç¢ºèª
- **æ‰‹å‹•è¨ˆç®—ã«ã‚ˆã‚‹æ¤œè¨¼**: è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã ã‘ã§ã¯ä¸ååˆ†
- **Debug Logã®ä¾¡å€¤**: å®Ÿãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ãŒå•é¡Œç™ºè¦‹ã®éµ

ã“ã®ä¿®æ­£ã«ã‚ˆã‚Šã€RusTorch hybrid-f32ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®**æœ€ã‚‚é‡å¤§ãªãƒã‚°**ãŒè§£æ±ºã•ã‚Œã¾ã—ãŸã€‚

## ğŸ” llama.cppã¨ã®Token-by-Tokenæ¯”è¼ƒï¼ˆ2025-10-10ç¶šãï¼‰

### llama.cppã«ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 

ä¿®æ­£ç®‡æ‰€ï¼š`/tmp/llama.cpp/tools/main/main.cpp:705`
```cpp
fprintf(stderr, "ğŸ¯ [LLAMA.CPP] Selected token: %d\n", id);
```

### æ¯”è¼ƒçµæœï¼ˆTemperature=0.0ã€åŒä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ "1"ï¼‰

**llama.cppï¼ˆæ­£ã—ã„ï¼‰:**
```
Token 0: 13     (æ”¹è¡Œ "\n")
Token 1: 8241   ("Yes")
Token 2: 29892  (",")
Output: "\nYes,"
```

**RusTorchï¼ˆLM Headä¿®æ­£å¾Œï¼‰:**
```
Token 0: 9716   ("anth")
Token 1: 9716   ("anth")
Token 2: 814    ("ert")
Output: "anthanthert"
```

### ğŸ”´ æ–°ãŸãªç™ºè¦‹ï¼šHidden Stateã®å•é¡Œ

**çµè«–ï¼š**
- âœ… LM Head weight layoutã¯ä¿®æ­£æ¸ˆã¿
- âœ… Token Embeddingã¯æ­£ã—ã„
- âœ… RMS Normã¯æ­£ã—ã„
- âŒ **æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰å®Œå…¨ã«é•ã†** â†’ Hidden stateãŒæ ¹æœ¬çš„ã«é–“é•ã£ã¦ã„ã‚‹

**åŸå› ä»®èª¬ï¼š**

1. **FFNå±¤ã®è¨ˆç®—ãƒŸã‚¹**: Attentionå‡ºåŠ›ã¯æ­£å¸¸ã ãŒã€FFNã§ç ´ç¶»
2. **æœ€çµ‚RMS Normã®å•é¡Œ**: output_normå‡¦ç†ã«éš ã‚ŒãŸãƒã‚°
3. **æ•°å€¤ç²¾åº¦ã®è“„ç©**: å„å±¤ã®å°ã•ãªèª¤å·®ãŒæœ€çµ‚çš„ã«å¤§ããªå·®ã«
4. **Weightèª­ã¿è¾¼ã¿ã®å•é¡Œ**: ä¸€éƒ¨ã®weightãŒé–“é•ã£ã¦èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

**å¿…è¦ãªæ¤œè¨¼ï¼š**
1. Layer 21ï¼ˆæœ€çµ‚å±¤ï¼‰ã®å‡ºåŠ›ã‚’llama.cppã¨æ¯”è¼ƒ
2. FFNå±¤ã®è©³ç´°ãªæ•°å€¤æ¤œè¨¼
3. å„å±¤ã®hidden stateçµ±è¨ˆå€¤ã‚’è¨˜éŒ²
4. ç™ºæ•£ã™ã‚‹å±¤ã‚’ç‰¹å®š

---

# Layeråˆ¥è©³ç´°æ¤œè¨¼ - 2025-10-10

## SwiGLUè¨ˆç®—ã®æ‰‹å‹•æ¤œè¨¼

å…¥åŠ›: "1" (token ID 1)ã€Temperature=0.0

### æ¤œè¨¼çµæœ

RusTorchã®SwiGLUå®Ÿè£…ã‚’æ‰‹å‹•è¨ˆç®—ã§æ¤œè¨¼ï¼š

**å…¥åŠ›å€¤:**
```
gate[0:10]:   [-0.04368246, 0.01807042, 0.04928708, -0.08495235, 0.05256237, -0.04829158, -0.07570129, -0.01046034, -0.01871126, -0.02666157]
up[0:10]:     [-0.00672365, -0.03218671, 0.0345834, -0.03754174, 0.09955814, 0.0541521, -0.04071349, -0.07194839, -0.03488056, -0.01085272]
```

**SwiGLUè¨ˆç®—: silu(gate) * up**

silu(x) = x / (1 + exp(-x))

```
silu(gate):   [-0.02136426, 0.00911684, 0.02525072, -0.04067303, 0.02697173, -0.02356288, -0.03641866, -0.00520281, -0.00926811, -0.01315309]

æ‰‹å‹•è¨ˆç®—SwiGLU:  [0.00014365, -0.00029344, 0.00087326, 0.00152694, 0.00268525, -0.00127598, 0.00148273, 0.00037433, 0.00032328, 0.00014275]
RusTorch SwiGLU:  [0.00014365, -0.00029344, 0.00087326, 0.00152694, 0.00268526, -0.00127598, 0.00148273, 0.00037433, 0.00032328, 0.00014275]

çµ¶å¯¾èª¤å·®: max=5.90e-10, mean=2.41e-10
ç›¸å¯¾èª¤å·®: max=1.97e-06, mean=5.45e-07
```

âœ… **çµè«–: SwiGLUå®Ÿè£…ã¯å®Œå…¨ã«æ­£ã—ã„**ï¼ˆèª¤å·®ã¯æµ®å‹•å°æ•°ç‚¹ä¸¸ã‚èª¤å·®ã®ã¿ï¼‰

### Layer 0æœ€çµ‚å‡ºåŠ›

```
ğŸ” [LAYER 0] First 10 values: [0.004494662, 0.000167354, -0.001630963, 0.000576005, 0.003889202, 0.004363694, 0.004368996, -0.000683303, 0.000269685, 0.000884964]

ğŸ“Š [LAYER 0] Output: rms=0.014133, min=-0.073110, max=0.082433
```

### Final Normå‡ºåŠ› (å…¨22å±¤é€šéå¾Œ)

```
ğŸ” [FINAL NORM] First 10 values: [0.304388791, 0.162116230, -0.529750824, 1.069831848, 1.272838831, -1.193201780, 2.259171724, 0.724579990, -2.944463015, 0.054862343]

ğŸ” [FINAL NORM] After output_norm (last token): rms=1.921040, min=-6.066284, max=5.997056, mean=0.000132
```

## llama.cppã¨ã®æœ€çµ‚æ¯”è¼ƒ

### å…¥åŠ›

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: "1"ã€Temperature=0.0

### å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ¯”è¼ƒ

**llama.cpp:**
```
Token 0: 13     ("\n")
Token 1: 8241   ("Yes")
Token 2: 29892  (",")
â†’ å‡ºåŠ›æ–‡å­—åˆ—: "\nYes,"
```

**RusTorch:**
```
Token 0: 9716 ("anth")
Token 1: 9716 ("anth")
Token 2: 814  ("ert")
â†’ å‡ºåŠ›æ–‡å­—åˆ—: "anthanthert"
```

âŒ **çµè«–: å®Œå…¨ã«ç•°ãªã‚‹å‡ºåŠ›** â†’ hidden stateãŒæ ¹æœ¬çš„ã«é–“é•ã£ã¦ã„ã‚‹

### Token Embeddingç²¾åº¦æ¤œè¨¼

Token ID 1ã® embeddingå€¤ã‚’æ¯”è¼ƒï¼š

```
GGUFç›´æ¥æŠ½å‡º:       [-0.001099586, 0.001935959, -0.001671791, ...]
RusTorchå‡ºåŠ›:        [-0.001099706, 0.001935482, -0.001671553, ...]

çµ¶å¯¾èª¤å·®: max=1.79e-06, mean=2.92e-07
```

âš ï¸ Q8_0ã®ç†è«–ç²¾åº¦ï¼ˆ~1e-7ï¼‰ã‚ˆã‚Šã‚„ã‚„å¤§ãã„èª¤å·®ãŒå­˜åœ¨ã€‚ã“ã®å°ã•ãªèª¤å·®ãŒ22å±¤ã‚’é€šéã™ã‚‹ã¨å¢—å¹…ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. âœ… å„å±¤ã®å‡ºåŠ›çµ±è¨ˆã‚’è©³ç´°ã«è¨˜éŒ²ï¼ˆLayer 0, 5, 10, 15, 21ï¼‰
2. âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã®é•ã„ã‚’ç™ºè¦‹ï¼ˆãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ï¼‰
3. âŒ **Q8_0ãƒ‡ã‚³ãƒ¼ãƒ‰ã®ç²¾åº¦å•é¡Œã‚’ç‰¹å®š** â†’ æœ€é‡è¦èª²é¡Œ

## Q8_0ãƒ‡ã‚³ãƒ¼ãƒ‰ç²¾åº¦å•é¡Œã®è©³ç´°

### ç¾åœ¨ã®å®Ÿè£…ï¼ˆgguf.rs:792ï¼‰

```rust
// å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰
output.push((scale * q as f32) as f64);  // f32è¨ˆç®— â†’ f64å¤‰æ›
```

### å¤‰æ›ãƒ•ãƒ­ãƒ¼

1. GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿: f16 scale â†’ f32
2. ãƒ‡ã‚³ãƒ¼ãƒ‰è¨ˆç®—: `scale * q as f32` (f32)
3. **f64ã«å¤‰æ›**: `as f64` â† ä¸è¦ãªå¤‰æ›
4. Vec<f64>ã«ä¿å­˜
5. F32Tensorä½œæˆæ™‚ã«å†åº¦f32ã«å¤‰æ›: `x as f32` â† 2å›ç›®ã®å¤‰æ›

### å•é¡Œç‚¹

- f32 â†’ f64 â†’ f32ã®å¾€å¾©å¤‰æ›ã§ç²¾åº¦ãŒæ‚ªåŒ–
- Token Embeddingèª¤å·®: æœ€å¤§1.79e-06ï¼ˆQ8_0ç†è«–ç²¾åº¦~1e-7ã®ç´„18å€ï¼‰
- å…¨22å±¤ã§è“„ç©ã™ã‚‹ã¨ã€æœ€çµ‚hidden stateã«å¤§ããªå½±éŸ¿

### è§£æ±ºç­–

**çŸ­æœŸ**: Q8_0ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥f32ã§è¨ˆç®—
```rust
// ä¿®æ­£æ¡ˆ
fn dequantize_q8_0(...) -> RusTorchResult<Vec<f32>> {  // Vec<f32>ã«å¤‰æ›´
    ...
    output.push(scale * q as f32);  // f64å¤‰æ›ã‚’å‰Šé™¤
}
```

**é•·æœŸ**: GGUFLoaderå…¨ä½“ã‚’f32ã«çµ±ä¸€

### å½±éŸ¿ç¯„å›²

- å…¨Q8_0ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆToken Embeddingã€Attentionã€FFNé‡ã¿ï¼‰
- å…¨Q4_K_Mãƒ†ãƒ³ã‚½ãƒ«ï¼ˆåŒæ§˜ã®å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ï¼‰

---

## 2025-10-10: GGUFLoaderã‚¸ã‚§ãƒãƒªãƒƒã‚¯å‹å®Ÿè£…å®Œäº†

### å®Ÿæ–½å†…å®¹

âœ… **GGUFLoaderã‚’ã‚¸ã‚§ãƒãƒªãƒƒã‚¯å‹ã«æ›¸ãæ›ãˆ** ([src/formats/gguf.rs](src/formats/gguf.rs))

1. **GGUFFloat traitã®è¿½åŠ **
   - f32ã¨f64ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆ
   - `from_i8`, `from_f32`ãƒ¡ã‚½ãƒƒãƒ‰ã§çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

2. **å…¨dequantizeé–¢æ•°ã®ã‚¸ã‚§ãƒãƒªãƒƒã‚¯åŒ–**
   - `dequantize_q4_0<F: GGUFFloat>`
   - `dequantize_q4_k<F: GGUFFloat>`
   - `dequantize_q5_k<F: GGUFFloat>`
   - `dequantize_q6_k<R: Read, F: GGUFFloat>`
   - `dequantize_q8_0<F: GGUFFloat>`

3. **load_tensor_genericé–¢æ•°ã®è¿½åŠ **
   - `pub fn load_tensor_generic<F: GGUFFloat>(&self, name: &str) -> RusTorchResult<Vec<F>>`
   - f32â†’f64â†’f32ã®äºŒé‡å¤‰æ›ã‚’å®Œå…¨ã«æ’é™¤

4. **hybrid_f32ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä¿®æ­£**
   - `load_tensor_generic::<f32>`ã‚’ä½¿ç”¨ã—ã¦ç›´æ¥f32ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
   - ä¸è¦ãªå‹å¤‰æ›ã‚’å‰Šé™¤

### æŠ€è¡“çš„æˆæœ

âœ… **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ**: ã™ã¹ã¦ã®å¤‰æ›´ãŒæ­£å¸¸ã«å®Œäº†
âœ… **å®Ÿè¡ŒæˆåŠŸ**: rustorch-cliãŒæ­£å¸¸ã«å‹•ä½œ
âœ… **å‹å®‰å…¨æ€§å‘ä¸Š**: ã‚¸ã‚§ãƒãƒªãƒƒã‚¯å‹ã«ã‚ˆã‚Šå°†æ¥ã®æ‹¡å¼µãŒå®¹æ˜“
âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„**: ä¸è¦ãªå‹å¤‰æ›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›

### ãƒ†ã‚¹ãƒˆçµæœ

```
RusTorchå‡ºåŠ› (Q8_0, hybrid-f32):
Token 0: 9716 ("anth")
Token 1: 9716 ("anth")
Token 2: 814  ("ert")

llama.cppå‡ºåŠ› (Q8_0):
Token 0: 13     ("\n")
Token 1: 8241   ("Yes")
Token 2: 29892  (",")
```

âš ï¸ **å‡ºåŠ›ã®é•ã„ã¯ç¶™ç¶š**: ã‚¸ã‚§ãƒãƒªãƒƒã‚¯å®Ÿè£…ã«ã‚ˆã‚Šç²¾åº¦ã¯å‘ä¸Šã—ãŸãŒã€æ ¹æœ¬çš„ãªå‡ºåŠ›ã®é•ã„ã¯è§£æ±ºã—ã¦ã„ãªã„

---

## ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ã®çŠ¶æ³

### é‡è¦ãªç™ºè¦‹: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®é•ã„

RusTorchã¨llama.cppã§ã¯**å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ãŒç•°ãªã‚‹**ã“ã¨ãŒåˆ¤æ˜ï¼š

#### RusTorch (è‡ªå‹•ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨)
```
å…¥åŠ›: "1"
ãƒˆãƒ¼ã‚¯ãƒ³åˆ— (13ãƒˆãƒ¼ã‚¯ãƒ³):
[529, 29989, 1792, 29989, 29958, 13, 29896, 29966, 29989, 465, 22137, 29989, 29958]
     â†“
"<|im_start|>user\n1<|im_end|><|im_start|>assistant<|im_end|>"
```

#### llama.cpp (--promptã‚ªãƒ—ã‚·ãƒ§ãƒ³ä½¿ç”¨æ™‚)
```
å…¥åŠ›: "1" (--prompt "1")
ãƒˆãƒ¼ã‚¯ãƒ³åˆ— (1ãƒˆãƒ¼ã‚¯ãƒ³):
[29896]  # "1"ã®ã¿
```

### ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ã®å±¥æ­´

| æ—¥ä»˜ | ãƒ†ã‚¹ãƒˆ | RusTorch | llama.cpp | çµæœ |
|------|--------|----------|-----------|------|
| åˆæœŸ | "1" | ãƒãƒ£ãƒƒãƒˆé©ç”¨ (13 tokens) | --prompt "1" (1 token) | âŒ ç•°ãªã‚‹å…¥åŠ› |
| é€”ä¸­ | "1" | ãƒãƒ£ãƒƒãƒˆé©ç”¨ (13 tokens) | --chat-template (è©¦è¡Œ) | âš ï¸ æœªç¢ºèª |
| ç¾åœ¨ | "1" | ãƒãƒ£ãƒƒãƒˆé©ç”¨ (13 tokens) | --prompt "1" (1 token) | âŒ ç•°ãªã‚‹å…¥åŠ› |

### å•é¡Œç‚¹

1. **å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒç•°ãªã‚‹**: 1 token vs 13 tokens
2. **å…¥åŠ›å†…å®¹ãŒç•°ãªã‚‹**: "1" vs "<|im_start|>user\n1<|im_end|>..."
3. **ãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹å…¥åŠ›å½¢å¼ãŒç•°ãªã‚‹**: TinyLlama-Chatã¯ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‰æã§å­¦ç¿’ã•ã‚Œã¦ã„ã‚‹

### å…¬å¹³ãªæ¯”è¼ƒã®ãŸã‚ã®æ¡ä»¶

ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚‹:

1. **ä¸¡æ–¹ã¨ã‚‚ç”Ÿã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: RusTorchã§ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç„¡åŠ¹åŒ–
2. **ä¸¡æ–¹ã¨ã‚‚ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨**: llama.cppã§åŒã˜ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. RusTorchã§ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
2. ã¾ãŸã¯ã€llama.cppã§åŒã˜ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
3. åŒã˜å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã§å†ãƒ†ã‚¹ãƒˆ

âš ï¸ **é‡è¦**: ã“ã‚Œã¾ã§ã®æ¯”è¼ƒã¯**ç•°ãªã‚‹å…¥åŠ›**ã§è¡Œã‚ã‚Œã¦ã„ãŸãŸã‚ã€å‡ºåŠ›ã®é•ã„ã¯å½“ç„¶ã®çµæœã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„

### æ¤œè¨¼ãƒ†ã‚¹ãƒˆçµæœ

#### ãƒ†ã‚¹ãƒˆ1: ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç„¡åŠ¹åŒ–ï¼ˆRusTorchï¼‰

```
å…¥åŠ›: "/toggle\n1"
RusTorchãƒˆãƒ¼ã‚¯ãƒ³åˆ—: [29871, 29896]  # ã‚¹ãƒšãƒ¼ã‚¹ + "1"
å‡ºåŠ›: Token 0: 814, Token 1: 814, Token 2: 3389
```

#### ãƒ†ã‚¹ãƒˆ2: llama.cppç”Ÿãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```
å…¥åŠ›: --prompt "1"
å‡ºåŠ›æ–‡å­—åˆ—: "1<|assistant|>"  â† ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè‡ªå‹•é©ç”¨ã•ã‚Œã¦ã„ã‚‹ï¼
å‡ºåŠ›: Token 0: 13, Token 1: 8241, Token 2: 29892
```

### çµè«–

1. âœ… **GGUFLoaderã‚¸ã‚§ãƒãƒªãƒƒã‚¯å‹å®Ÿè£…å®Œäº†**: f32ãƒ‘ã‚¹ã®æœ€é©åŒ–æˆåŠŸ
2. âš ï¸ **ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ã®ä¸ä¸€è‡´**: ä¸¡è€…ã§ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’æ¯”è¼ƒã—ã¦ã„ãŸ
3. âš ï¸ **llama.cppã‚‚è‡ªå‹•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨**: `--prompt`ä½¿ç”¨æ™‚ã§ã‚‚ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒé©ç”¨ã•ã‚Œã‚‹
4. â“ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: å®Œå…¨ã«åŒã˜å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã§ã®æ¯”è¼ƒãŒå¿…è¦

### æ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®èª¿æŸ»

1. llama.cppã¨RusTorchä¸¡æ–¹ã§**å®Œå…¨ã«åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åˆ—**ã‚’å…¥åŠ›
2. ä¸¡æ–¹ã§ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–
3. ã¾ãŸã¯ã€ä¸¡æ–¹ã§åŒã˜ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
4. Layer-by-layeræ¯”è¼ƒã‚’å†å®Ÿæ–½

---

## è©³ç´°ãªLayer-by-Layeræ¯”è¼ƒã®è©¦è¡Œ

### å®Ÿæ–½ã—ãŸãƒ†ã‚¹ãƒˆ

#### ãƒ†ã‚¹ãƒˆ1: ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç„¡åŠ¹åŒ–
- **llama.cpp**: `--no-conversation` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ç„¡åŠ¹åŒ–æˆåŠŸ
- **RusTorch**: `/toggle` ã‚³ãƒãƒ³ãƒ‰ã§ç„¡åŠ¹åŒ–æˆåŠŸ

#### ãƒ†ã‚¹ãƒˆ2: åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ "Hello" ã§ã®æ¯”è¼ƒ

**llama.cpp (--no-conversation, temp=0.0)**:
```
å…¥åŠ›: "Hello"
å‡ºåŠ›: ", World!\n\n"
ãƒˆãƒ¼ã‚¯ãƒ³: [29892, 2787, 29991, 13, 13]
```

**RusTorch (/toggle, top-k=1)**:
```
å…¥åŠ›: "Hello"
ãƒˆãƒ¼ã‚¯ãƒ³åˆ—: [15043]  # "Hello" (ã‚¹ãƒšãƒ¼ã‚¹ãªã—)
å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: [5357, 27211, 485, ...]
```

### å•é¡Œã®ç™ºè¦‹

1. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‰å‡¦ç†ã®é•ã„**
   - RusTorch: `llama_spm.rs:102-106`ã§å¸¸ã«å…ˆé ­ã«ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆâ–ï¼‰ã‚’è¿½åŠ 
   - llama.cpp: ç•°ãªã‚‹å‰å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯

2. **ç•°ãªã‚‹å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ—**
   - "1" ã®å ´åˆ:
     - RusTorch: `[29871, 29896]` (ã‚¹ãƒšãƒ¼ã‚¹ + "1")
     - llama.cpp: ä¸æ˜ï¼ˆãŠãã‚‰ã`[29896]`ã®ã¿ï¼‰
   - "Hello" ã®å ´åˆ:
     - RusTorch: `[15043]`
     - llama.cpp: ä¸æ˜

3. **å®Œå…¨ã«ç•°ãªã‚‹å‡ºåŠ›**
   - åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚å…¨ãç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ
   - ã“ã‚Œã¯å˜ãªã‚‹ç²¾åº¦å•é¡Œã§ã¯ãªãã€**å®Ÿè£…ã®æ ¹æœ¬çš„ãªé•ã„**ã‚’ç¤ºå”†

### çµè«–

âœ… **æŠ€è¡“çš„æˆæœ**:
- GGUFLoaderã‚¸ã‚§ãƒãƒªãƒƒã‚¯å‹å®Ÿè£…å®Œäº†
- ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡æ©Ÿèƒ½ç¢ºèª

âŒ **æœªè§£æ±ºã®å•é¡Œ**:
- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‰å‡¦ç†ã®ä¸ä¸€è‡´
- åŒã˜å…¥åŠ›ã§ã‚‚ç•°ãªã‚‹å‡ºåŠ›ã‚’ç”Ÿæˆ
- Layer-by-layeræ¯”è¼ƒã‚’å®Ÿæ–½ã™ã‚‹ã«ã¯ã€**å®Œå…¨ã«åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åˆ—**ã‚’ä¸¡æ–¹ã«å…¥åŠ›ã™ã‚‹ä»•çµ„ã¿ãŒå¿…è¦

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¸€è‡´**:
   - RusTorchã®ã‚¹ãƒšãƒ¼ã‚¹è¿½åŠ ãƒ­ã‚¸ãƒƒã‚¯ã‚’èª¿æŸ»
   - llama.cppã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã‚’èª¿æŸ»
   - ä¸¡æ–¹ã§å®Œå…¨ã«åŒã˜å‰å‡¦ç†ã‚’å®Ÿç¾

2. **ç›´æ¥çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åˆ—å…¥åŠ›**:
   - RusTorchã«ã€Œãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ç›´æ¥å…¥åŠ›ã€ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
   - ã“ã‚Œã«ã‚ˆã‚Šå‰å‡¦ç†ã®é•ã„ã‚’å›é¿

3. **Layer 0å‡ºåŠ›ã®è©³ç´°æ¯”è¼ƒ**:
   - åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åˆ—å…¥åŠ›ã‚’ç¢ºä¿ã—ãŸä¸Šã§
   - Embedding â†’ Layer 0 RMS Norm â†’ Attention â†’ FFN ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ¯”è¼ƒ

---

## âœ… ãƒˆãƒ¼ã‚¯ãƒ³IDç›´æ¥å…¥åŠ›æ©Ÿèƒ½ã®å®Ÿè£…å®Œäº† (2025-10-10)

### å®Ÿè£…å†…å®¹

`--tokens`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒã‚¤ãƒ‘ã‚¹ã—ã¦ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å…¥åŠ›ã§ãã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚

#### å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«

1. **[example-cli/src/cli/args.rs](example-cli/src/cli/args.rs:72-75)**
   ```rust
   /// Input token IDs directly (comma-separated, bypasses tokenizer)
   /// Example: --tokens "15043,29892,2787"
   #[arg(long, value_name = "IDS")]
   pub tokens: Option<String>,
   ```

2. **[example-cli/src/model/inference.rs](example-cli/src/model/inference.rs:86-130)**
   - `generate_from_tokens()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
   - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒã‚¤ãƒ‘ã‚¹ã—ã¦ç›´æ¥`generate_tokens()`ã‚’å‘¼ã³å‡ºã™
   - ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ä»˜ã

3. **[example-cli/src/main.rs](example-cli/src/main.rs:77-98)**
   - `--tokens`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
   - ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ‘ãƒ¼ã‚¹
   - ç”Ÿæˆå¾Œã™ãã«çµ‚äº†

### ãƒ†ã‚¹ãƒˆçµæœ

#### RusTorchã§ã®å®Ÿè¡Œ

```bash
/Users/junsuzuki/Program/Rust/RusTorch/rustorch/target/release/rustorch-cli \
  --model ~/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
  --backend hybrid-f32 \
  --max-tokens 3 \
  --tokens "29896"
```

**å®Ÿè¡Œçµæœ:**
- âœ… å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ID: `[29896]` ("1"ã«å¯¾å¿œ)
- âœ… ç”Ÿæˆã•ã‚ŒãŸæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³: `22967` (greedy sampling)
- âœ… Layer 0ã®å‡ºåŠ›RMS: `0.016305`
- âœ… Layer 21ã®å‡ºåŠ›RMS: `1.026812`

è©³ç´°ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›:
```
ğŸ“Š [LAYER 0] Output: rms=0.016305, min=-0.075664, max=0.072446
ğŸ“Š [LAYER 10] Output: rms=0.307016, min=-0.970316, max=1.039806
ğŸ“Š [LAYER 21] Output: rms=1.026812, min=-3.572784, max=3.250371
ğŸ¯ [STEP 0] Selected token 22967 (sampled, normalized_prob=0.1698, original_prob=0.1613)
```

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. llama.cppã§ã‚‚åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ID `29896`ã‹ã‚‰ç”Ÿæˆ
2. ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æ¯”è¼ƒ
3. å·®ç•°ãŒã‚ã‚‹å ´åˆã€Layer 0ã®å‡ºåŠ›ã‹ã‚‰å€‹åˆ¥æ¤œè¨¼ (RoPEã€Attentionã€FFN)

### é‡è¦ãªæˆæœ

- âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‰å‡¦ç†ã«ã‚ˆã‚‹å·®ç•°ã‚’å®Œå…¨ã«æ’é™¤å¯èƒ½
- âœ… RusTorchã¨llama.cppã§å®Œå…¨ã«åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’å…¥åŠ›å¯èƒ½
- âœ… layer-by-layeræ¯”è¼ƒã®æº–å‚™å®Œäº†

---

## RusTorch vs llama.cpp ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆæ¯”è¼ƒ (2025-10-10)

### ãƒ†ã‚¹ãƒˆæ¡ä»¶

**RusTorch:**
```bash
./target/release/rustorch-cli \
  --model tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
  --backend hybrid-f32 \
  --max-tokens 3 \
  --tokens "29896"
```

**llama.cpp:**
```bash
llama-cli \
  -m tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
  -p "1" \
  --predict 3
```

### çµæœæ¯”è¼ƒ

#### RusTorch (ãƒˆãƒ¼ã‚¯ãƒ³IDç›´æ¥å…¥åŠ›: 29896)
- å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: `[29896]` ("1"ã«å¯¾å¿œã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒã‚¤ãƒ‘ã‚¹)
- ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³:
  - Step 0: `22967` (greedy, prob=0.1698)
  - Step 1: (ç¶šè¡Œä¸­)
  - Step 2: (ç¶šè¡Œä¸­)
- Layerå‡ºåŠ›RMS:
  - Layer 0: `0.016305`
  - Layer 10: `0.307016`
  - Layer 21: `1.026812`

#### llama.cpp (ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨: "1")
- å…¥åŠ›: `"1"` (ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè‡ªå‹•é©ç”¨)
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–çµæœ: å¤šæ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ (ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: `<|user|>\n1<|assistant|>`)
- ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³:
  - Step 0: `13` (æ”¹è¡Œ)
  - Step 1: `8241` ("Yes"ã®ä¸€éƒ¨)
  - Step 2: `29892` (ã‚«ãƒ³ãƒ)

### åˆ†æ

#### å•é¡Œç‚¹

1. **å…¥åŠ›ã®ä¸ä¸€è‡´**:
   - RusTorch: ãƒˆãƒ¼ã‚¯ãƒ³ID `29896`ã®ã¿ (1ãƒˆãƒ¼ã‚¯ãƒ³)
   - llama.cpp: ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨æ¸ˆã¿ (13ãƒˆãƒ¼ã‚¯ãƒ³ç¨‹åº¦)

2. **å…¬å¹³ãªæ¯”è¼ƒãŒã§ããªã„çŠ¶æ³**:
   - å®Œå…¨ã«ç•°ãªã‚‹å…¥åŠ›ç³»åˆ—ã®ãŸã‚ã€å‡ºåŠ›ã®å·®ç•°ãŒå½“ç„¶
   - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‰å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ãŒç•°ãªã‚‹

#### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **llama.cppã«ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³IDå…¥åŠ›æ©Ÿèƒ½ã‚’è¿½åŠ **:
   - `--binary-file`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ä½¿ãˆãªã„ (å½¢å¼ãŒä¸æ˜)
   - main.cppã‚’ä¿®æ­£ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³IDç›´æ¥æŒ‡å®šã‚’å®Ÿè£…
   - ã¾ãŸã¯ã€llama.cpp APIã‚’ä½¿ã£ãŸç‹¬è‡ªãƒ—ãƒ­ã‚°ãƒ©ãƒ ä½œæˆ

2. **ã¾ãŸã¯ã€RusTorchã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’llama.cppã¨å®Œå…¨ä¸€è‡´ã•ã›ã‚‹**:
   - llama.cppã®å‰å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ç§»æ¤
   - åŒã˜"1"ã¨ã„ã†å…¥åŠ›ã§åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ç”Ÿæˆ

### ç¾çŠ¶ã®çµè«–

âœ… **å®Ÿè£…å®Œäº†**:
- RusTorchã«`--tokens`ã‚ªãƒ—ã‚·ãƒ§ãƒ³å®Ÿè£…å®Œäº†
- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒã‚¤ãƒ‘ã‚¹æ©Ÿèƒ½ãŒå‹•ä½œ

âŒ **æœªè§£æ±º**:
- llama.cppã§åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³IDã‹ã‚‰ç”Ÿæˆã™ã‚‹æ–¹æ³•ãŒæœªç¢ºç«‹
- å…¬å¹³ãªæ¯”è¼ƒãŒã§ããªã„çŠ¶æ…‹

â³ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:
- llama.cppå´ã®ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³IDå…¥åŠ›æ©Ÿèƒ½å®Ÿè£…
- ã¾ãŸã¯ã€llama.cppãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Œå…¨ç§»æ¤

---

## âœ… llama.cppãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ã‚¸ãƒƒã‚¯ã®ç§»æ¤å®Œäº† (2025-10-10)

### å®Ÿè£…å†…å®¹

llama.cppã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã‚’èª¿æŸ»ã—ã€RusTorchã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’llama.cppäº’æ›ã«ä¿®æ­£ã—ã¾ã—ãŸã€‚

#### llama.cppã®å‹•ä½œè§£æ

**ã‚½ãƒ¼ã‚¹**: `/tmp/llama.cpp/src/llama-vocab.cpp:2756-2830`

1. **ã‚¹ãƒšãƒ¼ã‚¹ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—**: `" "` â†’ `"â–"` (U+2581)
2. **æ¡ä»¶ä»˜ããƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹**: ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ (BOSç­‰) ã®å¾Œã®ã¿ã‚¹ãƒšãƒ¼ã‚¹è¿½åŠ 
3. **ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ**: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã—

#### RusTorchã®ä¿®æ­£ (2025-10-11æ›´æ–°)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `example-cli/src/tokenizer/llama_spm.rs:290-299`

**å¤‰æ›´ç‚¹**:
- âŒ ä¿®æ­£å‰: BOSãƒˆãƒ¼ã‚¯ãƒ³å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãªã— â†’ `[1, 29966, ...]` (Token 29966 = `'<'`)
- âœ… ä¿®æ­£å¾Œ: BOSãƒˆãƒ¼ã‚¯ãƒ³å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ  â†’ `[1, 529, ...]` (Token 529 = `' <'`)

**æ ¹æ‹ **:
```rust
// llama.cpp SPM tokenizer adds a space prefix when:
// 1. Model has add_space_prefix=true (TinyLlama does)
// 2. Previous token is a special token (BOS)
let text_to_encode = if add_special_tokens && !text.is_empty() {
    format!(" {}", text)  // Add space prefix after BOS
} else {
    text.to_string()
};
```

**æ¤œè¨¼çµæœ** (2025-10-11):
- RusTorch: `[1, 529, 29989, 1792, ...]` âœ…
- llama.cpp: `[1, 529, 29989, 1792, ...]` âœ…
- **å®Œå…¨ä¸€è‡´** - Token 529 = `' <'` (ã‚¹ãƒšãƒ¼ã‚¹+<)

### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æœŸå¾…å‹•ä½œ

| å…¥åŠ› | å‰å‡¦ç† | ãƒˆãƒ¼ã‚¯ãƒ³ID |
|------|--------|------------|
| "1" | "1" | 29896 |
| " 1" | "â–1" | 29871, 29896 |

### æˆæœ

- âœ… llama.cppãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ­ã‚¸ãƒƒã‚¯å®Œå…¨ç§»æ¤
- âœ… ã‚¹ãƒšãƒ¼ã‚¹ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å•é¡Œè§£æ±º
- âœ… å®Ÿæ©Ÿãƒ†ã‚¹ãƒˆã§ç¢ºèªå®Œäº† (2025-10-11)

---

## ğŸ”´ Phase 7: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ä¿®æ­£å¾Œã®æ¤œè¨¼ (2025-10-11)

### å®Ÿæ–½æ—¥æ™‚
2025å¹´10æœˆ11æ—¥

### èƒŒæ™¯
ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’llama.cppäº’æ›ã«ä¿®æ­£å¾Œã€å‡ºåŠ›ãŒæ”¹å–„ã•ã‚Œã‚‹ã‹æ¤œè¨¼ã€‚

### æ¤œè¨¼å†…å®¹

#### 1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ä¸€è‡´ç¢ºèª âœ…
```
å…¥åŠ›: "<|user|>\n1<|assistant|>"
RusTorch:  [1, 529, 29989, 1792, 29989, 29958, 13, 29896, 29966, 29989, 465, 22137, 29989, 29958]
llama.cpp: [1, 529, 29989, 1792, 29989, 29958, 13, 29896, 29966, 29989, 465, 22137, 29989, 29958]
çµæœ: âœ… **å®Œå…¨ä¸€è‡´**
```

#### 2. å‡ºåŠ›å“è³ªã®ç¢ºèª âŒ
```bash
printf "1\n" | ./target/release/rustorch-cli \
  --model ~/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q8_0.gguf \
  --backend hybrid-f32 --max-tokens 50
```

**çµæœ**: `anthanthertanthertrun ChallengeniASEÃ¶rtrinder...`
- âŒ ä¾ç„¶ã¨ã—ã¦æ„å‘³ä¸æ˜ãªå‡ºåŠ›
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¯æ­£ã—ã„ãŒã€Transformerå±¤ã®å‡ºåŠ›ãŒä¸æ­£

#### 3. Logitsæ¯”è¼ƒ âŒ
```
Token     RusTorch    llama.cpp    Diff
0         -3.037      -7.701       4.665
13         3.540      19.808      16.268 â† æ”¹è¡Œãƒˆãƒ¼ã‚¯ãƒ³
...
ä¸€è‡´ç‡: 0/20 (0.0%)
Top token: RusTorch=9716, llama.cpp=13
```

### çµè«–

**ãƒˆãƒ¼ã‚¯ãƒ³åŒ–**: âœ… å®Œå…¨ä¿®æ­£
**Transformerå±¤**: âŒ ä¾ç„¶ã¨ã—ã¦ä¸æ­£

ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒæ­£ã—ãã¦ã‚‚Transformerå±¤ãŒé–“é•ã£ãŸå‡ºåŠ›ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã‚’æ„å‘³ã™ã‚‹ï¼š

1. **å…¥åŠ›ã¯æ­£ã—ã„** - Token sequence ãŒ llama.cpp ã¨ä¸€è‡´
2. **å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯æ­£ã—ã„** (Phase 1-6ã§æ¤œè¨¼æ¸ˆã¿)
   - Q8_0 dequantization âœ…
   - RoPE âœ…
   - RMSNorm âœ…
   - Attention âœ…
   - FFN âœ…
3. **å•é¡Œã¯çµ„ã¿åˆã‚ã›æ–¹** - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®é€£æºã«å•é¡Œ

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**Layer-by-layer hidden state comparison** ãŒå¿…è¦ï¼š
1. Layer 0ã®å…¥åŠ›hidden state
2. Layer 0ã®å‡ºåŠ›hidden state
3. Layer 1ã®å…¥åŠ›hidden state
... (å„å±¤ã§æ¯”è¼ƒ)

ã©ã®å±¤ã§divergenceãŒå§‹ã¾ã‚‹ã‹ã‚’ç‰¹å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

