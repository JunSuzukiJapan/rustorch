//! F32Tensor - f32å°‚ç”¨ãƒ†ãƒ³ã‚½ãƒ«å®Ÿè£…
//! F32Tensor - f32-specific tensor implementation
//!
//! å¤‰æ›ã‚³ã‚¹ãƒˆå®Œå…¨å‰Šé™¤ã‚’ç›®çš„ã¨ã—ãŸf32å°‚ç”¨ãƒ†ãƒ³ã‚½ãƒ«
//! f32-specific tensor aimed at complete conversion cost elimination

use crate::error::RusTorchResult;
use ndarray::{Array, IxDyn};
use std::sync::Arc;

/// ãƒ‡ãƒã‚¤ã‚¹æœ€é©åŒ–çŠ¶æ…‹
/// Device optimization state
#[derive(Debug, Clone)]
pub enum DeviceState {
    CPU,
    Metal { device_id: usize },
    CoreML { device_id: usize },
    Synchronized, // å…¨ãƒ‡ãƒã‚¤ã‚¹åŒæœŸæ¸ˆã¿
}

/// Metalå…±æœ‰ãƒãƒƒãƒ•ã‚¡ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
/// Metal shared buffer (placeholder)
#[derive(Debug)]
pub struct MetalBuffer {
    _device_id: usize,
    _size: usize,
}

impl MetalBuffer {
    pub fn new(device_id: usize, size: usize) -> Self {
        Self {
            _device_id: device_id,
            _size: size,
        }
    }
}

/// CoreMLå…±æœ‰ãƒãƒƒãƒ•ã‚¡ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
/// CoreML shared buffer (placeholder)
#[derive(Debug)]
pub struct CoreMLBuffer {
    _device_id: usize,
    _shape: Vec<usize>,
}

impl CoreMLBuffer {
    pub fn new(device_id: usize, shape: Vec<usize>) -> Self {
        Self {
            _device_id: device_id,
            _shape: shape,
        }
    }
}

/// f32å°‚ç”¨ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆå¤‰æ›ã‚³ã‚¹ãƒˆæœ€å°åŒ–ï¼‰
/// f32-specific tensor (conversion cost minimization)
#[derive(Debug)]
pub struct F32Tensor {
    /// CPUå´ãƒ‡ãƒ¼ã‚¿
    /// CPU-side data
    pub data: Array<f32, IxDyn>,

    /// GPUå…±æœ‰ãƒãƒƒãƒ•ã‚¡ï¼ˆMetalç”¨ï¼‰
    /// GPU shared buffer (for Metal)
    pub metal_buffer: Option<Arc<MetalBuffer>>,

    /// Neural Engineå…±æœ‰ãƒãƒƒãƒ•ã‚¡ï¼ˆCoreMLç”¨ï¼‰
    /// Neural Engine shared buffer (for CoreML)
    pub coreml_buffer: Option<Arc<CoreMLBuffer>>,

    /// ãƒ‡ãƒã‚¤ã‚¹æœ€é©åŒ–çŠ¶æ…‹
    /// Device optimization state
    pub device_state: DeviceState,

    /// å‹¾é…è¿½è·¡
    /// Gradient tracking
    pub requires_grad: bool,

    /// ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶
    /// Tensor shape
    shape: Vec<usize>,
}

impl F32Tensor {
    /// æ–°ã—ã„F32Tensorã‚’ä½œæˆ
    /// Create new F32Tensor
    pub fn new(data: Vec<f32>, shape: Vec<usize>) -> RusTorchResult<Self> {
        let total_elements: usize = shape.iter().product();

        if data.len() != total_elements {
            return Err(crate::error::RusTorchError::InvalidParameters {
                operation: "F32Tensor::new".to_string(),
                message: format!(
                    "Data length {} doesn't match shape elements {}",
                    data.len(),
                    total_elements
                ),
            });
        }

        let ndarray_data = Array::from_shape_vec(IxDyn(&shape), data)
            .map_err(|e| crate::error::RusTorchError::TensorOp {
                message: format!("Failed to create ndarray: {}", e),
                source: None,
            })?;

        Ok(Self {
            data: ndarray_data,
            metal_buffer: None,
            coreml_buffer: None,
            device_state: DeviceState::CPU,
            requires_grad: false,
            shape,
        })
    }

    /// ã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    /// Create zero tensor
    pub fn zeros(shape: &[usize]) -> Self {
        let total_elements: usize = shape.iter().product();
        let data = vec![0.0f32; total_elements];

        Self::new(data, shape.to_vec()).expect("Failed to create zero tensor")
    }

    /// ãƒ©ãƒ³ãƒ€ãƒ ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    /// Create random tensor
    pub fn randn(shape: &[usize]) -> Self {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        let total_elements: usize = shape.iter().product();
        let data: Vec<f32> = (0..total_elements)
            .map(|_| rng.gen_range(-1.0..1.0))
            .collect();

        Self::new(data, shape.to_vec()).expect("Failed to create random tensor")
    }

    /// ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã‚’å–å¾—
    /// Get tensor shape
    pub fn shape(&self) -> &[usize] {
        &self.shape
    }

    /// ãƒ‡ãƒã‚¤ã‚¹çŠ¶æ…‹ã‚’å–å¾—
    /// Get device state
    pub fn device_state(&self) -> &DeviceState {
        &self.device_state
    }


    /// ãƒ†ãƒ³ã‚½ãƒ«åŠ ç®—ï¼ˆf32å°‚ç”¨ï¼‰
    /// Tensor addition (f32-specific)
    pub fn add(&self, other: &F32Tensor) -> RusTorchResult<F32Tensor> {
        if self.shape != other.shape {
            return Err(crate::error::RusTorchError::InvalidParameters {
                operation: "F32Tensor::add".to_string(),
                message: format!(
                    "Shape mismatch: {:?} vs {:?}",
                    self.shape, other.shape
                ),
            });
        }

        let result_data: Vec<f32> = self.data.iter()
            .zip(other.data.iter())
            .map(|(a, b)| a + b)
            .collect();

        F32Tensor::new(result_data, self.shape.clone())
    }

    /// ãƒ†ãƒ³ã‚½ãƒ«åˆè¨ˆï¼ˆf32å°‚ç”¨ï¼‰
    /// Tensor sum (f32-specific)
    pub fn sum(&self) -> RusTorchResult<f32> {
        Ok(self.data.iter().sum::<f32>())
    }

    /// è¡Œåˆ—ä¹—ç®—ï¼ˆf32å°‚ç”¨ï¼‰
    /// Matrix multiplication (f32-specific)
    pub fn matmul(&self, other: &F32Tensor) -> RusTorchResult<F32Tensor> {
        if self.shape.len() != 2 || other.shape.len() != 2 {
            return Err(crate::error::RusTorchError::InvalidParameters {
                operation: "F32Tensor::matmul".to_string(),
                message: "Both tensors must be 2D matrices".to_string(),
            });
        }

        let (m, k1) = (self.shape[0], self.shape[1]);
        let (k2, n) = (other.shape[0], other.shape[1]);

        if k1 != k2 {
            return Err(crate::error::RusTorchError::InvalidParameters {
                operation: "F32Tensor::matmul".to_string(),
                message: format!(
                    "Matrix dimension mismatch: {}x{} Ã— {}x{}",
                    m, k1, k2, n
                ),
            });
        }

        // ã‚·ãƒ³ãƒ—ãƒ«ãªf32è¡Œåˆ—ä¹—ç®—å®Ÿè£…
        let mut result_data = vec![0.0f32; m * n];

        for i in 0..m {
            for j in 0..n {
                let mut sum = 0.0f32;
                for k in 0..k1 {
                    let a_val = self.data[[i, k]];
                    let b_val = other.data[[k, j]];
                    sum += a_val * b_val;
                }
                result_data[i * n + j] = sum;
            }
        }

        F32Tensor::new(result_data, vec![m, n])
    }
}

/// Cloneãƒˆãƒ¬ã‚¤ãƒˆã®å®Ÿè£…
/// Clone trait implementation
impl Clone for F32Tensor {
    fn clone(&self) -> Self {
        Self {
            data: self.data.clone(),
            metal_buffer: self.metal_buffer.clone(),
            coreml_buffer: self.coreml_buffer.clone(),
            device_state: self.device_state.clone(),
            requires_grad: self.requires_grad,
            shape: self.shape.clone(),
        }
    }
}

impl F32Tensor {
    /// Metal GPUã«ç§»å‹•ï¼ˆå¤‰æ›ãªã—ï¼‰
    /// Move to Metal GPU (no conversion)
    pub fn to_metal(&mut self, device_id: usize) -> RusTorchResult<()> {
        crate::hybrid_f32_experimental!();

        // Metalå…±æœ‰ãƒãƒƒãƒ•ã‚¡ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ Metal API ã‚’ä½¿ç”¨ï¼‰
        let buffer_size = self.data.len() * std::mem::size_of::<f32>();
        self.metal_buffer = Some(Arc::new(MetalBuffer::new(device_id, buffer_size)));
        self.device_state = DeviceState::Metal { device_id };

        println!("ğŸš€ F32Tensor moved to Metal GPU {} (zero-copy)", device_id);
        Ok(())
    }

    /// Neural Engineã«ç§»å‹•ï¼ˆå¤‰æ›ãªã—ï¼‰
    /// Move to Neural Engine (no conversion)
    pub fn to_coreml(&mut self, device_id: usize) -> RusTorchResult<()> {
        crate::hybrid_f32_experimental!();

        // CoreMLå…±æœ‰ãƒãƒƒãƒ•ã‚¡ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ MLMultiArray ã‚’ä½¿ç”¨ï¼‰
        self.coreml_buffer = Some(Arc::new(CoreMLBuffer::new(device_id, self.shape.clone())));
        self.device_state = DeviceState::CoreML { device_id };

        println!("ğŸ§  F32Tensor moved to Neural Engine {} (zero-copy)", device_id);
        Ok(())
    }

    /// CPUã«ç§»å‹•
    /// Move to CPU
    pub fn to_cpu(&mut self) -> RusTorchResult<()> {
        self.metal_buffer = None;
        self.coreml_buffer = None;
        self.device_state = DeviceState::CPU;

        println!("ğŸ’» F32Tensor moved to CPU");
        Ok(())
    }

    /// å…¨ãƒ‡ãƒã‚¤ã‚¹åŒæœŸ
    /// Synchronize all devices
    pub fn synchronize_all(&mut self) -> RusTorchResult<()> {
        crate::hybrid_f32_experimental!();

        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯å„ãƒ‡ãƒã‚¤ã‚¹é–“ã§ãƒ‡ãƒ¼ã‚¿åŒæœŸ
        self.device_state = DeviceState::Synchronized;

        println!("ğŸ”„ F32Tensor synchronized across all devices");
        Ok(())
    }

    /// è¡Œåˆ—ä¹—ç®—ï¼ˆå¤‰æ›ãƒ¬ã‚¹å®Ÿè¡Œï¼‰
    /// Matrix multiplication (conversion-less execution)
    pub fn matmul_optimized(&self, other: &F32Tensor) -> RusTorchResult<F32Tensor> {
        crate::hybrid_f32_experimental!();

        // æœ€é©ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
        let optimal_device = self.select_optimal_device_for_matmul(&other);

        match optimal_device {
            DeviceState::Metal { device_id } => {
                println!("âš¡ Executing matmul on Metal GPU {} (f32 direct)", device_id);
                self.execute_metal_matmul_f32(other)
            }
            DeviceState::CoreML { device_id } => {
                println!("ğŸ§  Executing matmul on Neural Engine {} (f32 direct)", device_id);
                self.execute_coreml_matmul_f32(other)
            }
            DeviceState::CPU => {
                println!("ğŸ’» Executing matmul on CPU (f32 direct)");
                self.execute_cpu_matmul_f32(other)
            }
            DeviceState::Synchronized => {
                println!("ğŸ”„ Executing matmul on synchronized devices");
                self.execute_cpu_matmul_f32(other) // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            }
        }
    }

    /// æœ€é©ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆè¡Œåˆ—ä¹—ç®—ç”¨ï¼‰
    /// Select optimal device (for matrix multiplication)
    fn select_optimal_device_for_matmul(&self, _other: &F32Tensor) -> DeviceState {
        let matrix_size = self.shape.iter().product::<usize>();

        match matrix_size {
            size if size > 50000 => DeviceState::Metal { device_id: 0 }, // å¤§è¦æ¨¡ â†’ Metal
            size if size > 1000 => DeviceState::CoreML { device_id: 0 }, // ä¸­è¦æ¨¡ â†’ Neural Engine
            _ => DeviceState::CPU, // å°è¦æ¨¡ â†’ CPU
        }
    }

    /// Metal GPU f32ç›´æ¥å®Ÿè¡Œ
    /// Metal GPU f32 direct execution
    fn execute_metal_matmul_f32(&self, other: &F32Tensor) -> RusTorchResult<F32Tensor> {
        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ Metal Performance Shaders ã‚’ä½¿ç”¨
        // ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…: CPUè¨ˆç®—ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        self.execute_cpu_matmul_f32(other)
    }

    /// Neural Engine f32ç›´æ¥å®Ÿè¡Œ
    /// Neural Engine f32 direct execution
    fn execute_coreml_matmul_f32(&self, other: &F32Tensor) -> RusTorchResult<F32Tensor> {
        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ CoreML MLCompute ã‚’ä½¿ç”¨
        // ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…: CPUè¨ˆç®—ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        self.execute_cpu_matmul_f32(other)
    }

    /// CPU f32ç›´æ¥å®Ÿè¡Œ
    /// CPU f32 direct execution
    fn execute_cpu_matmul_f32(&self, other: &F32Tensor) -> RusTorchResult<F32Tensor> {
        // ç°¡å˜ãªè¡Œåˆ—ä¹—ç®—å®Ÿè£…ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ BLAS ã‚’ä½¿ç”¨ï¼‰
        let a_shape = self.shape();
        let b_shape = other.shape();

        if a_shape.len() != 2 || b_shape.len() != 2 {
            return Err(crate::error::RusTorchError::InvalidParameters {
                operation: "matmul".to_string(),
                message: "Only 2D matrices supported in this experimental implementation".to_string(),
            });
        }

        if a_shape[1] != b_shape[0] {
            return Err(crate::error::RusTorchError::InvalidParameters {
                operation: "matmul".to_string(),
                message: format!(
                    "Matrix dimensions don't match: {}x{} and {}x{}",
                    a_shape[0], a_shape[1], b_shape[0], b_shape[1]
                ),
            });
        }

        let result_shape = vec![a_shape[0], b_shape[1]];
        let mut result_data = vec![0.0f32; result_shape.iter().product()];

        // å˜ç´”ãªè¡Œåˆ—ä¹—ç®—
        for i in 0..a_shape[0] {
            for j in 0..b_shape[1] {
                let mut sum = 0.0f32;
                for k in 0..a_shape[1] {
                    let a_idx = i * a_shape[1] + k;
                    let b_idx = k * b_shape[1] + j;
                    sum += self.data[[i, k]] * other.data[[k, j]];
                }
                let result_idx = i * b_shape[1] + j;
                result_data[result_idx] = sum;
            }
        }

        F32Tensor::new(result_data, result_shape)
    }

    /// ãƒ‡ãƒ¼ã‚¿ã‚’f32ã‚¹ãƒ©ã‚¤ã‚¹ã¨ã—ã¦å–å¾—
    /// Get data as f32 slice
    pub fn as_slice(&self) -> &[f32] {
        self.data.as_slice().unwrap()
    }

    /// ãƒ‡ãƒ¼ã‚¿ã®è¦ç´ æ•°ã‚’å–å¾—
    /// Get number of elements
    pub fn len(&self) -> usize {
        self.data.len()
    }

    /// ãƒ†ãƒ³ã‚½ãƒ«ãŒç©ºã‹ã©ã†ã‹
    /// Check if tensor is empty
    pub fn is_empty(&self) -> bool {
        self.data.is_empty()
    }
}