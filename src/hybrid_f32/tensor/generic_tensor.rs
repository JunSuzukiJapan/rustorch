// ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ãƒ†ãƒ³ã‚½ãƒ«çµ±ä¸€API
// Generic tensor unified API

use super::complex_tensor::ComplexTensor;
use super::core::F32Tensor;
use super::f64_tensor::F64Tensor;
use super::type_conversion::{TensorConversion, TensorVariant};
use crate::common::RusTorchResult;
use std::fmt::Debug;
use std::ops::{Add, Div, Mul, Neg, Sub};

/// ãƒ†ãƒ³ã‚½ãƒ«ã®åŸºæœ¬æ“ä½œã‚’å®šç¾©ã™ã‚‹ãƒˆãƒ¬ã‚¤ãƒˆ
/// Trait defining basic tensor operations
pub trait TensorOps: Debug + Clone {
    type Scalar: Copy + Debug;

    /// æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    /// Create a new tensor
    fn zeros(shape: &[usize]) -> RusTorchResult<Self>;
    fn ones(shape: &[usize]) -> RusTorchResult<Self>;
    fn randn(shape: &[usize]) -> RusTorchResult<Self>;

    /// åŸºæœ¬ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    /// Basic properties
    fn shape(&self) -> &[usize];
    fn ndim(&self) -> usize;
    fn numel(&self) -> usize;
    fn dtype(&self) -> &'static str;

    /// å‹¾é…è¨­å®š
    /// Gradient settings
    fn requires_grad_(self, requires_grad: bool) -> Self;

    /// å½¢çŠ¶æ“ä½œ
    /// Shape operations
    fn reshape(&self, new_shape: &[usize]) -> RusTorchResult<Self>;
    fn transpose(&self) -> RusTorchResult<Self>;
    fn unsqueeze(&self, dim: usize) -> RusTorchResult<Self>;
    fn expand(&self, new_shape: &[usize]) -> RusTorchResult<Self>;
    fn transpose_dims(&self, dim1: usize, dim2: usize) -> RusTorchResult<Self>;

    /// çµ±è¨ˆæ“ä½œ
    /// Statistical operations
    fn sum_scalar(&self) -> Self::Scalar;
    fn mean_scalar(&self) -> Self::Scalar;
}

/// è¡Œåˆ—æ¼”ç®—ã‚’å®šç¾©ã™ã‚‹ãƒˆãƒ¬ã‚¤ãƒˆ
/// Trait defining matrix operations
pub trait MatrixOps: TensorOps {
    /// è¡Œåˆ—ç©
    /// Matrix multiplication
    fn matmul(&self, other: &Self) -> RusTorchResult<Self>;

    /// è»¢ç½®
    /// Transpose
    fn t(&self) -> RusTorchResult<Self> {
        self.transpose()
    }
}

/// æ•°å­¦é–¢æ•°ã‚’å®šç¾©ã™ã‚‹ãƒˆãƒ¬ã‚¤ãƒˆ
/// Trait defining mathematical functions
pub trait MathOps: TensorOps {
    /// æŒ‡æ•°é–¢æ•°
    /// Exponential function
    fn exp(&self) -> RusTorchResult<Self>;

    /// å¯¾æ•°é–¢æ•°
    /// Logarithm function
    fn log(&self) -> RusTorchResult<Self>;

    /// ã¹ãä¹—
    /// Power function
    fn pow(&self, exponent: f64) -> RusTorchResult<Self>;

    /// å¹³æ–¹æ ¹
    /// Square root
    fn sqrt(&self) -> RusTorchResult<Self>;

    /// ä¸‰è§’é–¢æ•°
    /// Trigonometric functions
    fn sin(&self) -> RusTorchResult<Self>;
    fn cos(&self) -> RusTorchResult<Self>;
    fn tan(&self) -> RusTorchResult<Self>;

    /// ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
    /// Softmax
    fn softmax(&self, dim: Option<usize>) -> RusTorchResult<Self>;
}

/// è¤‡ç´ æ•°å°‚ç”¨æ“ä½œã‚’å®šç¾©ã™ã‚‹ãƒˆãƒ¬ã‚¤ãƒˆ
/// Trait defining complex-specific operations
pub trait ComplexOps: TensorOps {
    type RealTensor: TensorOps;

    /// å®Ÿéƒ¨ã‚’å–å¾—
    /// Get real part
    fn real(&self) -> Self::RealTensor;

    /// è™šéƒ¨ã‚’å–å¾—
    /// Get imaginary part
    fn imag(&self) -> Self::RealTensor;

    /// çµ¶å¯¾å€¤ã‚’å–å¾—
    /// Get absolute value
    fn abs(&self) -> Self::RealTensor;

    /// ä½ç›¸ã‚’å–å¾—
    /// Get phase
    fn angle(&self) -> Self::RealTensor;

    /// å…±å½¹è¤‡ç´ æ•°ã‚’å–å¾—
    /// Get complex conjugate
    fn conj(&self) -> Self;
}

// F32Tensorç”¨å®Ÿè£…
impl TensorOps for F32Tensor {
    type Scalar = f32;

    fn zeros(shape: &[usize]) -> RusTorchResult<Self> {
        F32Tensor::zeros(shape)
    }

    fn ones(shape: &[usize]) -> RusTorchResult<Self> {
        F32Tensor::ones(shape)
    }

    fn randn(shape: &[usize]) -> RusTorchResult<Self> {
        F32Tensor::randn(shape)
    }

    fn shape(&self) -> &[usize] {
        self.shape()
    }

    fn ndim(&self) -> usize {
        self.ndim()
    }

    fn numel(&self) -> usize {
        self.numel()
    }

    fn dtype(&self) -> &'static str {
        "f32"
    }

    fn requires_grad_(self, requires_grad: bool) -> Self {
        let mut result = self;
        result.requires_grad = requires_grad;
        result
    }

    fn reshape(&self, new_shape: &[usize]) -> RusTorchResult<Self> {
        self.reshape(new_shape)
    }

    fn transpose(&self) -> RusTorchResult<Self> {
        self.transpose()
    }

    fn unsqueeze(&self, dim: usize) -> RusTorchResult<Self> {
        self.unsqueeze(dim)
    }

    fn expand(&self, new_shape: &[usize]) -> RusTorchResult<Self> {
        self.expand(new_shape)
    }

    fn transpose_dims(&self, dim1: usize, dim2: usize) -> RusTorchResult<Self> {
        self.transpose_dims(dim1, dim2)
    }

    fn sum_scalar(&self) -> Self::Scalar {
        self.sum().unwrap_or(0.0)
    }

    fn mean_scalar(&self) -> Self::Scalar {
        // meané–¢æ•°ãŒãªã„ã®ã§ã€sum/lengthã§è¨ˆç®—
        let sum_val = self.sum().unwrap_or(0.0);
        let len = self.data.len() as f32;
        if len > 0.0 {
            sum_val / len
        } else {
            0.0
        }
    }
}

impl MatrixOps for F32Tensor {
    fn matmul(&self, other: &Self) -> RusTorchResult<Self> {
        eprintln!("ğŸ¯ [TRAIT_MATMUL] MatrixOps::matmul called");
        self.matmul(other)
    }
}

impl MathOps for F32Tensor {
    fn exp(&self) -> RusTorchResult<Self> {
        let exp_data = self.data.mapv(|x| x.exp());
        let (data_vec, _offset) = exp_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn log(&self) -> RusTorchResult<Self> {
        let log_data = self.data.mapv(|x| x.ln());
        let (data_vec, _offset) = log_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn pow(&self, exponent: f64) -> RusTorchResult<Self> {
        let pow_data = self.data.mapv(|x| x.powf(exponent as f32));
        let (data_vec, _offset) = pow_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn sqrt(&self) -> RusTorchResult<Self> {
        let sqrt_data = self.data.mapv(|x| x.sqrt());
        let (data_vec, _offset) = sqrt_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn sin(&self) -> RusTorchResult<Self> {
        let sin_data = self.data.mapv(|x| x.sin());
        let (data_vec, _offset) = sin_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn cos(&self) -> RusTorchResult<Self> {
        let cos_data = self.data.mapv(|x| x.cos());
        let (data_vec, _offset) = cos_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn tan(&self) -> RusTorchResult<Self> {
        let tan_data = self.data.mapv(|x| x.tan());
        let (data_vec, _offset) = tan_data.into_raw_vec_and_offset();
        let shape = self.shape();
        let mut result = F32Tensor::new(data_vec, &shape)?;
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn softmax(&self, dim: Option<usize>) -> RusTorchResult<Self> {
        self.softmax(dim)
    }
}

// F64Tensorç”¨å®Ÿè£…
impl TensorOps for F64Tensor {
    type Scalar = f64;

    fn zeros(shape: &[usize]) -> RusTorchResult<Self> {
        F64Tensor::zeros(shape)
    }

    fn ones(shape: &[usize]) -> RusTorchResult<Self> {
        F64Tensor::ones(shape)
    }

    fn randn(shape: &[usize]) -> RusTorchResult<Self> {
        F64Tensor::randn(shape)
    }

    fn shape(&self) -> &[usize] {
        self.shape()
    }

    fn ndim(&self) -> usize {
        self.ndim()
    }

    fn numel(&self) -> usize {
        self.numel()
    }

    fn dtype(&self) -> &'static str {
        "f64"
    }

    fn requires_grad_(self, requires_grad: bool) -> Self {
        let mut result = self;
        result.requires_grad = requires_grad;
        result
    }

    fn reshape(&self, new_shape: &[usize]) -> RusTorchResult<Self> {
        self.reshape(new_shape)
    }

    fn transpose(&self) -> RusTorchResult<Self> {
        self.transpose()
    }

    fn unsqueeze(&self, dim: usize) -> RusTorchResult<Self> {
        self.unsqueeze(dim)
    }

    fn expand(&self, new_shape: &[usize]) -> RusTorchResult<Self> {
        self.expand(new_shape)
    }

    fn transpose_dims(&self, dim1: usize, dim2: usize) -> RusTorchResult<Self> {
        self.transpose_dims(dim1, dim2)
    }

    fn sum_scalar(&self) -> Self::Scalar {
        self.sum()
    }

    fn mean_scalar(&self) -> Self::Scalar {
        // meané–¢æ•°ãŒãªã„ã®ã§ã€sum/lengthã§è¨ˆç®—
        let sum_val = self.sum();
        let len = self.data.len() as f64;
        if len > 0.0 {
            sum_val / len
        } else {
            0.0
        }
    }
}

impl MatrixOps for F64Tensor {
    fn matmul(&self, other: &Self) -> RusTorchResult<Self> {
        self.matmul(other)
    }
}

impl MathOps for F64Tensor {
    fn exp(&self) -> RusTorchResult<Self> {
        let exp_data = self.data.mapv(|x| x.exp());
        let mut result = F64Tensor::new(exp_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn log(&self) -> RusTorchResult<Self> {
        let log_data = self.data.mapv(|x| x.ln());
        let mut result = F64Tensor::new(log_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn pow(&self, exponent: f64) -> RusTorchResult<Self> {
        let pow_data = self.data.mapv(|x| x.powf(exponent));
        let mut result = F64Tensor::new(pow_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn sqrt(&self) -> RusTorchResult<Self> {
        let sqrt_data = self.data.mapv(|x| x.sqrt());
        let mut result = F64Tensor::new(sqrt_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn sin(&self) -> RusTorchResult<Self> {
        let sin_data = self.data.mapv(|x| x.sin());
        let mut result = F64Tensor::new(sin_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn cos(&self) -> RusTorchResult<Self> {
        let cos_data = self.data.mapv(|x| x.cos());
        let mut result = F64Tensor::new(cos_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn tan(&self) -> RusTorchResult<Self> {
        let tan_data = self.data.mapv(|x| x.tan());
        let mut result = F64Tensor::new(tan_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn softmax(&self, dim: Option<usize>) -> RusTorchResult<Self> {
        self.softmax(dim)
    }
}

// ComplexTensorç”¨å®Ÿè£…
impl TensorOps for ComplexTensor {
    type Scalar = num_complex::Complex64;

    fn zeros(shape: &[usize]) -> RusTorchResult<Self> {
        ComplexTensor::zeros(shape)
    }

    fn ones(shape: &[usize]) -> RusTorchResult<Self> {
        ComplexTensor::ones(shape)
    }

    fn randn(shape: &[usize]) -> RusTorchResult<Self> {
        // è¤‡ç´ æ•°ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ†ãƒ³ã‚½ãƒ«ï¼šå®Ÿéƒ¨ã¨è™šéƒ¨ãŒç‹¬ç«‹ã—ãŸæ­£è¦åˆ†å¸ƒ
        use ndarray::Array;
        use ndarray_rand::rand_distr::StandardNormal;
        use ndarray_rand::RandomExt;
        use num_complex::Complex64;

        let real_part: Array<f64, _> = Array::random(shape, StandardNormal);
        let imag_part: Array<f64, _> = Array::random(shape, StandardNormal);

        let complex_data = real_part
            .iter()
            .zip(imag_part.iter())
            .map(|(&r, &i)| Complex64::new(r, i))
            .collect::<Vec<_>>();

        let data = Array::from_shape_vec(shape, complex_data)
            .map_err(|e| crate::error::RusTorchError::tensor_op(e.to_string()))?;

        Ok(ComplexTensor::new(data))
    }

    fn shape(&self) -> &[usize] {
        self.shape()
    }

    fn ndim(&self) -> usize {
        self.ndim()
    }

    fn numel(&self) -> usize {
        self.numel()
    }

    fn dtype(&self) -> &'static str {
        "complex64"
    }

    fn requires_grad_(self, requires_grad: bool) -> Self {
        let mut result = self;
        result.requires_grad = requires_grad;
        result
    }

    fn reshape(&self, new_shape: &[usize]) -> RusTorchResult<Self> {
        self.reshape(new_shape)
    }

    fn transpose(&self) -> RusTorchResult<Self> {
        self.transpose()
    }

    fn unsqueeze(&self, dim: usize) -> RusTorchResult<Self> {
        self.unsqueeze(dim)
    }

    fn expand(&self, new_shape: &[usize]) -> RusTorchResult<Self> {
        self.expand(new_shape)
    }

    fn transpose_dims(&self, dim1: usize, dim2: usize) -> RusTorchResult<Self> {
        self.transpose_dims(dim1, dim2)
    }

    fn sum_scalar(&self) -> Self::Scalar {
        self.sum()
    }

    fn mean_scalar(&self) -> Self::Scalar {
        use num_complex::Complex64;
        // meané–¢æ•°ãŒãªã„ã®ã§ã€sum/lengthã§è¨ˆç®—
        let sum_val = self.sum();
        let len = self.data.len() as f64;
        if len > 0.0 {
            sum_val / len
        } else {
            Complex64::new(0.0, 0.0)
        }
    }
}

impl MatrixOps for ComplexTensor {
    fn matmul(&self, other: &Self) -> RusTorchResult<Self> {
        self.matmul(other)
    }
}

impl MathOps for ComplexTensor {
    fn exp(&self) -> RusTorchResult<Self> {
        let exp_data = self.data.mapv(|x| x.exp());
        let mut result = ComplexTensor::new(exp_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn log(&self) -> RusTorchResult<Self> {
        let log_data = self.data.mapv(|x| x.ln());
        let mut result = ComplexTensor::new(log_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn pow(&self, exponent: f64) -> RusTorchResult<Self> {
        use num_complex::Complex64;
        let exp_complex = Complex64::new(exponent, 0.0);
        let pow_data = self.data.mapv(|x| x.powc(exp_complex));
        let mut result = ComplexTensor::new(pow_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn sqrt(&self) -> RusTorchResult<Self> {
        let sqrt_data = self.data.mapv(|x| x.sqrt());
        let mut result = ComplexTensor::new(sqrt_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn sin(&self) -> RusTorchResult<Self> {
        let sin_data = self.data.mapv(|x| x.sin());
        let mut result = ComplexTensor::new(sin_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn cos(&self) -> RusTorchResult<Self> {
        let cos_data = self.data.mapv(|x| x.cos());
        let mut result = ComplexTensor::new(cos_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn tan(&self) -> RusTorchResult<Self> {
        let tan_data = self.data.mapv(|x| x.tan());
        let mut result = ComplexTensor::new(tan_data);
        result.requires_grad = self.requires_grad;
        Ok(result)
    }

    fn softmax(&self, _dim: Option<usize>) -> RusTorchResult<Self> {
        // è¤‡ç´ æ•°ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã¯å®šç¾©ãŒè¤‡é›‘ãªãŸã‚ã€å®Ÿéƒ¨ã®ã¿ã«é©ç”¨
        Err(crate::error::RusTorchError::InvalidOperation(
            "Softmax is not well-defined for complex tensors".to_string(),
        ))
    }
}

impl ComplexOps for ComplexTensor {
    type RealTensor = F64Tensor;

    fn real(&self) -> Self::RealTensor {
        F64Tensor::new(self.real())
    }

    fn imag(&self) -> Self::RealTensor {
        F64Tensor::new(self.imag())
    }

    fn abs(&self) -> Self::RealTensor {
        F64Tensor::new(self.abs())
    }

    fn angle(&self) -> Self::RealTensor {
        F64Tensor::new(self.angle())
    }

    fn conj(&self) -> Self {
        self.conj()
    }
}

/// ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
/// Helper functions for generic tensor operations
pub struct TensorUtils;

impl TensorUtils {
    /// å‹å®‰å…¨ãªåŠ ç®—
    /// Type-safe addition
    pub fn add<T>(a: &T, b: &T) -> RusTorchResult<T>
    where
        T: TensorOps + Add<Output = T> + Clone,
    {
        Ok(a.clone() + b.clone())
    }

    /// å‹å®‰å…¨ãªè¡Œåˆ—ç©
    /// Type-safe matrix multiplication
    pub fn matmul<T>(a: &T, b: &T) -> RusTorchResult<T>
    where
        T: MatrixOps,
    {
        a.matmul(b)
    }

    /// å‹ã‚’æŒ‡å®šã—ãŸãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    /// Create tensor with specified type
    pub fn zeros_like<T>(tensor: &T) -> RusTorchResult<T>
    where
        T: TensorOps,
    {
        T::zeros(tensor.shape())
    }

    /// å‹ã‚’æŒ‡å®šã—ãŸãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
    /// Create tensor with specified type
    pub fn ones_like<T>(tensor: &T) -> RusTorchResult<T>
    where
        T: TensorOps,
    {
        T::ones(tensor.shape())
    }

    /// ãƒ†ãƒ³ã‚½ãƒ«ã®å‹æƒ…å ±ã‚’è¡¨ç¤º
    /// Display tensor type information
    pub fn tensor_info<T>(tensor: &T) -> String
    where
        T: TensorOps,
    {
        format!(
            "Tensor(dtype={}, shape={:?}, numel={})",
            tensor.dtype(),
            tensor.shape(),
            tensor.numel()
        )
    }
}

/// ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ãƒ†ãƒ³ã‚½ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒª
/// Generic tensor factory
pub struct TensorFactory;

impl TensorFactory {
    /// æŒ‡å®šã—ãŸç²¾åº¦ã§ã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    /// Create zero tensor with specified precision
    pub fn zeros(dtype: &str, shape: &[usize]) -> RusTorchResult<TensorVariant> {
        match dtype {
            "f32" => Ok(TensorVariant::F32(F32Tensor::zeros(shape)?)),
            "f64" => Ok(TensorVariant::F64(F64Tensor::zeros(shape)?)),
            "complex64" => Ok(TensorVariant::Complex(ComplexTensor::zeros(shape)?)),
            _ => Err(crate::error::RusTorchError::InvalidOperation(format!(
                "Unsupported dtype: {}",
                dtype
            ))),
        }
    }

    /// æŒ‡å®šã—ãŸç²¾åº¦ã§ãƒ¯ãƒ³ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    /// Create ones tensor with specified precision
    pub fn ones(dtype: &str, shape: &[usize]) -> RusTorchResult<TensorVariant> {
        match dtype {
            "f32" => Ok(TensorVariant::F32(F32Tensor::ones(shape)?)),
            "f64" => Ok(TensorVariant::F64(F64Tensor::ones(shape)?)),
            "complex64" => Ok(TensorVariant::Complex(ComplexTensor::ones(shape)?)),
            _ => Err(crate::error::RusTorchError::InvalidOperation(format!(
                "Unsupported dtype: {}",
                dtype
            ))),
        }
    }

    /// æŒ‡å®šã—ãŸç²¾åº¦ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ
    /// Create random tensor with specified precision
    pub fn randn(dtype: &str, shape: &[usize]) -> RusTorchResult<TensorVariant> {
        match dtype {
            "f32" => Ok(TensorVariant::F32(F32Tensor::randn(shape)?)),
            "f64" => Ok(TensorVariant::F64(F64Tensor::randn(shape)?)),
            "complex64" => Ok(TensorVariant::Complex(ComplexTensor::randn(shape)?)),
            _ => Err(crate::error::RusTorchError::InvalidOperation(format!(
                "Unsupported dtype: {}",
                dtype
            ))),
        }
    }

    /// æœ€é©ãªç²¾åº¦ã‚’è‡ªå‹•é¸æŠ
    /// Automatically select optimal precision
    pub fn auto_precision(
        shape: &[usize],
        requires_high_precision: bool,
        is_complex: bool,
    ) -> RusTorchResult<TensorVariant> {
        let dtype = match (requires_high_precision, is_complex) {
            (true, true) => "complex64",
            (true, false) => "f64",
            (false, true) => "complex64", // è¤‡ç´ æ•°ã¯å¸¸ã«é«˜ç²¾åº¦
            (false, false) => "f32",
        };
        Self::zeros(dtype, shape)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::hybrid_f32::tensor::{ComplexTensor, F32Tensor, F64Tensor};

    #[test]
    fn test_generic_tensor_ops() {
        let f32_tensor = F32Tensor::ones(&[2, 2]).unwrap();
        let f64_tensor = F64Tensor::ones(&[2, 2]).unwrap();

        assert_eq!(f32_tensor.dtype(), "f32");
        assert_eq!(f64_tensor.dtype(), "f64");
        assert_eq!(f32_tensor.shape(), f64_tensor.shape());
    }

    #[test]
    fn test_tensor_factory() {
        let f32_zeros = TensorFactory::zeros("f32", &[2, 2]).unwrap();
        let f64_ones = TensorFactory::ones("f64", &[3, 3]).unwrap();
        let complex_randn = TensorFactory::randn("complex64", &[2, 2]).unwrap();

        assert_eq!(f32_zeros.dtype(), "f32");
        assert_eq!(f64_ones.dtype(), "f64");
        assert_eq!(complex_randn.dtype(), "complex64");
    }

    #[test]
    fn test_tensor_utils() {
        let tensor = F32Tensor::ones(&[2, 2]).unwrap();
        let info = TensorUtils::tensor_info(&tensor);

        assert!(info.contains("f32"));
        assert!(info.contains("[2, 2]"));
        assert!(info.contains("4"));
    }

    #[test]
    fn test_math_ops() {
        let tensor = F32Tensor::ones(&[2, 2]).unwrap();
        let exp_tensor = tensor.exp().unwrap();
        let log_tensor = tensor.log().unwrap();

        // e^1 â‰ˆ 2.718
        assert!((exp_tensor.sum().unwrap() - 4.0 * std::f32::consts::E).abs() < 1e-5);
        // ln(1) = 0
        assert!(log_tensor.sum().unwrap().abs() < 1e-5);
    }

    #[test]
    fn test_complex_ops() {
        let complex_tensor = ComplexTensor::ones(&[2, 2]).unwrap();
        let real_part = complex_tensor.real();
        let imag_part = complex_tensor.imag();
        let conj_tensor = complex_tensor.conj();

        // Note: real/imag return ArrayBase, not tensor objects
        // Just verify they return data
        assert!(real_part.len() > 0);
        assert!(imag_part.len() > 0);
        assert_eq!(conj_tensor.dtype(), "complex64");
    }
}
