/// Factory functions for automatic model creation from GGUF files
/// GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°

use crate::error::{RusTorchError, RusTorchResult};
use crate::formats::gguf::GGUFLoader;
use crate::models::architecture::ModelArchitecture;
use crate::models::llama::LlamaModel;

/// Create a model from GGUF file with automatic architecture detection
/// ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
///
/// This function reads the GGUF metadata to determine the model architecture
/// and creates the appropriate model type.
/// ã“ã®é–¢æ•°ã¯GGUFãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åˆ¤å®šã—ã€
/// é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’ä½œæˆã—ã¾ã™ã€‚
///
/// # Arguments
/// * `gguf_path` - Path to the GGUF model file
///
/// # Returns
/// * `LlamaModel` - Currently only LLaMA models are supported
///
/// # Example
/// ```no_run
/// use rustorch::models::factory::create_model_from_gguf;
///
/// let model = create_model_from_gguf("models/tinyllama.gguf").unwrap();
/// ```
///
/// # Supported Architectures
/// - LLaMA (Meta)
/// - Mistral (mapped to LLaMA for compatibility)
/// - Future: Phi, Gemma, Qwen
pub fn create_model_from_gguf(gguf_path: &str) -> RusTorchResult<LlamaModel> {
    // Load GGUF metadata to determine architecture
    let loader = GGUFLoader::from_file(gguf_path)?;

    // Detect architecture from metadata
    let arch_str = loader.get_architecture()
        .unwrap_or_else(|| "llama".to_string());

    let architecture = ModelArchitecture::from_gguf_string(&arch_str)
        .unwrap_or(ModelArchitecture::LLaMA);

    eprintln!("ðŸ” Detected architecture: {} (from GGUF metadata: {})", architecture, arch_str);

    // Create appropriate model based on architecture
    match architecture {
        ModelArchitecture::LLaMA | ModelArchitecture::Mistral => {
            // Mistral uses the same architecture as LLaMA with minor differences
            // Mistralã¯LLaMAã¨åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ï¼ˆã‚ãšã‹ãªé•ã„ã‚ã‚Šï¼‰
            LlamaModel::from_gguf(gguf_path)
        }
        _ => {
            // For now, fall back to LLaMA for unsupported architectures
            // ç¾æ™‚ç‚¹ã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯LLaMAã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            eprintln!("âš ï¸  Architecture {} not yet fully supported, using LLaMA compatibility mode", architecture);
            LlamaModel::from_gguf(gguf_path)
        }
    }
}

/// Detect model architecture from GGUF file without loading the full model
/// ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã›ãšã«GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¤œå‡º
///
/// # Arguments
/// * `gguf_path` - Path to the GGUF model file
///
/// # Returns
/// * `ModelArchitecture` - Detected architecture
///
/// # Example
/// ```no_run
/// use rustorch::models::factory::detect_architecture;
///
/// let arch = detect_architecture("models/tinyllama.gguf").unwrap();
/// println!("Detected: {}", arch);
/// ```
pub fn detect_architecture(gguf_path: &str) -> RusTorchResult<ModelArchitecture> {
    let loader = GGUFLoader::from_file(gguf_path)?;

    let arch_str = loader.get_architecture()
        .ok_or_else(|| RusTorchError::ParseError(
            "No architecture metadata found in GGUF file".to_string()
        ))?;

    ModelArchitecture::from_gguf_string(&arch_str)
        .ok_or_else(|| RusTorchError::ParseError(
            format!("Unknown architecture: {}", arch_str)
        ))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_architecture_detection_unit() {
        // Unit test for architecture string parsing
        let arch = ModelArchitecture::from_gguf_string("llama");
        assert_eq!(arch, Some(ModelArchitecture::LLaMA));

        let arch = ModelArchitecture::from_gguf_string("mistral");
        assert_eq!(arch, Some(ModelArchitecture::Mistral));
    }
}
