//! # RusTorch ğŸš€
//! 
//! **A production-ready deep learning library in Rust with PyTorch-like API, combining safety and speed**
//! 
//! RusTorch is a fully functional deep learning library that leverages Rust's safety and performance,
//! providing comprehensive tensor operations, automatic differentiation, neural network layers,
//! transformer architectures, GPU acceleration, and advanced memory optimization features.
//! 
//! ## âœ¨ Key Features
//! 
//! - **ğŸ”¥ Comprehensive Tensor Operations**: Math operations, broadcasting, indexing, and statistics
//! - **ğŸ¤– Transformer Architecture**: Complete transformer implementation with multi-head attention
//! - **âš¡ SIMD Optimizations**: AVX2/SSE4.1 vectorized operations for high performance
//! - **ğŸ”„ Unified Parallel Operations**: Trait-based parallel tensor operations with intelligent scheduling
//! - **ğŸ® GPU Integration**: CUDA/Metal/OpenCL support with automatic device selection
//! - **ğŸ’¾ Advanced Memory Management**: Zero-copy operations, SIMD-aligned allocation, and memory pools
//! - **ğŸ§  Automatic Differentiation**: Tape-based computational graph for gradient computation
//! - **ğŸ—ï¸ Neural Network Layers**: Linear, Conv2d, RNN/LSTM/GRU, BatchNorm, Dropout, and more
//! 
//! ## ğŸš€ Quick Start
//! 
//! ```rust
//! use rustorch::prelude::*;
//! 
//! // Create tensors
//! let a = Tensor::from_vec(vec![1.0f32, 2.0, 3.0, 4.0], vec![2, 2]);
//! let b = Tensor::from_vec(vec![5.0f32, 6.0, 7.0, 8.0], vec![2, 2]);
//! 
//! // Basic operations
//! let c = &a + &b;  // Addition
//! let d = a.matmul(&b);  // Matrix multiplication
//! 
//! // Mathematical functions
//! let e = a.sin();  // Sine function
//! let f = a.exp();  // Exponential function
//! 
//! println!("Shape: {:?}", c.shape());
//! println!("Result: {:?}", c.as_slice());
//! ```
//! 
//! ## ğŸ—ï¸ Architecture Overview
//! 
//! The library is organized into several key modules:
//! 
//! - [`tensor`]: Core tensor operations with parallel and GPU acceleration
//! - [`nn`]: Neural network layers and building blocks
//! - [`autograd`]: Automatic differentiation system
//! - [`optim`]: Optimization algorithms (SGD, Adam, etc.)
//! - [`gpu`]: GPU acceleration support (CUDA, Metal, OpenCL)
//! - [`simd`]: SIMD vectorized operations
//! - [`memory`]: Advanced memory management and pooling
//! - [`data`]: Data loading and processing utilities
//! 
//! ## ğŸ”„ Parallel Operations
//! 
//! RusTorch provides a unified trait-based system for parallel tensor operations:
//! 
//! ```rust
//! use rustorch::tensor::{Tensor, parallel_traits::*};
//! 
//! let tensor1 = Tensor::<f32>::ones(&[1000, 1000]);
//! let tensor2 = Tensor::<f32>::ones(&[1000, 1000]);
//! 
//! // Automatic parallel execution for large tensors
//! let result = tensor1.batch_elementwise_op(&tensor2, |a, b| a + b).unwrap();
//! 
//! // Parallel matrix multiplication
//! let matmul_result = tensor1.batch_matmul(&tensor2).unwrap();
//! 
//! // Parallel reduction operations
//! let sum = tensor1.parallel_sum(0).unwrap();
//! ```
//! 
//! ## ğŸ® GPU Integration
//! 
//! Seamless GPU acceleration with automatic device selection:
//! 
//! ```rust
//! use rustorch::tensor::{Tensor, gpu_parallel::*};
//! use rustorch::gpu::DeviceType;
//! 
//! let tensor1 = Tensor::<f32>::ones(&[1000, 1000]);
//! let tensor2 = Tensor::<f32>::ones(&[1000, 1000]);
//! 
//! // GPU-accelerated operations with automatic fallback
//! let result = tensor1.gpu_elementwise_op(&tensor2, |a, b| a + b).unwrap();
//! 
//! // Transfer tensors between devices
//! let gpu_tensor = tensor1.to_device(DeviceType::Cuda(0)).unwrap();
//! let cpu_tensor = gpu_tensor.to_cpu().unwrap();
//! ```
//! 
//! ## ğŸ’¾ Memory Optimization
//! 
//! Advanced memory management for optimal performance:
//! 
//! ```rust
//! use rustorch::tensor::{Tensor, memory_optimized::*, zero_copy::*};
//! 
//! // Memory-optimized tensor operations
//! let config = MemoryOptimizedConfig {
//!     strategy: AllocationStrategy::Pool,
//!     enable_inplace: true,
//!     ..Default::default()
//! };
//! 
//! let tensor = Tensor::<f32>::ones(&[1000, 1000]);
//! let optimized = tensor.with_memory_strategy(&config);
//! 
//! // Zero-copy operations
//! let view = tensor.zero_copy_view();
//! let result = view.elementwise_with(&view, |a, b| a * 2.0).unwrap();
//! ```

#![warn(missing_docs)]
#![warn(rustdoc::missing_crate_level_docs)]

/// Common utilities and shared functionality
/// å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨å…±æœ‰æ©Ÿèƒ½
pub mod common;
/// Tensor operations and data structures
/// ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
pub mod tensor;
/// Automatic differentiation module
/// è‡ªå‹•å¾®åˆ†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
pub mod autograd;
/// Neural network layers and building blocks
/// ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨æ§‹æˆè¦ç´ 
pub mod nn;
/// Optimization algorithms
/// æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
pub mod optim;
/// Data loading and processing utilities
/// ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‡¦ç†ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
pub mod data;
/// GPU acceleration support (CUDA, Metal, OpenCL)
/// GPUåŠ é€Ÿã‚µãƒãƒ¼ãƒˆï¼ˆCUDAã€Metalã€OpenCLï¼‰
pub mod gpu;
/// Distributed training support for multi-GPU and multi-machine training
/// ãƒãƒ«ãƒGPUãŠã‚ˆã³ãƒãƒ«ãƒãƒã‚·ãƒ³å­¦ç¿’ç”¨åˆ†æ•£å­¦ç¿’ã‚µãƒãƒ¼ãƒˆ
pub mod distributed;
/// Memory management and pooling utilities
/// ãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ãƒ—ãƒ¼ãƒªãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
pub mod memory;
/// SIMD vectorized operations for performance optimization
/// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãŸã‚ã®SIMDãƒ™ã‚¯ãƒˆãƒ«åŒ–æ“ä½œ
pub mod simd;
/// Utility functions
/// ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
pub mod utils;
/// Pre-built models and architectures
/// äº‹å‰æ§‹ç¯‰ãƒ¢ãƒ‡ãƒ«ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
pub mod models;

/// Re-exports of commonly used items
pub mod prelude {
    pub use crate::tensor::Tensor;
    pub use crate::nn::{Module, Linear, Conv2d, MaxPool2d, AvgPool2d, BatchNorm1d, BatchNorm2d, Dropout, AlphaDropout, dropout, RNNCell, RNN, LSTMCell, LSTM, GRUCell, GRU};
    pub use crate::autograd::Variable;
    pub use crate::nn::activation::{relu, sigmoid, tanh, leaky_relu, softmax, gelu, swish, elu, selu, mish, hardswish};
    pub use crate::nn::loss::{mse_loss, binary_cross_entropy, cross_entropy, nll_loss, huber_loss};
    pub use crate::optim::{Optimizer, SGD, Adam, LRScheduler, StepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau};
    pub use crate::data::{Dataset, TensorDataset, DataLoader};
    pub use crate::models::{Model, ModelMode, ModelBuilder, CNN, CNNBuilder, ResNet, ResNetBuilder};
    pub use crate::models::{RNNModel, RNNModelBuilder, LSTMModel, LSTMModelBuilder};
    pub use crate::models::{TransformerModel, TransformerModelBuilder, BERT, BERTBuilder};
    pub use crate::models::{Trainer, TrainingConfig, TrainingResult, InferenceEngine, Metrics};
    pub use crate::models::{ModelSaver, ModelLoader, SerializationFormat};
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_tensor_creation() {
        let t = tensor::Tensor::from_vec(vec![1.0, 2.0, 3.0], vec![3]);
        assert_eq!(t.size(), vec![3]);
    }
}
