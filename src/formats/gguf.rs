//! GGUF format support for loading quantized models
//! 量子化モデル読み込み用GGUF形式サポート
//!
//! Reference: <https://github.com/ggerganov/ggml/blob/master/docs/gguf.md>

use crate::error::{RusTorchError, RusTorchResult};
use crate::tensor::Tensor;
use std::collections::HashMap;
use std::fs::File;
use std::io::{BufReader, Read, Seek, SeekFrom};
use std::path::{Path, PathBuf};

/// Trait for floating point types used in GGUF dequantization
/// GGUF量子化解除で使用する浮動小数点型のトレイト
pub trait GGUFFloat: Copy + From<f32> + std::fmt::Debug {
    fn from_i8(val: i8) -> Self;
    fn from_f32(val: f32) -> Self;
}

impl GGUFFloat for f32 {
    fn from_i8(val: i8) -> Self {
        val as f32
    }

    fn from_f32(val: f32) -> Self {
        val
    }
}

impl GGUFFloat for f64 {
    fn from_i8(val: i8) -> Self {
        val as f64
    }

    fn from_f32(val: f32) -> Self {
        val as f64
    }
}

const GGUF_MAGIC: u32 = 0x46554747; // "GGUF" in little-endian
const GGUF_VERSION_V3: u32 = 3;
const GGUF_VERSION_V2: u32 = 2;

/// GGUF file header
#[derive(Debug, Clone)]
pub struct GGUFHeader {
    pub magic: u32,
    pub version: u32,
    pub tensor_count: u64,
    pub metadata_kv_count: u64,
}

/// GGUF metadata key-value pair
#[derive(Debug, Clone)]
pub struct GGUFMetadata {
    pub key: String,
    pub value: GGUFValue,
}

/// GGUF value types
#[derive(Debug, Clone)]
pub enum GGUFValue {
    UInt8(u8),
    Int8(i8),
    UInt16(u16),
    Int16(i16),
    UInt32(u32),
    Int32(i32),
    Float32(f32),
    Bool(bool),
    String(String),
    Array(Vec<GGUFValue>),
    UInt64(u64),
    Int64(i64),
    Float64(f64),
}

impl GGUFValue {
    pub fn as_u32(&self) -> Option<u32> {
        match self {
            GGUFValue::UInt32(v) => Some(*v),
            GGUFValue::Int32(v) if *v >= 0 => Some(*v as u32),
            _ => None,
        }
    }

    pub fn as_u64(&self) -> Option<u64> {
        match self {
            GGUFValue::UInt64(v) => Some(*v),
            GGUFValue::UInt32(v) => Some(*v as u64),
            _ => None,
        }
    }

    pub fn as_str(&self) -> Option<&str> {
        match self {
            GGUFValue::String(s) => Some(s),
            _ => None,
        }
    }
}

/// GGUF tensor information
#[derive(Debug, Clone)]
pub struct GGUFTensorInfo {
    pub name: String,
    pub dims: Vec<u64>,
    pub ggml_type: u32,
    pub offset: u64,
}

/// GGML quantization types
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u32)]
#[allow(non_camel_case_types)]
pub enum GGMLType {
    F32 = 0,
    F16 = 1,
    Q4_0 = 2,
    Q4_1 = 3,
    Q5_0 = 6,
    Q5_1 = 7,
    Q8_0 = 8,
    Q8_1 = 9,
    Q2_K = 10,
    Q3_K = 11,
    Q4_K = 12,
    Q5_K = 13,
    Q6_K = 14,
    Q8_K = 15,
}

impl GGMLType {
    pub fn from_u32(value: u32) -> RusTorchResult<Self> {
        match value {
            0 => Ok(GGMLType::F32),
            1 => Ok(GGMLType::F16),
            2 => Ok(GGMLType::Q4_0),
            3 => Ok(GGMLType::Q4_1),
            6 => Ok(GGMLType::Q5_0),
            7 => Ok(GGMLType::Q5_1),
            8 => Ok(GGMLType::Q8_0),
            9 => Ok(GGMLType::Q8_1),
            10 => Ok(GGMLType::Q2_K),
            11 => Ok(GGMLType::Q3_K),
            12 => Ok(GGMLType::Q4_K),
            13 => Ok(GGMLType::Q5_K),
            14 => Ok(GGMLType::Q6_K),
            15 => Ok(GGMLType::Q8_K),
            _ => Err(RusTorchError::ParseError(format!(
                "Unknown GGML type: {}",
                value
            ))),
        }
    }

    pub fn element_size(&self) -> usize {
        match self {
            GGMLType::F32 => 4,
            GGMLType::F16 => 2,
            GGMLType::Q4_0 => 18, // 16 nibbles + 2 bytes for scale
            GGMLType::Q4_1 => 20, // 16 nibbles + 2*2 bytes for scale and min
            GGMLType::Q5_0 => 22,
            GGMLType::Q5_1 => 24,
            GGMLType::Q8_0 => 34,
            GGMLType::Q8_1 => 36,
            GGMLType::Q2_K => 82,
            GGMLType::Q3_K => 110,
            GGMLType::Q4_K => 144,
            GGMLType::Q5_K => 176,
            GGMLType::Q6_K => 210,
            GGMLType::Q8_K => 292,
        }
    }
}

/// GGUF file loader
pub struct GGUFLoader {
    header: GGUFHeader,
    metadata: HashMap<String, GGUFValue>,
    tensors: Vec<GGUFTensorInfo>,
    data_offset: u64,
    file_path: PathBuf,
}

impl GGUFLoader {
    /// Load GGUF file from path
    pub fn from_file<P: AsRef<Path>>(path: P) -> RusTorchResult<Self> {
        let file = File::open(path.as_ref()).map_err(|e| {
            RusTorchError::IoError(format!(
                "Failed to open GGUF file {}: {}",
                path.as_ref().display(),
                e
            ))
        })?;
        let mut reader = BufReader::new(file);

        // Read header
        let header = Self::read_header(&mut reader)?;

        // Read metadata
        let metadata_vec = Self::read_metadata(&mut reader, header.metadata_kv_count)?;
        let metadata: HashMap<String, GGUFValue> =
            metadata_vec.into_iter().map(|m| (m.key, m.value)).collect();

        // Read tensor info
        let tensors = Self::read_tensor_info(&mut reader, header.tensor_count)?;

        // Calculate data offset
        let data_offset = reader
            .stream_position()
            .map_err(|e| RusTorchError::IoError(format!("Failed to get stream position: {}", e)))?;

        // Align to 32 bytes
        let aligned_offset = (data_offset + 31) & !31;
        reader
            .seek(SeekFrom::Start(aligned_offset))
            .map_err(|e| RusTorchError::IoError(format!("Failed to seek to data: {}", e)))?;

        let final_offset = reader
            .stream_position()
            .map_err(|e| RusTorchError::IoError(format!("Failed to get final position: {}", e)))?;

        // eprintln!("🔧 [GGUF] data_offset (after alignment) = {}", final_offset);

        Ok(Self {
            header,
            metadata,
            tensors,
            data_offset: final_offset,
            file_path: path.as_ref().to_path_buf(),
        })
    }

    /// Get model parameters from metadata
    pub fn get_model_params(&self) -> RusTorchResult<ModelParams> {
        // Get vocab_size from tokenizer.ggml.tokens array length
        let vocab_size = self
            .metadata
            .get("tokenizer.ggml.tokens")
            .and_then(|v| match v {
                GGUFValue::Array(arr) => Some(arr.len() as u32),
                _ => None,
            })
            .or_else(|| {
                self.metadata
                    .get("llama.vocab_size")
                    .and_then(|v| v.as_u32())
            })
            .ok_or_else(|| {
                RusTorchError::ParseError("Missing vocab_size or tokenizer.ggml.tokens".to_string())
            })?;

        let hidden_size = self
            .metadata
            .get("llama.embedding_length")
            .or_else(|| self.metadata.get("llama.embed_length"))
            .and_then(|v| v.as_u32())
            .ok_or_else(|| RusTorchError::ParseError("Missing embedding_length".to_string()))?;

        let num_layers = self
            .metadata
            .get("llama.block_count")
            .and_then(|v| v.as_u32())
            .ok_or_else(|| RusTorchError::ParseError("Missing block_count".to_string()))?;

        let num_heads = self
            .metadata
            .get("llama.attention.head_count")
            .and_then(|v| v.as_u32())
            .ok_or_else(|| RusTorchError::ParseError("Missing head_count".to_string()))?;

        // Read num_kv_heads from GGUF metadata
        // For GQA (Grouped Query Attention), this is typically less than num_heads
        // TinyLlama uses 4 KV heads with 32 query heads
        let num_kv_heads = self
            .metadata
            .get("llama.attention.head_count_kv")
            .and_then(|v| v.as_u32())
            .unwrap_or(num_heads); // Default to num_heads if not specified (MHA case)

        let context_length = self
            .metadata
            .get("llama.context_length")
            .and_then(|v| v.as_u32())
            .unwrap_or(2048);

        // Read FFN intermediate size (feed_forward_length)
        // Mistral: 14336, TinyLlama: 5632
        let intermediate_size = self
            .metadata
            .get("llama.feed_forward_length")
            .and_then(|v| v.as_u32());

        // Read RoPE frequency base
        // Mistral uses 1000000.0, standard LLaMA uses 10000.0
        let rope_freq_base = self
            .metadata
            .get("llama.rope.freq_base")
            .and_then(|v| match v {
                GGUFValue::Float32(f) => Some(*f),
                _ => None,
            });

        Ok(ModelParams {
            vocab_size,
            hidden_size,
            num_layers,
            num_heads,
            num_kv_heads,
            context_length,
            intermediate_size,
            rope_freq_base,
        })
    }

    /// Get tensor by name
    pub fn get_tensor(&self, name: &str) -> Option<&GGUFTensorInfo> {
        self.tensors.iter().find(|t| t.name == name)
    }

    /// List all tensor names
    pub fn tensor_names(&self) -> Vec<&str> {
        self.tensors.iter().map(|t| t.name.as_str()).collect()
    }

    /// Get tensor info by name
    /// テンソル情報を名前で取得
    pub fn get_tensor_info(&self, name: &str) -> Option<&GGUFTensorInfo> {
        self.tensors.iter().find(|t| t.name == name)
    }

    /// Get string value from metadata by key
    /// メタデータからキーで文字列値を取得
    pub fn get_string(&self, key: &str) -> Option<String> {
        self.metadata.get(key).and_then(|v| v.as_str()).map(|s| s.to_string())
    }

    /// Get architecture string from GGUF metadata
    /// GGUFメタデータからアーキテクチャ文字列を取得
    pub fn get_architecture(&self) -> Option<String> {
        self.get_string("general.architecture")
            .or_else(|| self.get_string("llama.architecture"))
    }

    /// Get all metadata keys
    /// すべてのメタデータキーを取得
    pub fn get_all_keys(&self) -> Vec<String> {
        self.metadata.keys().cloned().collect()
    }

    /// Get metadata keys matching a prefix
    /// プレフィックスに一致するメタデータキーを取得
    pub fn get_keys_with_prefix(&self, prefix: &str) -> Vec<String> {
        self.metadata
            .keys()
            .filter(|k| k.starts_with(prefix))
            .cloned()
            .collect()
    }

    /// Get integer value from metadata by key
    /// メタデータからキーで整数値を取得
    pub fn get_u32(&self, key: &str) -> Option<u32> {
        self.metadata.get(key).and_then(|v| v.as_u32())
    }

    /// Get integer value from metadata by key
    /// メタデータからキーで整数値を取得
    pub fn get_u64(&self, key: &str) -> Option<u64> {
        self.metadata.get(key).and_then(|v| v.as_u64())
    }

    /// Dump all metadata to stderr for debugging
    /// デバッグ用にすべてのメタデータを標準エラー出力にダンプ
    pub fn dump_metadata(&self, filter_prefix: Option<&str>) {
        eprintln!("\n=== GGUF Metadata Dump ===");
        eprintln!("Total keys: {}", self.metadata.len());

        let mut keys: Vec<_> = self.metadata.keys().collect();
        keys.sort();

        for key in keys {
            if let Some(prefix) = filter_prefix {
                if !key.starts_with(prefix) {
                    continue;
                }
            }

            if let Some(value) = self.metadata.get(key) {
                match value {
                    GGUFValue::String(s) => {
                        eprintln!("  {} [string]: {}", key, s);
                    }
                    GGUFValue::UInt8(n) => eprintln!("  {} [u8]: {}", key, n),
                    GGUFValue::Int8(n) => eprintln!("  {} [i8]: {}", key, n),
                    GGUFValue::UInt16(n) => eprintln!("  {} [u16]: {}", key, n),
                    GGUFValue::Int16(n) => eprintln!("  {} [i16]: {}", key, n),
                    GGUFValue::UInt32(n) => eprintln!("  {} [u32]: {}", key, n),
                    GGUFValue::Int32(n) => eprintln!("  {} [i32]: {}", key, n),
                    GGUFValue::UInt64(n) => eprintln!("  {} [u64]: {}", key, n),
                    GGUFValue::Int64(n) => eprintln!("  {} [i64]: {}", key, n),
                    GGUFValue::Float32(n) => eprintln!("  {} [f32]: {}", key, n),
                    GGUFValue::Float64(n) => eprintln!("  {} [f64]: {}", key, n),
                    GGUFValue::Bool(b) => eprintln!("  {} [bool]: {}", key, b),
                    GGUFValue::Array(arr) => {
                        eprintln!("  {} [array]: {} elements", key, arr.len());
                    }
                }
            }
        }
        eprintln!("=== End Metadata Dump ===\n");
    }

    /// Get model information summary
    /// モデル情報サマリーを取得
    pub fn get_model_info(&self) -> String {
        let arch = self.get_architecture().unwrap_or_else(|| "unknown".to_string());
        let name = self.get_string("general.name").unwrap_or_else(|| "unknown".to_string());
        let vocab = self.get_u32("llama.vocab_size")
            .or_else(|| self.get_u32("gpt2.vocab_size"))
            .unwrap_or(0);
        let layers = self.get_u32("llama.block_count")
            .or_else(|| self.get_u32("gpt2.block_count"))
            .unwrap_or(0);

        format!(
            "Model: {} | Architecture: {} | Vocab: {} | Layers: {}",
            name, arch, vocab, layers
        )
    }

    /// Extract tokenizer vocabulary from GGUF metadata
    /// Returns a vector of tokens in vocabulary order
    pub fn extract_tokenizer_vocab(&self) -> RusTorchResult<Vec<String>> {
        let tokens = self
            .metadata
            .get("tokenizer.ggml.tokens")
            .ok_or_else(|| {
                RusTorchError::ParseError("Missing tokenizer.ggml.tokens in GGUF metadata".to_string())
            })?;

        match tokens {
            GGUFValue::Array(arr) => {
                let mut vocab = Vec::with_capacity(arr.len());
                for token_value in arr {
                    match token_value {
                        GGUFValue::String(s) => vocab.push(s.clone()),
                        _ => {
                            return Err(RusTorchError::ParseError(
                                "Invalid token type in tokenizer.ggml.tokens".to_string(),
                            ))
                        }
                    }
                }
                Ok(vocab)
            }
            _ => Err(RusTorchError::ParseError(
                "tokenizer.ggml.tokens is not an array".to_string(),
            )),
        }
    }

    /// Get tokenizer model type (e.g., "llama", "gpt2")
    pub fn get_tokenizer_model(&self) -> Option<String> {
        self.metadata
            .get("tokenizer.ggml.model")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
    }

    /// Extract BPE merge rules from GGUF metadata
    /// Returns a vector of merge pairs (token1, token2)
    pub fn extract_bpe_merges(&self) -> RusTorchResult<Vec<(String, String)>> {
        let merges = self
            .metadata
            .get("tokenizer.ggml.merges")
            .ok_or_else(|| {
                RusTorchError::ParseError("Missing tokenizer.ggml.merges in GGUF metadata".to_string())
            })?;

        match merges {
            GGUFValue::Array(arr) => {
                let mut merge_rules = Vec::with_capacity(arr.len());
                for merge_value in arr {
                    match merge_value {
                        GGUFValue::String(s) => {
                            // Merge format: "token1 token2"
                            let parts: Vec<&str> = s.splitn(2, ' ').collect();
                            if parts.len() == 2 {
                                merge_rules.push((parts[0].to_string(), parts[1].to_string()));
                            }
                        }
                        _ => {
                            return Err(RusTorchError::ParseError(
                                "Invalid merge type in tokenizer.ggml.merges".to_string(),
                            ))
                        }
                    }
                }
                Ok(merge_rules)
            }
            _ => Err(RusTorchError::ParseError(
                "tokenizer.ggml.merges is not an array".to_string(),
            )),
        }
    }

    /// Extract token scores from GGUF metadata
    pub fn extract_token_scores(&self) -> RusTorchResult<Vec<f32>> {
        let scores = self
            .metadata
            .get("tokenizer.ggml.scores")
            .ok_or_else(|| {
                RusTorchError::ParseError("Missing tokenizer.ggml.scores in GGUF metadata".to_string())
            })?;

        match scores {
            GGUFValue::Array(arr) => {
                let mut score_values = Vec::with_capacity(arr.len());
                for score_value in arr {
                    match score_value {
                        GGUFValue::Float32(f) => score_values.push(*f),
                        _ => {
                            return Err(RusTorchError::ParseError(
                                "Invalid score type in tokenizer.ggml.scores".to_string(),
                            ))
                        }
                    }
                }
                Ok(score_values)
            }
            _ => Err(RusTorchError::ParseError(
                "tokenizer.ggml.scores is not an array".to_string(),
            )),
        }
    }

    fn read_header(reader: &mut BufReader<File>) -> RusTorchResult<GGUFHeader> {
        let magic = Self::read_u32(reader)?;

        if magic != GGUF_MAGIC {
            return Err(RusTorchError::ParseError(format!(
                "Invalid GGUF magic number: 0x{:08x} (expected 0x{:08x})",
                magic, GGUF_MAGIC
            )));
        }

        let version = Self::read_u32(reader)?;

        if version != GGUF_VERSION_V3 && version != GGUF_VERSION_V2 {
            return Err(RusTorchError::ParseError(format!(
                "Unsupported GGUF version: {} (supported: v2, v3)",
                version
            )));
        }

        let tensor_count = Self::read_u64(reader)?;
        let metadata_kv_count = Self::read_u64(reader)?;

        Ok(GGUFHeader {
            magic,
            version,
            tensor_count,
            metadata_kv_count,
        })
    }

    fn read_metadata(
        reader: &mut BufReader<File>,
        count: u64,
    ) -> RusTorchResult<Vec<GGUFMetadata>> {
        let mut metadata = Vec::with_capacity(count as usize);

        for _ in 0..count {
            let key = Self::read_string(reader)?;
            let value_type = Self::read_u32(reader)?;
            let value = Self::read_value(reader, value_type)?;

            metadata.push(GGUFMetadata { key, value });
        }

        Ok(metadata)
    }

    fn read_tensor_info(
        reader: &mut BufReader<File>,
        count: u64,
    ) -> RusTorchResult<Vec<GGUFTensorInfo>> {
        let mut tensors = Vec::with_capacity(count as usize);

        for _ in 0..count {
            let name = Self::read_string(reader)?;
            let n_dims = Self::read_u32(reader)?;

            let mut dims = Vec::with_capacity(n_dims as usize);
            for _ in 0..n_dims {
                dims.push(Self::read_u64(reader)?);
            }

            let ggml_type = Self::read_u32(reader)?;
            let offset = Self::read_u64(reader)?;

            tensors.push(GGUFTensorInfo {
                name,
                dims,
                ggml_type,
                offset,
            });
        }

        Ok(tensors)
    }

    fn read_value(reader: &mut BufReader<File>, value_type: u32) -> RusTorchResult<GGUFValue> {
        match value_type {
            0 => Ok(GGUFValue::UInt8(Self::read_u8(reader)?)),
            1 => Ok(GGUFValue::Int8(Self::read_i8(reader)?)),
            2 => Ok(GGUFValue::UInt16(Self::read_u16(reader)?)),
            3 => Ok(GGUFValue::Int16(Self::read_i16(reader)?)),
            4 => Ok(GGUFValue::UInt32(Self::read_u32(reader)?)),
            5 => Ok(GGUFValue::Int32(Self::read_i32(reader)?)),
            6 => Ok(GGUFValue::Float32(Self::read_f32(reader)?)),
            7 => Ok(GGUFValue::Bool(Self::read_u8(reader)? != 0)),
            8 => Ok(GGUFValue::String(Self::read_string(reader)?)),
            9 => {
                let array_type = Self::read_u32(reader)?;
                let array_len = Self::read_u64(reader)?;
                let mut array = Vec::with_capacity(array_len as usize);
                for _ in 0..array_len {
                    array.push(Self::read_value(reader, array_type)?);
                }
                Ok(GGUFValue::Array(array))
            }
            10 => Ok(GGUFValue::UInt64(Self::read_u64(reader)?)),
            11 => Ok(GGUFValue::Int64(Self::read_i64(reader)?)),
            12 => Ok(GGUFValue::Float64(Self::read_f64(reader)?)),
            _ => Err(RusTorchError::ParseError(format!(
                "Unknown value type: {}",
                value_type
            ))),
        }
    }

    fn read_string(reader: &mut BufReader<File>) -> RusTorchResult<String> {
        let len = Self::read_u64(reader)?;
        if len == 0 {
            return Ok(String::new());
        }
        if len > 10_000_000 {
            return Err(RusTorchError::ParseError(format!(
                "String length too large: {}",
                len
            )));
        }
        let mut buf = vec![0u8; len as usize];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read string: {}", e)))?;
        String::from_utf8(buf)
            .map_err(|e| RusTorchError::ParseError(format!("Invalid UTF-8 in string: {}", e)))
    }

    // Helper methods for reading primitive types
    fn read_u8(reader: &mut BufReader<File>) -> RusTorchResult<u8> {
        let mut buf = [0u8; 1];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read u8: {}", e)))?;
        Ok(buf[0])
    }

    fn read_i8(reader: &mut BufReader<File>) -> RusTorchResult<i8> {
        Ok(Self::read_u8(reader)? as i8)
    }

    fn read_u16(reader: &mut BufReader<File>) -> RusTorchResult<u16> {
        let mut buf = [0u8; 2];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read u16: {}", e)))?;
        Ok(u16::from_le_bytes(buf))
    }

    fn read_i16(reader: &mut BufReader<File>) -> RusTorchResult<i16> {
        let mut buf = [0u8; 2];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read i16: {}", e)))?;
        Ok(i16::from_le_bytes(buf))
    }

    fn read_u32(reader: &mut BufReader<File>) -> RusTorchResult<u32> {
        let mut buf = [0u8; 4];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read u32: {}", e)))?;
        Ok(u32::from_le_bytes(buf))
    }

    fn read_i32(reader: &mut BufReader<File>) -> RusTorchResult<i32> {
        let mut buf = [0u8; 4];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read i32: {}", e)))?;
        Ok(i32::from_le_bytes(buf))
    }

    fn read_u64(reader: &mut BufReader<File>) -> RusTorchResult<u64> {
        let mut buf = [0u8; 8];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read u64: {}", e)))?;
        Ok(u64::from_le_bytes(buf))
    }

    fn read_i64(reader: &mut BufReader<File>) -> RusTorchResult<i64> {
        let mut buf = [0u8; 8];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read i64: {}", e)))?;
        Ok(i64::from_le_bytes(buf))
    }

    fn read_f32(reader: &mut BufReader<File>) -> RusTorchResult<f32> {
        let mut buf = [0u8; 4];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read f32: {}", e)))?;
        Ok(f32::from_le_bytes(buf))
    }

    fn read_f64(reader: &mut BufReader<File>) -> RusTorchResult<f64> {
        let mut buf = [0u8; 8];
        reader
            .read_exact(&mut buf)
            .map_err(|e| RusTorchError::IoError(format!("Failed to read f64: {}", e)))?;
        Ok(f64::from_le_bytes(buf))
    }

    /// Load tensor data by name
    /// Load tensor with generic float type
    /// ジェネリック型でテンソルを読み込む
    pub fn load_tensor_generic<F: GGUFFloat>(&self, name: &str) -> RusTorchResult<Vec<F>>
    where
        F: 'static,
    {
        let tensor_info = self
            .tensors
            .iter()
            .find(|t| t.name == name)
            .ok_or_else(|| RusTorchError::ParseError(format!("Tensor not found: {}", name)))?
            .clone();

        // Reopen file for reading tensor data
        let file = File::open(&self.file_path).map_err(|e| {
            RusTorchError::IoError(format!("Failed to reopen GGUF file: {}", e))
        })?;
        let mut reader = BufReader::new(file);

        // Seek to tensor data offset
        let absolute_offset = self.data_offset + tensor_info.offset;
        // Show offset for token_embd.weight to debug
        if name == "token_embd.weight" {
            eprintln!("🔧 [GGUF SEEK] '{}': data_offset={}, tensor_offset={}, absolute={}",
                     name, self.data_offset, tensor_info.offset, absolute_offset);
        }
        reader
            .seek(SeekFrom::Start(absolute_offset))
            .map_err(|e| RusTorchError::IoError(format!("Failed to seek to tensor data: {}", e)))?;

        // Calculate number of elements
        let num_elements: usize = tensor_info.dims.iter().map(|&d| d as usize).product();

        // Convert GGML type to GGMLType enum
        let ggml_type = GGMLType::from_u32(tensor_info.ggml_type)?;
        // eprintln!("🔧 [GGUF] Loading '{}': type={:?}, ggml_type_code={}",
        //           name, ggml_type, tensor_info.ggml_type);

        // BUGFIX 2025-10-16: Keep GGML dimensions as-is, don't reverse or transpose
        // GGML format stores dimensions in [ne[0], ne[1], ...] order
        // The model layer will handle transpose when needed (like Candle/rustorch-cli)
        let shape: Vec<usize> = tensor_info.dims.iter().map(|&d| d as usize).collect();

        // Read tensor data based on type
        // ジェネリック型を使用してf32→f64→f32の変換を回避
        let data = match ggml_type {
            GGMLType::F32 => {
                let mut data = Vec::with_capacity(num_elements);
                for _ in 0..num_elements {
                    let f32_val = Self::read_f32(&mut reader)?;
                    data.push(F::from_f32(f32_val));
                }
                data
            }
            GGMLType::F16 => {
                let mut data = Vec::with_capacity(num_elements);
                for _ in 0..num_elements {
                    let bits = Self::read_u16(&mut reader)?;
                    let f32_val = half::f16::from_bits(bits).to_f32();
                    data.push(F::from_f32(f32_val));
                }
                data
            }
            GGMLType::Q4_0 => {
                // Q4_0: Blocks of 32 elements
                // Block size: 18 bytes (2 bytes scale + 16 bytes quantized)
                Self::dequantize_q4_0::<F>(&mut reader, num_elements)?
            }
            GGMLType::Q4_K => {
                // Q4_K: Super-blocks of 256 elements (8 blocks of 32)
                // Block size: 144 bytes per super-block
                Self::dequantize_q4_k::<F>(&mut reader, num_elements)?
            }
            GGMLType::Q5_K => {
                // Q5_K: Super-blocks of 256 elements
                // Block size: 176 bytes per super-block
                Self::dequantize_q5_k::<F>(&mut reader, num_elements)?
            }
            GGMLType::Q6_K => {
                // Q6_K: Super-blocks of 256 elements (16 blocks of 16)
                // Block size: 210 bytes per super-block
                Self::dequantize_q6_k::<_, F>(&mut reader, num_elements)?
            }
            GGMLType::Q8_0 => {
                // Q8_0: Blocks of 32 elements
                // Block size: 34 bytes (2 bytes scale + 32 bytes quantized)
                Self::dequantize_q8_0::<F>(&mut reader, num_elements, name)?
            }
            _ => {
                return Err(RusTorchError::ParseError(format!(
                    "Tensor type {:?} not yet supported for loading",
                    ggml_type
                )))
            }
        };

        Ok(data)
    }

    /// Load tensor data by name (backward compatibility, returns f64)
    /// テンソルデータを名前で読み込む（後方互換性のためf64を返す）
    pub fn load_tensor(&self, name: &str) -> RusTorchResult<Tensor<f64>> {
        // BUGFIX 2025-10-16: Keep GGML dimensions as-is, matching load_tensor_generic
        let shape: Vec<usize> = self.tensors
            .iter()
            .find(|t| t.name == name)
            .ok_or_else(|| RusTorchError::ParseError(format!("Tensor not found: {}", name)))?
            .dims
            .iter()
            .map(|&d| d as usize)
            .collect();

        let data: Vec<f64> = self.load_tensor_generic(name)?;
        Ok(Tensor::from_vec(data, shape))
    }

    /// Dequantize Q4_0 format
    /// Q4_0: Blocks of 32 elements
    /// Block structure: scale (f16) + quants (16 bytes of 4-bit values)
    fn dequantize_q4_0<F: GGUFFloat>(
        reader: &mut BufReader<File>,
        num_elements: usize,
    ) -> RusTorchResult<Vec<F>> {
        const QK: usize = 32; // Elements per block
        const BLOCK_SIZE: usize = 18; // 2 bytes scale + 16 bytes quantized

        let num_blocks = (num_elements + QK - 1) / QK;
        let mut output = Vec::with_capacity(num_elements);

        for block_idx in 0..num_blocks {
            // Read scale (f16)
            let scale_bits = Self::read_u16(reader)?;
            let scale = half::f16::from_bits(scale_bits).to_f32();

            // Read 16 bytes of quantized 4-bit values (32 values total)
            let mut qs = [0u8; QK / 2];
            reader.read_exact(&mut qs).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q4_0 quants: {}", e))
            })?;

            // Debug: Print first block's scale and raw bytes
            if block_idx == 0 {
        // eprintln!("🐛 [Q4_0 DEBUG] First block:");
        // eprintln!("   scale_bits: 0x{:04x}", scale_bits);
        // eprintln!("   scale: {:.10}", scale);
        // eprintln!("   First 4 quant bytes: {:02x} {:02x} {:02x} {:02x}",
        //          qs[0], qs[1], qs[2], qs[3]);
            }

            // Dequantize: Each byte contains two 4-bit values
            // llama.cpp interleaves: x0[j] at position j, x1[j] at position j+16
            // Prepare output space for this block
            let block_start = output.len();
            for _ in 0..QK {
                output.push(F::from_f32(0.0));
            }

            for j in 0..QK / 2 {
                // Lower 4 bits (0-15) -> -8 to +7
                let x0 = ((qs[j] & 0x0F) as i8) - 8;
                // Upper 4 bits (0-15) -> -8 to +7
                let x1 = ((qs[j] >> 4) as i8) - 8;

                // llama.cpp interleaved layout: y[j] = x0, y[j+16] = x1
                // ジェネリック型を使用してf32→f64→f32の変換を回避
                output[block_start + j] = F::from_f32(x0 as f32 * scale);
                output[block_start + j + QK / 2] = F::from_f32(x1 as f32 * scale);
            }
        }

        // Trim to exact size
        output.truncate(num_elements);
        Ok(output)
    }

    /// Dequantize Q8_0 format
    /// Q8_0: Blocks of 32 elements, 8-bit quantization
    /// Block structure: scale (f16) + quants (32 bytes of 8-bit signed values)
    fn dequantize_q8_0<F: GGUFFloat>(
        reader: &mut BufReader<File>,
        num_elements: usize,
        tensor_name: &str,
    ) -> RusTorchResult<Vec<F>> {
        const QK: usize = 32; // Elements per block

        let num_blocks = (num_elements + QK - 1) / QK;
        let mut output = Vec::with_capacity(num_elements);

        for block_idx in 0..num_blocks {
            // Read scale (f16, 2 bytes)
            let scale_bits = Self::read_u16(reader)?;
            let scale = half::f16::from_bits(scale_bits).to_f32();

            // Read 32 quantized 8-bit signed values
            let mut quants = [0i8; QK];
            for q in &mut quants {
                let mut buf = [0u8; 1];
                reader.read_exact(&mut buf).map_err(|e| {
                    RusTorchError::IoError(format!("Failed to read Q8_0 quant: {}", e))
                })?;
                *q = buf[0] as i8;
            }

            // DEBUG: Print first block details and blocks for Token 1 (offsets 2048-2079)
            // Token 1's embedding spans elements 2048-4095
            // That's blocks 64-127 (each block = 32 elements)
            let element_start = block_idx * QK;
            let element_end = element_start + QK;

            // Show block 0 (Token 0 start), Token 1 first block, and Token 529's first 2 blocks
            // Token 1 starts at element 1 * 2048 = 2048 (block 64)
            // Token 529 starts at element 529 * 2048 = 1,083,392 (block 33856)
            // ONLY show for token_embd.weight tensor
            if tensor_name == "token_embd.weight" && (block_idx == 0 || block_idx == 64 || (block_idx >= 33856 && block_idx < 33858)) {
        eprintln!("\n🔍 [GGUF Q8_0 DEQUANT '{}'] Block {} (elements {}-{}):",
                 tensor_name, block_idx, element_start, element_end - 1);
        eprintln!("  scale_bits = 0x{:04x}", scale_bits);
        eprintln!("  scale = {:.9}", scale);
        eprintln!("  first 20 quants: {:?}", &quants[..20]);
        eprintln!("  first 10 dequantized:");
                for i in 0..10 {
        eprintln!("    [{}] = {:.9}", i, scale * quants[i] as f32);
                }
            }

            // Dequantize: value = scale * quant
            // ジェネリック型を使用してf32→f64→f32の変換を回避
            for &q in &quants {
                if output.len() >= num_elements {
                    break;
                }
                output.push(F::from_f32(scale * q as f32));
            }
        }

        output.truncate(num_elements);
        Ok(output)
    }

    /// Dequantize Q4_K format
    /// Q4_K: Super-blocks of 256 elements (8 blocks of 32 elements each)
    /// Block structure: scales (6-bit quantized) + mins (6-bit quantized) + quants (4-bit)
    fn dequantize_q4_k<F: GGUFFloat>(
        reader: &mut BufReader<File>,
        num_elements: usize,
    ) -> RusTorchResult<Vec<F>> {
        const QK_K: usize = 256;          // Elements per super-block
        const K_SCALE_SIZE: usize = 12;   // Scale data size

        let num_blocks = (num_elements + QK_K - 1) / QK_K;
        let mut output = Vec::with_capacity(num_elements);

        for _ in 0..num_blocks {
            // Read super-block data (144 bytes total)
            // Structure from llama.cpp:
            // - d (f16): super-scale
            // - dmin (f16): super-min
            // - scales[12]: quantized scales
            // - qs[QK_K/2]: 4-bit quantized values (128 bytes)

            // Read super-scale and super-min (f16 each)
            let d_bits = Self::read_u16(reader)?;
            let dmin_bits = Self::read_u16(reader)?;
            let d_f16 = half::f16::from_bits(d_bits);
            let dmin_f16 = half::f16::from_bits(dmin_bits);
            let d = d_f16.to_f32();
            let dmin = dmin_f16.to_f32();

            // Debug: verify f16 conversion
            if output.is_empty() {
        // eprintln!("🔍 [F16 DEBUG] d_bits=0x{:04x} -> d_f16={:?} -> d_f32={:.15e}", d_bits, d_f16, d);
            }

            // Read quantized scales (12 bytes)
            let mut scales = [0u8; K_SCALE_SIZE];
            reader.read_exact(&mut scales).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q4_K scales: {}", e))
            })?;

            // Read quantized values (128 bytes = 256 nibbles)
            let mut qs = vec![0u8; QK_K / 2];
            reader.read_exact(&mut qs).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q4_K quants: {}", e))
            })?;

            // Debug first block's raw values
            if output.is_empty() {
        // eprintln!("🔍 [Q4_K RAW] d_bits=0x{:04x}, dmin_bits=0x{:04x}", d_bits, dmin_bits);
        // eprintln!("🔍 [Q4_K RAW] d={:.10}, dmin={:.10}", d, dmin);
        // eprintln!("🔍 [Q4_K RAW] scales: {:02x?}", &scales[0..12]);
        // eprintln!("🔍 [Q4_K RAW] qs[0..16]: {:02x?}", &qs[0..16]);
            }

            // Dequantize: Process in pairs (64 elements at a time) like llama.cpp
            // Each pair processes 32 bytes: lower nibbles first, then upper nibbles
        // eprintln!("🔍 [Q4_K] Starting block processing, output.len()={}", output.len());

            for pair in 0..4 {
                let j1 = pair * 2;
                let j2 = pair * 2 + 1;
                let q_offset = pair * 32; // Byte offset in qs array

                // Get scale/min for both blocks in the pair
                let (scale1, min1) = if j1 < 4 {
                    let sc = scales[j1] & 63;
                    let mn = scales[j1 + 4] & 63;
                    (sc as f32, mn as f32)
                } else {
                    let sc = (scales[j1 + 4] & 0x0F) | ((scales[j1 - 4] >> 6) << 4);
                    let mn = (scales[j1 + 4] >> 4) | ((scales[j1] >> 6) << 4);
                    (sc as f32, mn as f32)
                };

                let (scale2, min2) = if j2 < 4 {
                    let sc = scales[j2] & 63;
                    let mn = scales[j2 + 4] & 63;
                    (sc as f32, mn as f32)
                } else {
                    let sc = (scales[j2 + 4] & 0x0F) | ((scales[j2 - 4] >> 6) << 4);
                    let mn = (scales[j2 + 4] >> 4) | ((scales[j2] >> 6) << 4);
                    (sc as f32, mn as f32)
                };

                let d1 = d * scale1;
                let m1 = dmin * min1;
                let d2 = d * scale2;
                let m2 = dmin * min2;

                // Debug: Print first block's values
                if output.is_empty() && pair == 0 {
        // eprintln!("🐛 [Q4_K DEBUG] First block:");
        // eprintln!("   d={:.10}, dmin={:.10}", d, dmin);
        // eprintln!("   scale1={}, min1={}", scale1, min1);
        // eprintln!("   d1={:.10}, m1={:.10}", d1, m1);
        // eprintln!("   First q_val: {}", qs[q_offset] & 0x0F);
        // eprintln!("   First dequant: {:.10}", d1 * (qs[q_offset] & 0x0F) as f32 - m1);
                }

                // Process 32 lower nibbles (block 1)
                // ジェネリック型を使用してf32→f64→f32の変換を回避
                for l in 0..32 {
                    if output.len() >= num_elements { break; }
                    let q_val = qs[q_offset + l] & 0x0F;
                    let val = d1 * q_val as f32 - m1;
                    output.push(F::from_f32(val));
                    // Debug: Print first 10 embedding values
                    if output.len() <= 10 {
        // eprintln!("🔍 [Q4_K OUTPUT] index={}: value={:.10}", output.len() - 1, val);
                    }
                }

                // Process 32 upper nibbles (block 2)
                for l in 0..32 {
                    if output.len() >= num_elements { break; }
                    let q_val = qs[q_offset + l] >> 4;
                    output.push(F::from_f32(d2 * q_val as f32 - m2));
                }
            }
        }

        output.truncate(num_elements);
        Ok(output)
    }

    /// Dequantize Q5_K format matching llama.cpp
    /// Q5_K: Super-blocks of 256 elements
    /// Block structure: d(f16), dmin(f16), scales[12], qh[32], qs[128] = 176 bytes
    fn dequantize_q5_k<F: GGUFFloat>(
        reader: &mut BufReader<File>,
        num_elements: usize,
    ) -> RusTorchResult<Vec<F>> {
        const QK_K: usize = 256;
        const K_SCALE_SIZE: usize = 12;

        let num_blocks = (num_elements + QK_K - 1) / QK_K;
        let mut output = Vec::with_capacity(num_elements);

        for _ in 0..num_blocks {
            // Read super-scale and super-min (f16 each)
            let d_bits = Self::read_u16(reader)?;
            let dmin_bits = Self::read_u16(reader)?;
            let d = half::f16::from_bits(d_bits).to_f32();
            let dmin = half::f16::from_bits(dmin_bits).to_f32();

            // Read quantized scales (12 bytes)
            let mut scales = [0u8; K_SCALE_SIZE];
            reader.read_exact(&mut scales).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q5_K scales: {}", e))
            })?;

            // Read high bits (32 bytes)
            let mut qh = [0u8; QK_K / 8];
            reader.read_exact(&mut qh).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q5_K qh: {}", e))
            })?;

            // Read quantized values (128 bytes = 256 nibbles)
            let mut qs = vec![0u8; QK_K / 2];
            reader.read_exact(&mut qs).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q5_K quants: {}", e))
            })?;

            // Dequantize: Process in pairs (64 elements at a time)
            for pair in 0..4 {
                let j1 = pair * 2;
                let j2 = pair * 2 + 1;
                let q_offset = pair * 32;

                // Get scale/min for both blocks
                let (scale1, min1) = if j1 < 4 {
                    let sc = scales[j1] & 63;
                    let mn = scales[j1 + 4] & 63;
                    (sc as f32, mn as f32)
                } else {
                    let sc = (scales[j1 + 4] & 0x0F) | ((scales[j1 - 4] >> 6) << 4);
                    let mn = (scales[j1 + 4] >> 4) | ((scales[j1] >> 6) << 4);
                    (sc as f32, mn as f32)
                };

                let (scale2, min2) = if j2 < 4 {
                    let sc = scales[j2] & 63;
                    let mn = scales[j2 + 4] & 63;
                    (sc as f32, mn as f32)
                } else {
                    let sc = (scales[j2 + 4] & 0x0F) | ((scales[j2 - 4] >> 6) << 4);
                    let mn = (scales[j2 + 4] >> 4) | ((scales[j2] >> 6) << 4);
                    (sc as f32, mn as f32)
                };

                let d1 = d * scale1;
                let m1 = dmin * min1;
                let d2 = d * scale2;
                let m2 = dmin * min2;

                // Process 32 lower nibbles + high bit (block 1)
                // ジェネリック型を使用してf32→f64→f32の変換を回避
                for l in 0..32 {
                    if output.len() >= num_elements { break; }
                    let q_low = qs[q_offset + l] & 0x0F;
                    let qh_byte = qh[l];
                    let q_high = (qh_byte >> j1) & 1;
                    let q_val = q_low | (q_high << 4);
                    output.push(F::from_f32(d1 * q_val as f32 - m1));
                }

                // Process 32 upper nibbles + high bit (block 2)
                for l in 0..32 {
                    if output.len() >= num_elements { break; }
                    let q_low = qs[q_offset + l] >> 4;
                    let qh_byte = qh[l];
                    let q_high = (qh_byte >> j2) & 1;
                    let q_val = q_low | (q_high << 4);
                    output.push(F::from_f32(d2 * q_val as f32 - m2));
                }
            }
        }

        output.truncate(num_elements);
        Ok(output)
    }

    /// Dequantize Q6_K format matching llama.cpp exactly
    fn dequantize_q6_k<R: Read, F: GGUFFloat>(
        reader: &mut BufReader<R>,
        num_elements: usize,
    ) -> RusTorchResult<Vec<F>> {
        const QK_K: usize = 256;

        let num_blocks = (num_elements + QK_K - 1) / QK_K;
        let mut output = vec![F::from_f32(0.0); num_elements];

        for block_idx in 0..num_blocks {
            let block_start = block_idx * QK_K;

            // Read ql[128], qh[64], sc[16], d(f16) = 210 bytes
            let mut ql = vec![0u8; 128];
            reader.read_exact(&mut ql).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q6_K ql: {}", e))
            })?;

            let mut qh = vec![0u8; 64];
            reader.read_exact(&mut qh).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q6_K qh: {}", e))
            })?;

            let mut sc = vec![0i8; 16];
            for scale in &mut sc {
                let mut buf = [0u8; 1];
                reader.read_exact(&mut buf).map_err(|e| {
                    RusTorchError::IoError(format!("Failed to read Q6_K scale: {}", e))
                })?;
                *scale = buf[0] as i8;
            }

            let mut d_buf = [0u8; 2];
            reader.read_exact(&mut d_buf).map_err(|e| {
                RusTorchError::IoError(format!("Failed to read Q6_K d: {}", e))
            })?;
            let d_bits = u16::from_le_bytes(d_buf);
            let d = half::f16::from_bits(d_bits).to_f32();

            // Dequantize: 2 chunks of 128 elements
            let mut y_idx = block_start;
            let mut ql_idx = 0;
            let mut qh_idx = 0;
            let mut sc_idx = 0;

            for _chunk in 0..2 {
                for l in 0..32 {
                    let is = l / 16;

                    let q1 = (((ql[ql_idx + l] & 0xF) | (((qh[qh_idx + l] >> 0) & 3) << 4)) as i8) - 32;
                    let q2 = (((ql[ql_idx + l + 32] & 0xF) | (((qh[qh_idx + l] >> 2) & 3) << 4)) as i8) - 32;
                    let q3 = (((ql[ql_idx + l] >> 4) | (((qh[qh_idx + l] >> 4) & 3) << 4)) as i8) - 32;
                    let q4 = (((ql[ql_idx + l + 32] >> 4) | (((qh[qh_idx + l] >> 6) & 3) << 4)) as i8) - 32;

                    // ジェネリック型を使用してf32→f64→f32の変換を回避
                    if y_idx + l < num_elements {
                        output[y_idx + l] = F::from_f32(d * sc[sc_idx + is] as f32 * q1 as f32);
                    }
                    if y_idx + l + 32 < num_elements {
                        output[y_idx + l + 32] = F::from_f32(d * sc[sc_idx + is + 2] as f32 * q2 as f32);
                    }
                    if y_idx + l + 64 < num_elements {
                        output[y_idx + l + 64] = F::from_f32(d * sc[sc_idx + is + 4] as f32 * q3 as f32);
                    }
                    if y_idx + l + 96 < num_elements {
                        output[y_idx + l + 96] = F::from_f32(d * sc[sc_idx + is + 6] as f32 * q4 as f32);
                    }
                }

                y_idx += 128;
                ql_idx += 64;
                qh_idx += 32;
                sc_idx += 8;
            }
        }

        Ok(output)
    }

}

/// Model parameters extracted from GGUF metadata
#[derive(Debug, Clone)]
pub struct ModelParams {
    pub vocab_size: u32,
    pub hidden_size: u32,
    pub num_layers: u32,
    pub num_heads: u32,
    pub num_kv_heads: u32,  // Number of key-value heads for GQA
    pub context_length: u32,
    pub intermediate_size: Option<u32>,  // FFN intermediate size (feed_forward_length)
    pub rope_freq_base: Option<f32>,     // RoPE frequency base
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ggml_type_element_size() {
        assert_eq!(GGMLType::F32.element_size(), 4);
        assert_eq!(GGMLType::F16.element_size(), 2);
        assert_eq!(GGMLType::Q4_K.element_size(), 144);
    }

    #[test]
    fn test_ggml_type_from_u32() {
        assert_eq!(GGMLType::from_u32(0).unwrap(), GGMLType::F32);
        assert_eq!(GGMLType::from_u32(12).unwrap(), GGMLType::Q4_K);
        assert!(GGMLType::from_u32(999).is_err());
    }

    #[test]
    fn test_q6k_dequantization_interleaved_pattern() {
        // Test that Q6_K dequantization uses correct interleaved indexing pattern
        // llama.cpp pattern: y[l], y[l+32], y[l+64], y[l+96] for each iteration

        use std::io::{Cursor, Write};
        use std::io::BufReader;

        // Create a minimal Q6_K block (210 bytes)
        // Structure: ql[128] + qh[64] + sc[16] + d(f16) = 210 bytes
        let mut block_data = Vec::new();

        // ql[128]: lower 4 bits - use simple pattern
        for i in 0..128 {
            block_data.push((i % 16) as u8);
        }

        // qh[64]: upper 2 bits - use simple pattern
        for i in 0..64 {
            block_data.push(((i % 4) << 0) | (((i + 1) % 4) << 2) | (((i + 2) % 4) << 4) | (((i + 3) % 4) << 6));
        }

        // sc[16]: scales - use constant value for simplicity
        for _ in 0..16 {
            block_data.push(10i8 as u8);
        }

        // d (f16): super-scale - use 0.01
        let d_f16 = half::f16::from_f32(0.01);
        block_data.write(&d_f16.to_bits().to_le_bytes()).unwrap();

        assert_eq!(block_data.len(), 210, "Q6_K block should be exactly 210 bytes");

        // Dequantize using our implementation
        let mut reader = BufReader::new(Cursor::new(block_data));
        let result: Vec<f32> = GGUFLoader::dequantize_q6_k(&mut reader, 256).unwrap();

        assert_eq!(result.len(), 256, "Should produce 256 values");

        // Verify interleaved pattern by checking specific positions
        // First chunk (0-127):
        // l=0: positions 0, 32, 64, 96 should come from first iteration
        // l=1: positions 1, 33, 65, 97 should come from second iteration

        // All values should be non-zero (since we used non-zero ql, qh, sc)
        let non_zero_count = result.iter().filter(|&&v| v.abs() > 1e-10).count();
        assert!(non_zero_count > 200, "Most values should be non-zero with our test data");

        // Values should be in reasonable range (d * sc * q where q is -32 to 31)
        // max = 0.01 * 10 * 31 = 3.1
        for (i, &val) in result.iter().enumerate() {
            assert!(val.abs() < 5.0, "Value at index {} = {} exceeds expected range", i, val);
        }
    }

    #[test]
    fn test_q6k_dequantization_known_values() {
        // Test Q6_K dequantization with known values
        // This verifies the exact computation: d * scale * (quantized_value - 32)

        use std::io::{Cursor, Write};
        use std::io::BufReader;

        let mut block_data = Vec::new();

        // ql[128]: Set first byte to specific value
        block_data.push(0x0F); // lower nibble = 15
        for _ in 1..128 {
            block_data.push(0x00);
        }

        // qh[64]: Set first byte to extract upper bits
        block_data.push(0b00000000); // upper 2 bits = 0 for all 4 values
        for _ in 1..64 {
            block_data.push(0x00);
        }

        // sc[16]: Set specific scales
        block_data.push(16i8 as u8);  // sc[0]
        block_data.push(0i8 as u8);   // sc[1]
        block_data.push(10i8 as u8);  // sc[2]
        for _ in 3..16 {
            block_data.push(0i8 as u8);
        }

        // d (f16): 0.001
        let d_f16 = half::f16::from_f32(0.001);
        block_data.write(&d_f16.to_bits().to_le_bytes()).unwrap();

        let mut reader = BufReader::new(Cursor::new(block_data));
        let result: Vec<f32> = GGUFLoader::dequantize_q6_k(&mut reader, 256).unwrap();

        // First value at position 0:
        // ql[0] & 0xF = 15, qh[0] >> 0 & 3 = 0
        // q1 = (15 | (0 << 4)) - 32 = 15 - 32 = -17
        // is = 0 / 16 = 0
        // value = d * sc[0] * q1 = 0.001 * 16 * (-17) = -0.272
        let expected_0 = 0.001_f32 * 16.0 * (-17.0);
        assert!((result[0] - expected_0).abs() < 0.001,
            "Position 0: expected {}, got {}", expected_0, result[0]);

        // Value at position 32 uses sc[2]:
        // ql[32] = 0, qh[32 % 64] = 0
        // q2 = (0 | (0 << 4)) - 32 = -32
        // value = d * sc[2] * q2 = 0.001 * 10 * (-32) = -0.32
        let expected_32 = 0.001 * 10.0 * (-32.0);
        assert!((result[32] - expected_32).abs() < 0.001,
            "Position 32: expected {}, got {}", expected_32, result[32]);
    }

    #[test]
    fn test_q6k_element_size() {
        // Verify Q6_K element size is correct
        // Q6_K block = 210 bytes for 256 elements
        assert_eq!(GGMLType::Q6_K.element_size(), 210);
    }
}
