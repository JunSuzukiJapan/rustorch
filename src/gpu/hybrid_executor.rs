use crate::dtype::DType;
/// Hybrid execution engine for CoreML + GPU fallback system
/// CoreML + GPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
use crate::error::{RusTorchError, RusTorchResult};
use crate::gpu::{DeviceCapability, DeviceType, GpuDevice, OpType};
use crate::gpu::smart_device_selector::{SmartDeviceSelector, OperationProfile, OperationType};
use crate::gpu::device_cache::{DeviceCache, CoreMLCache};
use std::collections::{HashMap, HashSet};
use std::sync::Arc;

/// Tensor information for device selection
/// ãƒ‡ãƒã‚¤ã‚¹é¸æŠç”¨ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±
#[derive(Debug, Clone)]
pub struct TensorInfo {
    pub dtype: DType,
    pub shape: Vec<usize>,
    pub requires_custom_kernel: bool,
    pub memory_size_bytes: usize,
}

/// Transfer method between devices
/// ãƒ‡ãƒã‚¤ã‚¹é–“è»¢é€æ–¹æ³•
#[derive(Debug, Clone, Copy)]
pub enum TransferMethod {
    ZeroCopy,    // Metal â†” CoreML
    HostStaging, // CUDA â†’ CoreML via host memory
    Standard,    // General case
}

/// Hybrid execution engine managing device selection and fallback
/// ãƒ‡ãƒã‚¤ã‚¹é¸æŠã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç®¡ç†ã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
pub struct HybridExecutor {
    primary_device: DeviceType,
    fallback_devices: Vec<DeviceType>,
    capability_cache: HashMap<DeviceType, DeviceCapability>,
    operation_routing: HashMap<OpType, Vec<DeviceType>>,

    // Performance thresholds
    small_tensor_threshold: usize, // < 1MB â†’ CPU
    large_tensor_threshold: usize, // > 100MB â†’ best GPU

    // Smart device selection
    smart_selector: SmartDeviceSelector,
    device_cache: DeviceCache,
}

impl HybridExecutor {
    /// Create new hybrid executor
    /// æ–°ã—ã„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œå™¨ã‚’ä½œæˆ
    pub fn new() -> Self {
        // Initialize device cache and detect available devices
        let device_cache = DeviceCache::new();
        device_cache.warmup(); // Pre-populate cache

        let available_devices = Self::detect_available_devices(&device_cache);

        let mut executor = Self {
            primary_device: DeviceType::Auto,
            fallback_devices: Vec::new(),
            capability_cache: HashMap::new(),
            operation_routing: HashMap::new(),
            small_tensor_threshold: 1_000_000,   // 1MB
            large_tensor_threshold: 100_000_000, // 100MB
            smart_selector: SmartDeviceSelector::new(available_devices),
            device_cache,
        };

        executor.initialize_device_capabilities();
        executor.build_fallback_chain();
        executor.setup_operation_routing();

        executor
    }

    /// Get global singleton instance
    /// ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    pub fn global() -> &'static Self {
        use std::sync::OnceLock;
        static EXECUTOR: OnceLock<HybridExecutor> = OnceLock::new();
        EXECUTOR.get_or_init(|| Self::new())
    }

    /// Select optimal device for operation
    /// æ¼”ç®—ã«æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ
    pub fn select_device(&self, op_type: OpType, tensor_info: &TensorInfo) -> DeviceType {
        // 1. Size-based heuristics
        if tensor_info.memory_size_bytes < self.small_tensor_threshold {
            return DeviceType::Cpu;
        }

        // 2. CoreML compatibility check
        if self.is_coreml_supported(&op_type, tensor_info) {
            #[cfg(feature = "coreml")]
            if self.is_device_available(DeviceType::CoreML(0)) {
                return DeviceType::CoreML(0);
            }
        }

        // 3. GPU fallback selection
        if let Some(gpu_device) = self.select_gpu_device(&op_type, tensor_info) {
            return gpu_device;
        }

        // 4. CPU fallback
        DeviceType::Cpu
    }

    /// Check if CoreML supports the operation
    /// CoreMLãŒæ¼”ç®—ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    fn is_coreml_supported(&self, op_type: &OpType, tensor_info: &TensorInfo) -> bool {
        #[cfg(feature = "coreml")]
        {
            // Get CoreML capability
            if let Some(capability) = self.capability_cache.get(&DeviceType::CoreML(0)) {
                // Check operation support
                if !capability.supports_operation(op_type) {
                    return false;
                }

                // Check data type support
                match tensor_info.dtype {
                    DType::Float16 | DType::Float32 => {
                        // Check tensor dimension limits
                        tensor_info.shape.len() <= 5 && !tensor_info.requires_custom_kernel
                    }
                    _ => false,
                }
            } else {
                false
            }
        }
        #[cfg(not(feature = "coreml"))]
        {
            false
        }
    }

    /// Select best GPU device for operation
    /// æ¼”ç®—ã«æœ€é©ãªGPUãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ
    fn select_gpu_device(&self, op_type: &OpType, tensor_info: &TensorInfo) -> Option<DeviceType> {
        // Check operation routing table
        if let Some(devices) = self.operation_routing.get(op_type) {
            for &device in devices {
                if self.is_device_available(device)
                    && self.is_operation_efficient(device, tensor_info)
                {
                    return Some(device);
                }
            }
        }

        // Default GPU selection
        #[cfg(feature = "cuda")]
        if self.is_device_available(DeviceType::Cuda(0)) {
            return Some(DeviceType::Cuda(0));
        }

        #[cfg(feature = "metal")]
        if self.is_device_available(DeviceType::Metal(0)) {
            return Some(DeviceType::Metal(0));
        }

        #[cfg(feature = "opencl")]
        if self.is_device_available(DeviceType::OpenCL(0)) {
            return Some(DeviceType::OpenCL(0));
        }

        None
    }

    /// Check if device is available
    /// ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    fn is_device_available(&self, device: DeviceType) -> bool {
        match device {
            DeviceType::Cpu => true,
            #[cfg(feature = "cuda")]
            DeviceType::Cuda(_) => crate::backends::DeviceManager::is_cuda_available(),
            #[cfg(feature = "metal")]
            DeviceType::Metal(_) => crate::backends::DeviceManager::is_metal_available(),
            #[cfg(feature = "opencl")]
            DeviceType::OpenCL(_) => crate::backends::DeviceManager::is_opencl_available(),
            #[cfg(feature = "coreml")]
            DeviceType::CoreML(_) => crate::backends::DeviceManager::is_coreml_available(),
            _ => false,
        }
    }

    /// Check if operation is efficient on device
    /// ãƒ‡ãƒã‚¤ã‚¹ã§ã®æ¼”ç®—ãŒåŠ¹ç‡çš„ã‹ãƒã‚§ãƒƒã‚¯
    fn is_operation_efficient(&self, device: DeviceType, tensor_info: &TensorInfo) -> bool {
        // Large tensors benefit more from GPU acceleration
        if tensor_info.memory_size_bytes > self.large_tensor_threshold {
            return matches!(
                device,
                DeviceType::Cuda(_) | DeviceType::Metal(_) | DeviceType::CoreML(_)
            );
        }

        true
    }

    /// Get next fallback device
    /// æ¬¡ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—
    pub fn next_fallback_device(&self, failed_device: DeviceType) -> DeviceType {
        if let Some(pos) = self
            .fallback_devices
            .iter()
            .position(|&d| d == failed_device)
        {
            if pos + 1 < self.fallback_devices.len() {
                return self.fallback_devices[pos + 1];
            }
        }
        DeviceType::Cpu
    }

    /// Initialize device capabilities
    /// ãƒ‡ãƒã‚¤ã‚¹èƒ½åŠ›ã‚’åˆæœŸåŒ–
    fn initialize_device_capabilities(&mut self) {
        // CPU capability
        let mut cpu_ops = HashSet::new();
        cpu_ops.insert(OpType::LinearAlgebra);
        cpu_ops.insert(OpType::Convolution);
        cpu_ops.insert(OpType::Activation);
        cpu_ops.insert(OpType::Reduction);
        cpu_ops.insert(OpType::Normalization);
        cpu_ops.insert(OpType::ComplexMath);
        cpu_ops.insert(OpType::Distribution);
        cpu_ops.insert(OpType::DistributedOps);

        self.capability_cache.insert(
            DeviceType::Cpu,
            DeviceCapability {
                device_type: DeviceType::Cpu,
                supports_f16: false,
                supports_f32: true,
                supports_f64: true,
                supports_complex: true,
                supports_distributed: true,
                max_memory_gb: 32.0,
                supported_operations: cpu_ops,
            },
        );

        // CoreML capability
        #[cfg(feature = "coreml")]
        {
            self.capability_cache
                .insert(DeviceType::CoreML(0), DeviceCapability::coreml_capability());
        }

        // GPU capabilities would be detected at runtime
    }

    /// Detect available devices using device cache
    /// ãƒ‡ãƒã‚¤ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦åˆ©ç”¨å¯èƒ½ãƒ‡ãƒã‚¤ã‚¹ã‚’æ¤œå‡º
    fn detect_available_devices(device_cache: &DeviceCache) -> Vec<DeviceType> {
        let mut available = Vec::new();

        // Always include CPU
        available.push(DeviceType::Cpu);

        // Check CoreML availability
        if device_cache.is_device_available(&DeviceType::CoreML(0)) {
            available.push(DeviceType::CoreML(0));
        }

        // Check Metal availability
        if device_cache.is_device_available(&DeviceType::Metal(0)) {
            available.push(DeviceType::Metal(0));
        }

        // Check CUDA availability
        if device_cache.is_device_available(&DeviceType::Cuda(0)) {
            available.push(DeviceType::Cuda(0));
        }

        available
    }

    /// Smart device selection for operation
    /// æ“ä½œç”¨ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹é¸æŠ
    pub fn select_optimal_device(&self, tensor_info: &TensorInfo, op_type: OpType) -> DeviceType {
        // Convert OpType to OperationType
        let operation_type = match op_type {
            OpType::LinearAlgebra => OperationType::MatrixMultiplication,
            OpType::Activation => OperationType::Activation,
            OpType::Convolution => OperationType::Convolution,
            OpType::Reduction | OpType::Normalization => OperationType::ElementWise,
            _ => OperationType::ElementWise,
        };

        // Create operation profile
        let profile = OperationProfile::new(
            operation_type,
            &tensor_info.shape,
            self.get_dtype_size(&tensor_info.dtype),
        );

        // Use smart selector for optimal device
        let selected = self.smart_selector.select_device(&profile);

        // Validate device is actually available
        if self.device_cache.is_device_available(&selected) {
            selected
        } else {
            // Fallback to first available device
            self.fallback_devices.first().cloned().unwrap_or(DeviceType::Cpu)
        }
    }

    /// Get fallback chain for specific operation
    /// ç‰¹å®šæ“ä½œç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ã‚’å–å¾—
    pub fn get_operation_fallback_chain(&self, tensor_info: &TensorInfo, op_type: OpType) -> Vec<DeviceType> {
        // Convert OpType to OperationType
        let operation_type = match op_type {
            OpType::LinearAlgebra => OperationType::MatrixMultiplication,
            OpType::Activation => OperationType::Activation,
            OpType::Convolution => OperationType::Convolution,
            OpType::Reduction | OpType::Normalization => OperationType::ElementWise,
            _ => OperationType::ElementWise,
        };

        // Create operation profile
        let profile = OperationProfile::new(
            operation_type,
            &tensor_info.shape,
            self.get_dtype_size(&tensor_info.dtype),
        );

        // Get smart fallback chain
        self.smart_selector.get_fallback_chain(&profile)
    }

    /// Get data type size in bytes
    /// ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ã‚µã‚¤ã‚ºã‚’ãƒã‚¤ãƒˆå˜ä½ã§å–å¾—
    fn get_dtype_size(&self, dtype: &DType) -> usize {
        match dtype {
            DType::Float16 | DType::BFloat16 => 2,
            DType::Float32 => 4,
            DType::Float64 => 8,
            DType::Int8 => 1,
            DType::Int16 => 2,
            DType::Int32 => 4,
            DType::Int64 => 8,
            DType::UInt8 => 1,
            DType::UInt16 => 2,
            DType::UInt32 => 4,
            DType::UInt64 => 8,
            DType::Bool => 1,
            DType::Complex64 => 8,  // 2 * 32-bit floats
            DType::Complex128 => 16, // 2 * 64-bit floats
        }
    }

    /// Build fallback device chain based on actual hardware availability
    /// å®Ÿéš›ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ©ç”¨å¯èƒ½æ€§ã«åŸºã¥ã„ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒã‚¤ã‚¹ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
    fn build_fallback_chain(&mut self) {
        self.fallback_devices.clear();

        // Primary: CoreML if available (Apple Silicon only)
        #[cfg(feature = "coreml")]
        if self.is_device_available(DeviceType::CoreML(0)) {
            self.fallback_devices.push(DeviceType::CoreML(0));
        }

        // Platform-specific GPU selection
        if self.is_apple_silicon() {
            // Apple Silicon: CoreML â†’ Metal â†’ OpenCL â†’ CPU
            #[cfg(feature = "metal")]
            if self.is_device_available(DeviceType::Metal(0)) {
                self.fallback_devices.push(DeviceType::Metal(0));
            }
        } else {
            // Intel/AMD: CUDA (if NVIDIA GPU) â†’ OpenCL â†’ CPU
            #[cfg(feature = "cuda")]
            if self.is_device_available(DeviceType::Cuda(0)) {
                self.fallback_devices.push(DeviceType::Cuda(0));
            }
        }

        // OpenCL as universal fallback (works on both platforms)
        #[cfg(feature = "opencl")]
        if self.is_device_available(DeviceType::OpenCL(0)) {
            self.fallback_devices.push(DeviceType::OpenCL(0));
        }

        // Final: CPU (always available)
        self.fallback_devices.push(DeviceType::Cpu);

        // Debug log the actual fallback chain
        #[cfg(debug_assertions)]
        {
            eprintln!("ğŸ”„ Fallback chain: {:?}", self.fallback_devices);
        }
    }

    /// Check if running on Apple Silicon
    /// Apple Siliconã§å®Ÿè¡Œã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    fn is_apple_silicon(&self) -> bool {
        #[cfg(target_os = "macos")]
        {
            // Check if we're on Apple Silicon (ARM64)
            cfg!(target_arch = "aarch64")
        }
        #[cfg(not(target_os = "macos"))]
        {
            false
        }
    }

    /// Setup operation routing table
    /// æ¼”ç®—ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¨­å®š
    fn setup_operation_routing(&mut self) {
        // Complex math: Skip CoreML, go to GPU
        self.operation_routing.insert(
            OpType::ComplexMath,
            vec![
                DeviceType::Cuda(0),
                DeviceType::Metal(0),
                DeviceType::OpenCL(0),
                DeviceType::Cpu,
            ],
        );

        // Distributed ops: Skip CoreML and single GPU
        self.operation_routing.insert(
            OpType::DistributedOps,
            vec![
                DeviceType::Cpu, // NCCL/MPI fallback to CPU coordination
            ],
        );

        // Custom kernels: GPU only
        self.operation_routing.insert(
            OpType::CustomKernel,
            vec![
                DeviceType::Cuda(0),
                DeviceType::Metal(0),
                DeviceType::OpenCL(0),
                DeviceType::Cpu,
            ],
        );
    }

    /// Check if any GPU support is available
    /// GPUã‚µãƒãƒ¼ãƒˆãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    pub fn has_gpu_support(&self) -> bool {
        #[cfg(any(feature = "cuda", feature = "metal", feature = "opencl"))]
        {
            self.is_device_available(DeviceType::Cuda(0))
                || self.is_device_available(DeviceType::Metal(0))
                || self.is_device_available(DeviceType::OpenCL(0))
        }
        #[cfg(not(any(feature = "cuda", feature = "metal", feature = "opencl")))]
        {
            false
        }
    }
}

impl Default for HybridExecutor {
    fn default() -> Self {
        Self::new()
    }
}

impl HybridExecutor {
    /// Execute operation with hybrid device selection
    /// ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‡ãƒã‚¤ã‚¹é¸æŠã§æ¼”ç®—ã‚’å®Ÿè¡Œ
    pub fn hybrid_operation<F, R>(
        &self,
        op_type: OpType,
        tensor_info: TensorInfo,
        operation: F,
    ) -> RusTorchResult<R>
    where
        F: Fn(DeviceType) -> RusTorchResult<R>,
    {
        // Get optimal device for this operation
        let device = self.select_device(op_type, &tensor_info);

        // Try to execute on selected device
        match operation(device) {
            Ok(result) => Ok(result),
            Err(err) => {
                // If failed, try fallback devices
                for fallback_device in self.get_fallback_chain(device) {
                    if let Ok(result) = operation(fallback_device) {
                        return Ok(result);
                    }
                }
                // If all devices failed, return original error
                Err(err)
            }
        }
    }

    /// Get fallback device chain for a given device
    /// æŒ‡å®šãƒ‡ãƒã‚¤ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ã‚’å–å¾—
    fn get_fallback_chain(&self, device: DeviceType) -> Vec<DeviceType> {
        match device {
            DeviceType::CoreML(_) => {
                vec![DeviceType::Metal(0), DeviceType::Cuda(0), DeviceType::Cpu]
            }
            DeviceType::Metal(_) => vec![DeviceType::Cuda(0), DeviceType::Cpu],
            DeviceType::Cuda(_) => vec![DeviceType::Metal(0), DeviceType::Cpu],
            DeviceType::OpenCL(_) => vec![DeviceType::Cpu],
            _ => vec![DeviceType::Cpu],
        }
    }
}

/// Trait for hybrid execution support
/// ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œã‚µãƒãƒ¼ãƒˆç”¨ãƒˆãƒ¬ã‚¤ãƒˆ
pub trait HybridExecution<T> {
    /// Execute operation with automatic device selection and fallback
    /// è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é¸æŠã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§æ¼”ç®—ã‚’å®Ÿè¡Œ
    fn hybrid_operation<F, R>(&self, op_type: OpType, operation: F) -> RusTorchResult<R>
    where
        F: Fn(DeviceType) -> RusTorchResult<R>;

    /// Get tensor information for device selection
    /// ãƒ‡ãƒã‚¤ã‚¹é¸æŠç”¨ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã‚’å–å¾—
    fn tensor_info(&self) -> TensorInfo;
}
