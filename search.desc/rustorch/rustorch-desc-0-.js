searchState.loadedDescShard("rustorch", 0, "RusTorch üöÄ\nAutomatic Mixed Precision (AMP) training support ‚Ä¶\nMacro for autocast-aware operations\nAutomatic differentiation module ‚Ä¶\nUnified compute backend abstraction layer ‚Ä¶\nCommon utilities and shared functionality ‚Ä¶\nPyTorch to RusTorch conversion system ‚Ä¶\nData loading and processing utilities (Phase 5 API) ‚Ä¶\nDebug and logging system „Éá„Éê„ÉÉ„Ç∞„Éª„É≠„Ç∞„Ç∑„Çπ„ÉÜ„É† ‚Ä¶\nMacro for easy diagnostic context creation\nDistributed training support for multi-GPU and ‚Ä¶\nMacro for creating distributed errors easily ‚Ä¶\nStatistical distributions module providing ‚Ä¶\nData types for tensors „ÉÜ„É≥„ÇΩ„É´Áî®„Éá„Éº„ÇøÂûã\nUnified error handling system ‚Ä¶\nMacro for creating error context with file location ‚Ä¶\nDynamic execution engine for runtime graph optimization ‚Ä¶\nModel format support and conversion utilities ‚Ä¶\nGPU acceleration support (CUDA, Metal, OpenCL) ‚Ä¶\nMacro for creating GPU errors easily ‚Ä¶\nHigh-performance linear algebra with BLAS integration ‚Ä¶\nConvenience macro for structured logging\nMemory management and pooling utilities ‚Ä¶\nModel import functionality for PyTorch and ONNX models ‚Ä¶\nPre-built models and architectures ‚Ä¶\nNeural network layers and building blocks ‚Ä¶\nOptimization algorithms ÊúÄÈÅ©Âåñ„Ç¢„É´„Ç¥„É™„Ç∫„É†\nCross-platform optimization module ‚Ä¶\nParallel processing utilities ‚Ä¶\nMacro for performance timing\nRe-exports of commonly used items\nProfile a code block ‚Ä¶\nProfile a function Èñ¢Êï∞„Çí„Éó„É≠„Éï„Ç°„Ç§„É´\nPerformance profiler ‚Ä¶\nQuantization support for model compression and ‚Ä¶\nSerialization and model I/O system (Phase 9) ‚Ä¶\nConvenience macro for chaining shape operations ‚Ä¶\nSIMD vectorized operations for performance optimization ‚Ä¶\nSparse tensor support and operations (Phase 12) ‚Ä¶\nSpecial mathematical functions (gamma, Bessel, error ‚Ä¶\nMacro for easy TensorBoard logging ‚Ä¶\nTensor operations and data structures ‚Ä¶\nConvenient macro for creating tensors with literal syntax\nHelper macros for error creation ‚Ä¶\nCreates an N-dimensional tensor with compile-time shape ‚Ä¶\nTensorBoard integration TensorBoardÁµ±Âêà TensorBoard ‚Ä¶\nMacro for timed code blocks\nConvenience macro for tracking allocations\nTraining loop abstractions and utilities ‚Ä¶\nUtility functions „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞ Utility ‚Ä¶\nData validation and quality assurance system ‚Ä¶\nComputer vision module providing image transforms, data ‚Ä¶\nVisualization tools for plots, graphs, and data analysis ‚Ä¶\nMacro for adding context to results ‚Ä¶\nGlobal AMP configuration „Ç∞„É≠„Éº„Éê„É´AMPË®≠ÂÆö\nAMP-aware optimizer wrapper that handles mixed precision ‚Ä¶\nAutocast context manager\nAutocast mode for mixed precision training ‚Ä¶\nUse BF16 for reduced precision\nUse FP16 for reduced precision\nGradient scaler for automatic mixed precision training\nStep skipped due to inf/nan in gradients\nMixed precision tensor operations\nNo autocasting\nStep skipped due to gradient overflow\nParameter group configuration for AMP\nState of the gradient scaler\nStatistics about gradient scaling ‚Ä¶\nResult of a gradient scaling step\nStep completed successfully\nTraining statistics for AMP optimization ‚Ä¶\nGradually adjust growth interval based on overflow ‚Ä¶\nAdd a parameter group with specific AMP settings\nCreate an autocast context\nBackoff factor for loss scale\nBackoff factor for scale\nBackoff factor when overflow occurs ‚Ä¶\nCreate config for BF16\nCheck if tensor can be safely cast to target dtype\nCast BF16 tensor to FP32 (passthrough for compatibility)\nGeneric tensor casting function\nCast tensor to target dtype\nCast tensor to BF16 (simulated - converts to BF16 and back ‚Ä¶\nCast tensor to FP16 (simulated - converts to FP16 and back ‚Ä¶\nCast tensor to FP32 (passthrough for compatibility)\nCheck for gradient overflow/underflow\nWhether to use gradient clipping for this group\nNumber of consecutive non-overflowed iterations\nNumber of consecutive non-overflow steps ‚Ä¶\nCurrent scale factor ÁèæÂú®„ÅÆ„Çπ„Ç±„Éº„É´‰øÇÊï∞\nCreate a default scaler\nDisable AMP globally\nPreferred reduced precision dtype (FP16 or BF16)\nWhether to use dynamic loss scaling\nEnable AMP globally\nWhether to enable autocast\nWhether dynamic scaling is enabled\nWhether scaling is enabled ‚Ä¶\nEnter the autocast context\nExit the autocast context\nCreate config for FP16 with static scaling\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet current AMP configuration\nGet recommended actions based on statistics\nGet current scale\nGet detailed statistics about scaling\nGet training statistics\nGrowth factor for loss scale\nGrowth factor for scale\nGrowth factor for scale updates ‚Ä¶\nGrowth interval\nNumber of iterations between scale updates\nInterval between growth updates ÊàêÈï∑Êõ¥Êñ∞„ÅÆÈñìÈöî\nNumber of iterations since last scale update\nCurrent growth tracking counter ‚Ä¶\nWhether overflow was detected ‚Ä¶\nInitial loss scale\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if AMP is enabled\nCheck if scaling is enabled\nCheck if training is stable (low overflow rate)\nLoad scaler state\nMaximum gradient norm for clipping\nCast tensor to autocast dtype if enabled\nGet memory footprint in bytes\nGet memory footprint for target dtype\nCreate a new autocast context\nCreate a new gradient scaler\nCreate a new AMP optimizer wrapper\nGet the underlying optimizer\nGet mutable reference to the underlying optimizer\nNumber of overflow occurrences ‚Ä¶\nRate of overflow occurrences „Ç™„Éº„Éê„Éº„Éï„É≠„ÉºÁô∫ÁîüÁéá\nParameter IDs in this group\nReset scaler state (useful after loading checkpoints)\nReset optimizer state\nScale the loss\nCurrent scale factor\nScale tensor directly\nGet the gradient scaler\nGet mutable reference to the gradient scaler\nGradient scaler statistics ÂãæÈÖç„Çπ„Ç±„Éº„É©„ÉºÁµ±Ë®à\nEnable or disable scaling\nSet scale manually\nSet scale bounds\nGet scaler state\nPerform optimizer step with scaling\nPerform an optimization step with AMP\nAdvanced step with gradient clipping and inf/nan detection\nGet success rate\nNumber of successful steps ÊàêÂäü„Åó„Åü„Çπ„ÉÜ„ÉÉ„ÉóÊï∞\nTotal number of optimization steps ‚Ä¶\nUnscale gradients\nUpdate the scale factor\nUpdate learning rate schedule and adaptive scaling\nWhether to use mixed precision for this group\nMixed precision training utilities\nZero gradients (delegate to underlying optimizer)\nGradient norm before clipping (if clipping was applied)\nNew scale factor after backoff\nScale factor used\nCurrent scale factor\nCurrent scale factor\nGet optimal dtype for current hardware\nCheck if hardware supports BF16\nCheck if hardware supports FP16\nCheck if operation should use reduced precision\nGradient function trait for backward computation ‚Ä¶\nA variable that supports automatic differentiation. ‚Ä¶\nApply the gradient function to compute input gradients ‚Ä¶\nBatch matrix multiplication for 4D tensors (batch_size, ‚Ä¶\nPerforms backward pass to compute gradients. ‚Ä¶\nPerforms backward pass with a specific gradient. ‚Ä¶\nGradient context management for automatic differentiation ‚Ä¶\nReturns the data tensor. ‚Ä¶\nReturns the argument unchanged.\nAutomatic differentiation functions Ëá™ÂãïÂæÆÂàÜÈñ¢Êï∞\nReturns the gradient tensor. ‚Ä¶\nGradient functions for automatic differentiation ‚Ä¶\nReturns the gradient function if any. ‚Ä¶\nAdvanced gradient computation utilities ‚Ä¶\nGradient checking utilities for numerical validation ‚Ä¶\nComputation graph for automatic differentiation ‚Ä¶\nHigher-order derivatives: Jacobian and Hessian computation ‚Ä¶\nReturns the unique identifier for this Variable ‚Ä¶\nCalls <code>U::from(self)</code>.\nLinear layer gradient functions ‚Ä¶\nMatrix multiplication with automatic differentiation ‚Ä¶\nCompute the mean of all elements with proper gradient ‚Ä¶\nMean of all elements with automatic differentiation support\nCreates a new variable with the given tensor. ‚Ä¶\nCreates a new variable with gradient function ‚Ä¶\nPower function with automatic differentiation support ‚Ä¶\nReturns whether this variable requires gradients. ‚Ä¶\nSum all elements with automatic differentiation support ‚Ä¶\nSum along a specific dimension (simplified implementation) ‚Ä¶\nTranspose the last two dimensions ÊúÄÂæå„ÅÆ2Ê¨°ÂÖÉ„ÇíËª¢ÁΩÆ\nGradient flow visualization for computational graphs ‚Ä¶\nZeros out the gradient. ‚Ä¶\nRAII guard for temporarily enabling anomaly detection ‚Ä¶\nRAII guard for temporarily enabling gradient computation ‚Ä¶\nGlobal gradient context state ‚Ä¶\nRAII guard for temporarily disabling gradient computation ‚Ä¶\nConvenience function for anomaly detection context ‚Ä¶\nConvenience function for enable_grad context ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if anomaly detection is enabled ‚Ä¶\nCheck if gradient computation is currently enabled ‚Ä¶\nCreate a new NoGradGuard, disabling gradient computation ‚Ä¶\nCreate a new EnableGradGuard, enabling gradient computation\nCreate a new AnomalyDetectionGuard, enabling anomaly ‚Ä¶\nConvenience function for no_grad context ‚Ä¶\nSet anomaly detection state Áï∞Â∏∏Ê§úÂá∫Áä∂ÊÖã„ÇíË®≠ÂÆö\nSet gradient computation state ÂãæÈÖçË®àÁÆóÁä∂ÊÖã„ÇíË®≠ÂÆö\nAddition function Âä†ÁÆóÈñ¢Êï∞\nFunction trait for automatic differentiation ‚Ä¶\nMatrix multiplication function Ë°åÂàó‰πóÁÆóÈñ¢Êï∞\nMultiplication function ‰πóÁÆóÈñ¢Êï∞\nSubtraction function Ê∏õÁÆóÈñ¢Êï∞\nSum function Á∑èÂíåÈñ¢Êï∞\nApply the backward pass ÈÄÜ‰ºùÊí≠„ÇíÈÅ©Áî®\nApply the forward pass È†Ü‰ºùÊí≠„ÇíÈÅ©Áî®\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nAddition backward function Âä†ÁÆó„ÅÆÈÄÜ‰ºùÊí≠Èñ¢Êï∞\nMatrix multiplication backward function ‚Ä¶\nMean backward function Âπ≥Âùá„ÅÆÈÄÜ‰ºùÊí≠Èñ¢Êï∞\nMultiplication backward function ‰πóÁÆó„ÅÆÈÄÜ‰ºùÊí≠Èñ¢Êï∞\nSubtraction backward function Ê∏õÁÆó„ÅÆÈÄÜ‰ºùÊí≠Èñ¢Êï∞\nSum backward function Á∑èÂíå„ÅÆÈÄÜ‰ºùÊí≠Èñ¢Êï∞\nPhantom data for type parameter ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nFirst input tensor data ‚Ä¶\nFirst input tensor data ‚Ä¶\nFirst input tensor data for gradient computation ‚Ä¶\nFirst input tensor data for gradient computation ‚Ä¶\nFirst input variable ÊúÄÂàù„ÅÆÂÖ•ÂäõÂ§âÊï∞\nFirst input variable ÊúÄÂàù„ÅÆÂÖ•ÂäõÂ§âÊï∞\nFirst input variable ÊúÄÂàù„ÅÆÂÖ•ÂäõÂ§âÊï∞\nFirst input variable ÊúÄÂàù„ÅÆÂÖ•ÂäõÂ§âÊï∞\nSecond input tensor data ‚Ä¶\nSecond input tensor data ‚Ä¶\nSecond input tensor data for gradient computation ‚Ä¶\nSecond input tensor data for gradient computation ‚Ä¶\nSecond input variable 2Áï™ÁõÆ„ÅÆÂÖ•ÂäõÂ§âÊï∞\nSecond input variable 2Áï™ÁõÆ„ÅÆÂÖ•ÂäõÂ§âÊï∞\nSecond input variable 2Áï™ÁõÆ„ÅÆÂÖ•ÂäõÂ§âÊï∞\nSecond input variable 2Áï™ÁõÆ„ÅÆÂÖ•ÂäõÂ§âÊï∞\nOriginal input tensor shape for gradient broadcasting ‚Ä¶\nInput variable for gradient propagation ‚Ä¶\nInput variable ÂÖ•ÂäõÂ§âÊï∞\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nNumber of elements Ë¶ÅÁ¥†Êï∞\nAutomatic differentiation error Ëá™ÂãïÂæÆÂàÜ„Ç®„É©„Éº\nBackend not available „Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅåÂà©Áî®‰∏çÂèØ\nData loading error „Éá„Éº„ÇøË™≠„ÅøËæº„Åø„Ç®„É©„Éº\nDataLoader system errors (Phase 5) ‚Ä¶\nDebug system and logging errors ‚Ä¶\nDevice operation failed „Éá„Éê„Ç§„ÇπÊìç‰ΩúÂ§±Êïó\nDistributed computing errors ‚Ä¶\nGPU-specific errors GPUÂõ∫Êúâ„Ç®„É©„Éº\nType alias for gradient errors using unified RusTorchError ‚Ä¶\nInput/Output and serialization errors ‚Ä¶\nImport-specific error „Ç§„É≥„Éù„Éº„ÉàÂõ∫Êúâ„Ç®„É©„Éº\nInvalid operation ÁÑ°Âäπ„Å™Êìç‰Ωú\nInvalid operation parameters ÁÑ°Âäπ„Å™Êìç‰Ωú„Éë„É©„É°„Éº„Çø\nKernel compilation error for GPU operations ‚Ä¶\nMemory allocation failed „É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶Â§±Êïó\nModel import/export error ‚Ä¶\nNeural network layer error ‚Ä¶\nFeature not implemented yet Ê©üËÉΩÊú™ÂÆüË£Ö\nOut of memory error „É°„É¢„É™‰∏çË∂≥„Ç®„É©„Éº\nProfiling and performance analysis errors ‚Ä¶\nSerialization error „Ç∑„É™„Ç¢„É©„Ç§„Çº„Éº„Ç∑„Éß„É≥„Ç®„É©„Éº\nSerialization and model I/O errors (Phase 9) ‚Ä¶\nShape mismatch between tensors ‚Ä¶\nTensor operation failed „ÉÜ„É≥„ÇΩ„É´Êìç‰ΩúÂ§±Êïó\nData validation and quality assurance errors ‚Ä¶\nVision processing errors Ë¶ñË¶öÂá¶ÁêÜ„Ç®„É©„Éº\nVisualization errors ÂèØË¶ñÂåñ„Ç®„É©„Éº\nCompute gradients of outputs with respect to inputs ‚Ä¶\nCompute the gradient of a scalar function ‚Ä¶\nUtility function to check if a variable is in the ‚Ä¶\nValidate gradient computation setup ‚Ä¶\nActual tensor shape that was provided ‚Ä¶\nAvailable memory in bytes ‚Ä¶\nName of the unavailable backend ‚Ä¶\nDevice identifier where the error occurred ‚Ä¶\nDevice where allocation failed ‚Ä¶\nExpected tensor shape ÊúüÂæÖ„Åï„Çå„Çã„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nFeature that is not yet implemented Êú™ÂÆüË£Ö„ÅÆÊ©üËÉΩ\nName of the neural network layer ‚Ä¶\nError message describing the tensor operation failure ‚Ä¶\nError message describing the device issue ‚Ä¶\nDescription of the parameter issue ‚Ä¶\nDetailed error message Ë©≥Á¥∞„Å™„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏\nError message from the layer ‚Ä¶\nAutograd error description Ëá™ÂãïÂæÆÂàÜ„Ç®„É©„Éº„ÅÆË™¨Êòé\nModel I/O error description „É¢„Éá„É´I/O„Ç®„É©„Éº„ÅÆË™¨Êòé\nImport operation error message ‚Ä¶\nData loading error description ‚Ä¶\nGPU operation error description GPUÊìç‰Ωú„Ç®„É©„Éº„ÅÆË™¨Êòé\nVision processing error description ‚Ä¶\nDistributed computing error description ‚Ä¶\nVisualization error description ÂèØË¶ñÂåñ„Ç®„É©„Éº„ÅÆË™¨Êòé\nProfiling error description ‚Ä¶\nData validation error description ‚Ä¶\nDebug system error description ‚Ä¶\nKernel compilation error description ‚Ä¶\nDataLoader error description DataLoader„Ç®„É©„Éº„ÅÆË™¨Êòé\nSerialization error description ‚Ä¶\nSerialization error description ‚Ä¶\nName of the operation with invalid parameters ‚Ä¶\nName of the invalid operation ÁÑ°Âäπ„Å™Êìç‰ΩúÂêç\nOperation that caused the serialization error ‚Ä¶\nRequested memory in bytes ‚Ä¶\nSize of the failed allocation in bytes ‚Ä¶\nOptional underlying error cause ‚Ä¶\nOptional underlying I/O error ‚Ä¶\nOptional underlying import error ‚Ä¶\nOptional underlying data loading error ‚Ä¶\nOptional underlying GPU error ‚Ä¶\nOptional underlying vision error ‚Ä¶\nOptional underlying distributed error ‚Ä¶\nOptional underlying visualization error ‚Ä¶\nGradient checking configuration ÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØË®≠ÂÆö\nResult of gradient checking ÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„ÅÆÁµêÊûú\nReturns the argument unchanged.\nReturns the argument unchanged.\nCheck gradients using finite differences ‚Ä¶\nSimplified gradient checking function with default ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nComputation graph for automatic differentiation ‚Ä¶\nNode in the computation graph Ë®àÁÆó„Ç∞„É©„Éï„ÅÆ„Éé„Éº„Éâ\nAccumulate gradient for this node ‚Ä¶\nAdd a function node to the graph ‚Ä¶\nAdd a leaf node to the graph ‚Ä¶\nPerform backward pass from the given node ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nFunction that created this node ‚Ä¶\nGet a node by ID ID„Å´„Çà„Å£„Å¶„Éé„Éº„Éâ„ÇíÂèñÂæó\nGet a mutable node by ID ‚Ä¶\nGradient accumulator ÂãæÈÖç„Ç¢„Ç≠„É•„É†„É¨„Éº„Çø\nInput tensors (kept for backward pass) ‚Ä¶\nInput nodes ÂÖ•Âäõ„Éé„Éº„Éâ\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new computation graph ‚Ä¶\nCreate a new function node ‚Ä¶\nCreate a new leaf node (no function, no inputs) ‚Ä¶\nWhether this node requires gradient computation ‚Ä¶\nClear gradient for this node ‚Ä¶\nCompute the Hessian matrix of a scalar-valued function ‚Ä¶\nCompute Hessian-vector product (HVP) efficiently ‚Ä¶\nCompute the Jacobian matrix of a vector-valued function ‚Ä¶\nLinear layer backward function ‚Ä¶\nBias variable for gradient propagation (optional) ‚Ä¶\nReturns the argument unchanged.\nInput tensor for gradient computation ‚Ä¶\nInput variable for gradient propagation ‚Ä¶\nCalls <code>U::from(self)</code>.\nWeight tensor for gradient computation ‚Ä¶\nWeight variable for gradient propagation ‚Ä¶\nGradient is decreasing\nDisconnected parameter (no gradient) ‚Ä¶\nGradient is exploding\nExploding gradient detected ÂãæÈÖçÁàÜÁô∫„ÅåÊ§úÂá∫„Åï„Çå„Åü\nInteractive gradient flow analyzer ‚Ä¶\nGradient flow issues detected ‚Ä¶\nSummary of gradient flow statistics ‚Ä¶\nGradient flow visualizer ‚Ä¶\nGradient trend analysis result ‚Ä¶\nGradient is increasing\nInput variable ÂÖ•ÂäõÂ§âÊï∞\nNode information for visualization ‚Ä¶\nNode types in the computation graph ‚Ä¶\nOperation node ÊºîÁÆó„Éé„Éº„Éâ\nLoss/output node ÊêçÂ§±/Âá∫Âäõ„Éé„Éº„Éâ\nParameter (trainable) „Éë„É©„É°„Éº„ÇøÔºàË®ìÁ∑¥ÂèØËÉΩÔºâ\nGradient is stable\nGradient is vanishing\nVanishing gradient detected ÂãæÈÖçÊ∂àÂ§±„ÅåÊ§úÂá∫„Åï„Çå„Åü\nAdd an operation node ÊºîÁÆó„Éé„Éº„Éâ„ÇíËøΩÂä†\nAnalyze gradient trends ÂãæÈÖç„Éà„É¨„É≥„Éâ„ÇíÂàÜÊûê\nAverage gradient norm Âπ≥ÂùáÂãæÈÖç„Éé„É´„É†\nClear the visualizer for reuse ‚Ä¶\nClear history Â±•Ê≠¥„Çí„ÇØ„É™„Ç¢\nDetect potential gradient flow issues ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet gradient history for a parameter ‚Ä¶\nGenerate a summary of gradient flow statistics ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum gradient norm ÊúÄÂ§ßÂãæÈÖç„Éé„É´„É†\nMinimum gradient norm ÊúÄÂ∞èÂãæÈÖç„Éé„É´„É†\nCreate a new gradient flow visualizer ‚Ä¶\nCreate a new gradient flow analyzer ‚Ä¶\nNumber of nodes with gradients ‚Ä¶\nNumber of parameter nodes „Éë„É©„É°„Éº„Çø„Éé„Éº„Éâ„ÅÆÊï∞\nRecord gradient norm for a parameter ‚Ä¶\nSave visualization to file ‚Ä¶\nGenerate DOT format for Graphviz ‚Ä¶\nTotal number of edges „Ç®„ÉÉ„Ç∏„ÅÆÁ∑èÊï∞\nTotal number of nodes „Éé„Éº„Éâ„ÅÆÁ∑èÊï∞\nTrace gradient flow from a variable ‚Ä¶\nGradient norm value\nGradient norm value\nNode label\nNode label\nNode label\nBackend factory for creating and managing compute backends ‚Ä¶\nResult type for backend operations (now unified) ‚Ä¶\nCore compute backend trait that all backends must implement\nParameters for convolution operations ‚Ä¶\nCPU computation CPUË®àÁÆó\nNVIDIA CUDA GPU NVIDIA CUDA GPU\nDevice memory buffer abstraction ‚Ä¶\nDevice information and capabilities ‚Ä¶\nTypes of compute devices Ë®àÁÆó„Éá„Éê„Ç§„Çπ„ÅÆÁ®ÆÈ°û\nContains the error value\nApple Metal GPU Apple Metal GPU\nContains the success value\nOpenCL compatible device OpenCL‰∫íÊèõ„Éá„Éê„Ç§„Çπ\nElement-wise addition: a + b Ë¶ÅÁ¥†„Åî„Å®Âä†ÁÆó: a + b\nAllocate memory on the device ‚Ä¶\nGet raw pointer for device operations (unsafe) ‚Ä¶\nGet all available backends on the current system ‚Ä¶\nAvailable memory in bytes ‚Ä¶\nGet backend-specific context for advanced operations ‚Ä¶\nBatch normalization „Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñ\nUnified Compute Backend Abstraction Layer\nConvolution operation Áï≥„ÅøËæº„ÅøÊìç‰Ωú\nCopy data from host to device ‚Ä¶\nCopy data from device to host ‚Ä¶\nUnified CPU backend implementation for RusTorch ‚Ä¶\nCreate the best available CPU backend for the current ‚Ä¶\nGet device information „Éá„Éê„Ç§„ÇπÊÉÖÂ†±„ÇíÂèñÂæó\nDevice type „Éá„Éê„Ç§„Çπ„Çø„Ç§„Éó\nDilation [height, width] or [depth, height, width] ËÜ®Âºµ [‚Ä¶\nElement-wise division: a / b Ë¶ÅÁ¥†„Åî„Å®Èô§ÁÆó: a / b\nExecute custom kernel/operation (backend-specific) ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nNumber of groups for grouped convolution ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if this backend is available on the current system ‚Ä¶\nKernel size [height, width] or [depth, height, width] ‚Ä¶\nMatrix multiplication: a @ b Ë°åÂàó‰πóÁÆó: a @ b\nMaximum element ÊúÄÂ§ßË¶ÅÁ¥†\nMaximum number of threads/cores ‚Ä¶\nMean of all elements ÂÖ®Ë¶ÅÁ¥†„ÅÆÂπ≥Âùá\nMinimum element ÊúÄÂ∞èË¶ÅÁ¥†\nElement-wise multiplication: a * b Ë¶ÅÁ¥†„Åî„Å®‰πóÁÆó: a * ‚Ä¶\nDevice name „Éá„Éê„Ç§„ÇπÂêç\nPadding [height, width] or [depth, height, width] ‚Ä¶\nActivation functions Ê¥ªÊÄßÂåñÈñ¢Êï∞\nReshape tensor to new shape ‚Ä¶\nApply sigmoid activation function ‚Ä¶\nSize of the buffer in bytes ‚Ä¶\nStride [height, width] or [depth, height, width] ‚Ä¶\nElement-wise subtraction: a - b Ë¶ÅÁ¥†„Åî„Å®Ê∏õÁÆó: a - b\nSum all elements ÂÖ®Ë¶ÅÁ¥†„ÅÆÂêàË®à\nSupports half precision ÂçäÁ≤æÂ∫¶„Çµ„Éù„Éº„Éà\nSupports double precision ÂÄçÁ≤æÂ∫¶„Çµ„Éù„Éº„Éà\nSynchronize device execution (wait for all operations to ‚Ä¶\nApply hyperbolic tangent activation function ‚Ä¶\nTotal memory in bytes Á∑è„É°„É¢„É™Ôºà„Éê„Ç§„ÉàÔºâ\nTranspose tensor along specified axes ‚Ä¶\nElement-wise addition Ë¶ÅÁ¥†„Åî„Å®„ÅÆÂä†ÁÆó\nBalance between performance and memory ‚Ä¶\nUnified compute backend trait (non-generic methods) ‚Ä¶\nGeneric backend operations ‚Ä¶\nConvolution operation Áï≥„ÅøËæº„ÅøÊìç‰Ωú\nCPU computation using optimized algorithms ‚Ä¶\nNVIDIA CUDA GPU acceleration NVIDIA CUDA GPUÂä†ÈÄü\nDevice manager for unified backend selection ‚Ä¶\nDevice to Host (GPU ‚Üí CPU) ‚Ä¶\nDevice types available for computation ‚Ä¶\nHost to Device (CPU ‚Üí GPU) ‚Ä¶\nUser-specified device priority ‚Ä¶\nMatrix multiplication Ë°åÂàó‰πóÁÆó\nMaximum value reduction ÊúÄÂ§ßÂÄ§„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nMean (average) reduction Âπ≥Âùá„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nAlways prefer backends with most available memory ‚Ä¶\nApple Metal GPU acceleration Apple Metal GPUÂä†ÈÄü\nMinimum value reduction ÊúÄÂ∞èÂÄ§„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nElement-wise multiplication Ë¶ÅÁ¥†„Åî„Å®„ÅÆ‰πóÁÆó\nOpenCL cross-platform GPU acceleration ‚Ä¶\nGeneric operation type for backend execution ‚Ä¶\nAlways prefer the fastest available backend ‚Ä¶\nPerformance metrics for operations ‚Ä¶\nProduct reduction Á©ç„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nReduction operations (sum, mean, etc.) ‚Ä¶\nReduction operation types „É™„ÉÄ„ÇØ„Ç∑„Éß„É≥Êìç‰ΩúÁ®ÆÂà•\nBackend selection strategy „Éê„ÉÉ„ÇØ„Ç®„É≥„ÉâÈÅ∏ÊäûÊà¶Áï•\nSum reduction ÂêàË®à„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nMemory transfer direction between host and device ‚Ä¶\nGet available device types ‚Ä¶\nGet available memory in bytes ‚Ä¶\nReturns the device type this backend handles ‚Ä¶\nDevice utilization percentage ‚Ä¶\nExecute a generic operation on this backend ‚Ä¶\nExecute operation on the best available backend ‚Ä¶\nExecution time in nanoseconds ÂÆüË°åÊôÇÈñìÔºà„Éä„ÉéÁßíÔºâ\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance statistics for a device ‚Ä¶\nGet backend-specific information ‚Ä¶\nGet performance metrics for the last operation ‚Ä¶\nGet the global device manager ‚Ä¶\nInitialize the backend „Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÇíÂàùÊúüÂåñ\nInitialize all available backends ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns whether this backend is available on the current ‚Ä¶\nMemory bandwidth in GB/s „É°„É¢„É™Â∏ØÂüüÂπÖÔºàGB/sÔºâ\nTransfer memory between host and device ‚Ä¶\nMemory usage in bytes „É°„É¢„É™‰ΩøÁî®ÈáèÔºà„Éê„Ç§„ÉàÔºâ\nCreate a new device manager ‚Ä¶\nRegister a compute backend (temporarily CPU-only) ‚Ä¶\nSelect the best backend for an operation ‚Ä¶\nSet device-specific configuration ‚Ä¶\nBackend selection strategy „Éê„ÉÉ„ÇØ„Ç®„É≥„ÉâÈÅ∏ÊäûÊà¶Áï•\nSynchronize device operations (wait for completion) ‚Ä¶\nFirst input tensor Á¨¨1ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\nFirst input tensor Á¨¨1ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\nLeft matrix tensor Â∑¶Ë°åÂàó„ÉÜ„É≥„ÇΩ„É´\nAxes along which to reduce (None for all axes) ‚Ä¶\nSecond input tensor Á¨¨2ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\nSecond input tensor Á¨¨2ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\nRight matrix tensor Âè≥Ë°åÂàó„ÉÜ„É≥„ÇΩ„É´\nInput tensor for convolution ‚Ä¶\nInput tensor for reduction ‚Ä¶\nConvolution kernel/filter ‚Ä¶\nType of reduction operation ‚Ä¶\nPadding for convolution (height, width) ‚Ä¶\nStride for convolution (height, width) ‚Ä¶\nUnified CPU backend implementation with SIMD optimizations ‚Ä¶\nCPU SIMD feature detection CPU SIMDÊ©üËÉΩÊ§úÂá∫\nAVX2 support\nAVX512F support\nDetect available SIMD features ‚Ä¶\nFMA support\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new unified CPU backend ‚Ä¶\nSSE4.1 support\nUnified error handling for RusTorch ‚Ä¶\nActivation function error Ê¥ªÊÄßÂåñÈñ¢Êï∞„Ç®„É©„Éº\nMemory alignment error „É°„É¢„É™„Ç¢„É©„Ç§„É°„É≥„Éà„Ç®„É©„Éº\nMemory allocation failed „É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶Â§±Êïó\nDistributed backend not supported ‚Ä¶\nBackward pass error ÈÄÜ‰ºùÊí≠„Ç®„É©„Éº\nBatch processing error „Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Ç®„É©„Éº\nCommunication failed ÈÄö‰ø°Â§±Êïó\nGPU context creation failed ‚Ä¶\nConvergence error ÂèéÊùü„Ç®„É©„Éº\nData loading errors „Éá„Éº„Çø„É≠„Éº„Éá„Ç£„É≥„Ç∞„Ç®„É©„Éº\nData loading errors „Éá„Éº„Çø„É≠„Éº„Éá„Ç£„É≥„Ç∞„Ç®„É©„Éº\nData loader error „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„Ç®„É©„Éº\nData type error „Éá„Éº„ÇøÂûã„Ç®„É©„Éº\nDataset error „Éá„Éº„Çø„Çª„ÉÉ„Éà„Ç®„É©„Éº\nMemory deallocation failed „É°„É¢„É™Ëß£ÊîæÂ§±Êïó\nGPU device not found GPU„Éá„Éê„Ç§„Çπ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ\nGPU device not supported ‚Ä¶\nTensor dimension mismatch error ‚Ä¶\nDistributed computing errors ‚Ä¶\nDistributed operation errors ÂàÜÊï£Êìç‰Ωú„Ç®„É©„Éº\nGPU driver error GPU„Éâ„É©„Ç§„Éê„Éº„Ç®„É©„Éº\nEmpty tensor error Á©∫„ÅÆ„ÉÜ„É≥„ÇΩ„É´„Ç®„É©„Éº\nContains the error value\nFile operation error „Éï„Ç°„Ç§„É´Êìç‰Ωú„Ç®„É©„Éº\nForward pass error È†Ü‰ºùÊí≠„Ç®„É©„Éº\nGeneric errors Ê±éÁî®„Ç®„É©„Éº\nGPU-specific errors GPUÂõ∫Êúâ„Ç®„É©„Éº\nGPU operation errors GPUÊìç‰Ωú„Ç®„É©„Éº\nGradient calculation error ÂãæÈÖçË®àÁÆó„Ç®„É©„Éº\nInsufficient tensor dimensions error ‚Ä¶\nInvalid GPU device ÁÑ°Âäπ„Å™GPU„Éá„Éê„Ç§„Çπ\nInvalid tensor index ‚Ä¶\nInvalid tensor operation ÁÑ°Âäπ„Å™„ÉÜ„É≥„ÇΩ„É´Êìç‰Ωú\nInvalid memory pointer ÁÑ°Âäπ„Å™„É°„É¢„É™„Éù„Ç§„É≥„Çø\nInvalid rank ÁÑ°Âäπ„Å™„É©„É≥„ÇØ\nInvalid tensor shape ÁÑ°Âäπ„Å™„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nInvalid world size ÁÑ°Âäπ„Å™„ÉØ„Éº„É´„Éâ„Çµ„Ç§„Ç∫\nI/O errors I/O„Ç®„É©„Éº\nGPU kernel compilation failed ‚Ä¶\nGPU kernel execution failed GPU„Ç´„Éº„Éç„É´ÂÆüË°åÂ§±Êïó\nNeural network layer error ‚Ä¶\nLearning rate error Â≠¶ÁøíÁéá„Ç®„É©„Éº\nLoss function error ÊêçÂ§±Èñ¢Êï∞„Ç®„É©„Éº\nGPU memory allocation failed GPU„É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶Â§±Êïó\nMemory management errors „É°„É¢„É™ÁÆ°ÁêÜ„Ç®„É©„Éº\nMemory management errors „É°„É¢„É™ÁÆ°ÁêÜ„Ç®„É©„Éº\nMemory leak detected „É°„É¢„É™„É™„Éº„ÇØÊ§úÂá∫\nGPU memory transfer failed GPU„É°„É¢„É™Ëª¢ÈÄÅÂ§±Êïó\nModel error „É¢„Éá„É´„Ç®„É©„Éº\nNetwork error „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç®„É©„Éº\nNeural network errors ‚Ä¶\nNeural network operation errors ‚Ä¶\nNode connection failed „Éé„Éº„ÉâÊé•Á∂öÂ§±Êïó\nContains the success value\nOptimization errors ÊúÄÈÅ©Âåñ„Ç®„É©„Éº\nOptimization errors ÊúÄÈÅ©Âåñ„Ç®„É©„Éº\nOptimizer error „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Éº„Ç®„É©„Éº\nGPU out of memory GPU„É°„É¢„É™‰∏çË∂≥\nParameter error „Éë„É©„É°„Éº„Çø„Ç®„É©„Éº\nMemory pool exhausted „É°„É¢„É™„Éó„Éº„É´ÊûØÊ∏á\nProcess group error „Éó„É≠„Çª„Çπ„Ç∞„É´„Éº„Éó„Ç®„É©„Éº\nUnified error type for all RusTorch operations ‚Ä¶\nCommon result type for RusTorch operations (Áµ±‰∏ÄÊ∏à„Åø) ‚Ä¶\nLearning rate scheduler error ‚Ä¶\nTensor shape mismatch error ‚Ä¶\nSynchronization failed ÂêåÊúüÂ§±Êïó\nTensor-specific errors „ÉÜ„É≥„ÇΩ„É´Âõ∫Êúâ„Ç®„É©„Éº\nTensor operation errors „ÉÜ„É≥„ÇΩ„É´Êìç‰Ωú„Ç®„É©„Éº\nTimeout error „Çø„Ç§„É†„Ç¢„Ç¶„Éà„Ç®„É©„Éº\nData transformation error „Éá„Éº„ÇøÂ§âÊèõ„Ç®„É©„Éº\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nActual shape ÂÆüÈöõ„ÅÆÂΩ¢Áä∂\nActual dimensions ÂÆüÈöõ„ÅÆÊ¨°ÂÖÉÊï∞\nExpected shape ÊúüÂæÖ„Åï„Çå„ÇãÂΩ¢Áä∂\nLeft tensor shape Â∑¶„ÉÜ„É≥„ÇΩ„É´„ÅÆÂΩ¢Áä∂\nRequired dimensions ÂøÖË¶Å„Å™Ê¨°ÂÖÉÊï∞\nRight tensor shape Âè≥„ÉÜ„É≥„ÇΩ„É´„ÅÆÂΩ¢Áä∂\nModel architecture parsing and analysis (legacy interface) ‚Ä¶\nModel architecture parsing and analysis (modular structure)\nSimplified PyTorch to RusTorch conversion implementation ‚Ä¶\nCore model parsing logic „Ç≥„Ç¢„É¢„Éá„É´Ëß£Êûê„É≠„Ç∏„ÉÉ„ÇØ\nError types for model parsing ‚Ä¶\nArchitecture description formats and serialization ‚Ä¶\nCore data types for model parsing ‚Ä¶\nModel graph validation functions ‚Ä¶\nPyTorch model parser PyTorch„É¢„Éá„É´„Éë„Éº„Çµ„Éº\nCompute execution order from architecture description ‚Ä¶\nExtract layer information from state dictionary ‚Ä¶\nReturns the argument unchanged.\nInfer layer type from name and parameters ‚Ä¶\nCalls <code>U::from(self)</code>.\nCreate new model parser ‚Ä¶\nParse architecture string as JSON or YAML ‚Ä¶\nParse PyTorch model into model graph ‚Ä¶\nParse parameter name to extract layer name and parameter ‚Ä¶\nParse simple architecture format (e.g., ‚Äúconv2d -&gt; relu ‚Ä¶\nCircular dependency detected Âæ™Áí∞‰æùÂ≠ò„ÇíÊ§úÂá∫\nContains the error value\nType alias for execution result containing layer order and ‚Ä¶\nIncompatible layer dimensions ‚Ä¶\nInvalid architecture format ‚Ä¶\nMissing layer connection ‚Ä¶\nContains the success value\nModel parsing errors „É¢„Éá„É´Ëß£Êûê„Ç®„É©„Éº\nType alias for parsing result containing layer order and ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nFirst layer name\nSecond layer name\nArchitecture description format for explicit model ‚Ä¶\nConnection definition between layers ‚Ä¶\nLayer definition in architecture description ‚Ä¶\nModel metadata information „É¢„Éá„É´„É°„Çø„Éá„Éº„ÇøÊÉÖÂ†±\nConnection type (optional) ‚Ä¶\nLayer connections „É¨„Ç§„É§„ÉºÊé•Á∂ö\nDescription Ë™¨Êòé\nFramework (pytorch, tensorflow, etc.) „Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSource layer name „ÇΩ„Éº„Çπ„É¨„Ç§„É§„ÉºÂêç\nInput shape hint ÂÖ•ÂäõÂΩ¢Áä∂„Éí„É≥„Éà\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLayer type specification „É¨„Ç§„É§„Éº„Çø„Ç§„Éó‰ªïÊßò\nLayer definitions „É¨„Ç§„É§„ÉºÂÆöÁæ©\nModel metadata „É¢„Éá„É´„É°„Çø„Éá„Éº„Çø\nModel name „É¢„Éá„É´Âêç\nLayer name/id „É¨„Ç§„É§„ÉºÂêç/ID\nOutput shape hint Âá∫ÂäõÂΩ¢Áä∂„Éí„É≥„Éà\nLayer parameters „É¨„Ç§„É§„Éº„Éë„É©„É°„Éº„Çø\nTarget layer name „Çø„Éº„Ç≤„ÉÉ„Éà„É¨„Ç§„É§„ÉºÂêç\nModel version „É¢„Éá„É´„Éê„Éº„Ç∏„Éß„É≥\n2D Average Pooling 2DÂπ≥Âùá„Éó„Éº„É™„É≥„Ç∞\n2D Batch Normalization 2D„Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñ\n1D Convolution layer 1DÁï≥„ÅøËæº„Åø„É¨„Ç§„É§„Éº\n2D Convolution layer 2DÁï≥„ÅøËæº„Åø„É¨„Ç§„É§„Éº\n3D Convolution layer 3DÁï≥„ÅøËæº„Åø„É¨„Ç§„É§„Éº\nDropout layer Dropout„É¨„Ç§„É§„Éº\nFlatten layer Flatten„É¨„Ç§„É§„Éº\nLayer information extracted from PyTorch model ‚Ä¶\nSupported layer types ‚Ä¶\nLinear/Dense layer Linear/Dense„É¨„Ç§„É§„Éº\n2D Max Pooling 2DÊúÄÂ§ß„Éó„Éº„É™„É≥„Ç∞\nModel architecture graph ‚Ä¶\nReLU activation ReLUÊ¥ªÊÄßÂåñ\nUnknown layer type ‰∏çÊòé„Å™„É¨„Ç§„É§„Éº„Çø„Ç§„Éó\nLayer connections (from -&gt; to) „É¨„Ç§„É§„ÉºÊé•Á∂öÔºàfrom ‚Ä¶\nLayer execution order „É¨„Ç§„É§„ÉºÂÆüË°åÈ†ÜÂ∫è\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nModel input layers „É¢„Éá„É´ÂÖ•Âäõ„É¨„Ç§„É§„Éº\nInput shape ÂÖ•ÂäõÂΩ¢Áä∂\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLayer type (Linear, Conv2d, etc.) ‚Ä¶\nLayers in the model „É¢„Éá„É´ÂÜÖ„ÅÆ„É¨„Ç§„É§„Éº\nLayer name „É¨„Ç§„É§„ÉºÂêç\nNumber of parameters „Éë„É©„É°„Éº„ÇøÊï∞\nModel output layers „É¢„Éá„É´Âá∫Âäõ„É¨„Ç§„É§„Éº\nOutput shape Âá∫ÂäõÂΩ¢Áä∂\nLayer parameters „É¨„Ç§„É§„Éº„Éë„É©„É°„Éº„Çø\nInput channels\nInput channels\nInput channels\nInput features\nKernel size\nKernel size\nKernel size\nKernel size\nKernel size\nNumber of features\nOutput channels\nOutput channels\nOutput channels\nOutput features\nDropout probability\nPadding\nPadding\nPadding\nStride\nStride\nStride\nStride\nStride\nModel validation functionality\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nValidate model graph for consistency ‚Ä¶\nValidate that all referenced layers exist ‚Ä¶\nInvalid parameter format ÁÑ°Âäπ„Å™„Éë„É©„É°„Éº„ÇøÂΩ¢Âºè\nLayer description for model conversion ‚Ä¶\nMissing parameter „Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ\nSimplified conversion error Á∞°Áï•ÂåñÂ§âÊèõ„Ç®„É©„Éº\nSimplified layer information Á∞°Áï•Âåñ„É¨„Ç§„É§„ÉºÊÉÖÂ†±\nSimplified converter Á∞°Áï•ÂåñÂ§âÊèõÂô®\nSimplified PyTorch model representation ‚Ä¶\nLayer not supported in simplified version ‚Ä¶\nConvert PyTorch model to simplified representation ‚Ä¶\nExecution order ÂÆüË°åÈ†ÜÂ∫è\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet layer by name ÂêçÂâç„Åß„É¨„Ç§„É§„Éº„ÇíÂèñÂæó\nInput tensor shape\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet all layer names ÂÖ®„É¨„Ç§„É§„ÉºÂêç„ÇíÂèñÂæó\nType of layer\nLayer type as string ‚Ä¶\nModel layers „É¢„Éá„É´„É¨„Ç§„É§„Éº\nLayer name\nLayer name „É¨„Ç§„É§„ÉºÂêç\nTotal number of parameters Á∑è„Éë„É©„É°„Éº„ÇøÊï∞\nOutput tensor shape\nParameter shapes „Éë„É©„É°„Éº„ÇøÂΩ¢Áä∂\nPrint model summary „É¢„Éá„É´Ë¶ÅÁ¥Ñ„ÇíË°®Á§∫\nSimulate forward pass (placeholder) ‚Ä¶\nConverted tensors Â§âÊèõ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´\nModel statistics „É¢„Éá„É´Áµ±Ë®à\nDataLoader implementation for Phase 5 - PyTorch-compatible ‚Ä¶\nDataset traits and implementations for Phase 5 ‚Ä¶\nSampling strategies for Phase 5 DataLoader „Éï„Çß„Éº„Ç∫5 ‚Ä¶\nMain DataLoader implementation using Phase 5 Dataset trait ‚Ä¶\nGet batch size „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if exhausted ÊûØÊ∏á„Åó„Åü„Åã„ÉÅ„Çß„ÉÉ„ÇØ\nCreate new DataLoader Êñ∞„Åó„ÅÑDataLoader„Çí‰ΩúÊàê\nGet next batch Ê¨°„ÅÆ„Éê„ÉÉ„ÉÅ„ÇíÂèñÂæó\nGet number of workers „ÉØ„Éº„Ç´„ÉºÊï∞„ÇíÂèñÂæó\nReset the sampler „Çµ„É≥„Éó„É©„Éº„Çí„É™„Çª„ÉÉ„Éà\nCreate DataLoader with all options ‚Ä¶\nAutomatic differentiation error Ëá™ÂãïÂæÆÂàÜ„Ç®„É©„Éº\nBackend not available „Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅåÂà©Áî®‰∏çÂèØ\nConcatenated dataset implementation ‚Ä¶\nData loading error type ‚Ä¶\nData loading error „Éá„Éº„ÇøË™≠„ÅøËæº„Åø„Ç®„É©„Éº\nDataLoader system errors (Phase 5) ‚Ä¶\nCore Dataset trait following PyTorch API (Phase 5 - Main ‚Ä¶\nDebug system and logging errors ‚Ä¶\nDevice operation failed „Éá„Éê„Ç§„ÇπÊìç‰ΩúÂ§±Êïó\nDistributed computing errors ‚Ä¶\nGPU-specific errors GPUÂõ∫Êúâ„Ç®„É©„Éº\nInput/Output and serialization errors ‚Ä¶\nImport-specific error „Ç§„É≥„Éù„Éº„ÉàÂõ∫Êúâ„Ç®„É©„Éº\nInvalid operation ÁÑ°Âäπ„Å™Êìç‰Ωú\nInvalid operation parameters ÁÑ°Âäπ„Å™Êìç‰Ωú„Éë„É©„É°„Éº„Çø\nIterable Dataset trait for streaming data ‚Ä¶\nKernel compilation error for GPU operations ‚Ä¶\nMemory allocation failed „É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶Â§±Êïó\nModel import/export error ‚Ä¶\nNeural network layer error ‚Ä¶\nFeature not implemented yet Ê©üËÉΩÊú™ÂÆüË£Ö\nOut of memory error „É°„É¢„É™‰∏çË∂≥„Ç®„É©„Éº\nProfiling and performance analysis errors ‚Ä¶\nSerialization error „Ç∑„É™„Ç¢„É©„Ç§„Çº„Éº„Ç∑„Éß„É≥„Ç®„É©„Éº\nSerialization and model I/O errors (Phase 9) ‚Ä¶\nShape mismatch between tensors ‚Ä¶\nSimple tensor dataset implementation ‚Ä¶\nTensor operation failed „ÉÜ„É≥„ÇΩ„É´Êìç‰ΩúÂ§±Êïó\nData validation and quality assurance errors ‚Ä¶\nVision processing errors Ë¶ñË¶öÂá¶ÁêÜ„Ç®„É©„Éº\nVisualization errors ÂèØË¶ñÂåñ„Ç®„É©„Éº\nAdd tensor to dataset ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate tensor dataset from features and targets ‚Ä¶\nGet item at index ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if dataset is empty ‚Ä¶\nCreate iterator over dataset ‚Ä¶\nReturns the number of samples in the dataset ‚Ä¶\nCreate new concatenated dataset ‚Ä¶\nCreate new tensor dataset ‚Ä¶\nGet reference to tensors „ÉÜ„É≥„ÇΩ„É´„ÅÆÂèÇÁÖß„ÇíÂèñÂæó\nActual tensor shape that was provided ‚Ä¶\nAvailable memory in bytes ‚Ä¶\nName of the unavailable backend ‚Ä¶\nDevice identifier where the error occurred ‚Ä¶\nDevice where allocation failed ‚Ä¶\nExpected tensor shape ÊúüÂæÖ„Åï„Çå„Çã„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nFeature that is not yet implemented Êú™ÂÆüË£Ö„ÅÆÊ©üËÉΩ\nName of the neural network layer ‚Ä¶\nError message describing the tensor operation failure ‚Ä¶\nError message describing the device issue ‚Ä¶\nDescription of the parameter issue ‚Ä¶\nDetailed error message Ë©≥Á¥∞„Å™„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏\nError message from the layer ‚Ä¶\nAutograd error description Ëá™ÂãïÂæÆÂàÜ„Ç®„É©„Éº„ÅÆË™¨Êòé\nModel I/O error description „É¢„Éá„É´I/O„Ç®„É©„Éº„ÅÆË™¨Êòé\nImport operation error message ‚Ä¶\nData loading error description ‚Ä¶\nGPU operation error description GPUÊìç‰Ωú„Ç®„É©„Éº„ÅÆË™¨Êòé\nVision processing error description ‚Ä¶\nDistributed computing error description ‚Ä¶\nVisualization error description ÂèØË¶ñÂåñ„Ç®„É©„Éº„ÅÆË™¨Êòé\nProfiling error description ‚Ä¶\nData validation error description ‚Ä¶\nDebug system error description ‚Ä¶\nKernel compilation error description ‚Ä¶\nDataLoader error description DataLoader„Ç®„É©„Éº„ÅÆË™¨Êòé\nSerialization error description ‚Ä¶\nSerialization error description ‚Ä¶\nName of the operation with invalid parameters ‚Ä¶\nName of the invalid operation ÁÑ°Âäπ„Å™Êìç‰ΩúÂêç\nOperation that caused the serialization error ‚Ä¶\nRequested memory in bytes ‚Ä¶\nSize of the failed allocation in bytes ‚Ä¶\nOptional underlying error cause ‚Ä¶\nOptional underlying I/O error ‚Ä¶\nOptional underlying import error ‚Ä¶\nOptional underlying data loading error ‚Ä¶\nOptional underlying GPU error ‚Ä¶\nOptional underlying vision error ‚Ä¶\nOptional underlying distributed error ‚Ä¶\nOptional underlying visualization error ‚Ä¶\nBatch sampler - wraps another sampler to yield batches of ‚Ä¶\nRandom sampler - returns shuffled indices ‚Ä¶\nCore Sampler trait following PyTorch API PyTorch ‚Ä¶\nSequential sampler - returns indices in order ‚Ä¶\nSubset random sampler - samples from a subset of indices ‚Ä¶\nWeighted random sampler ‚Ä¶\nGet batch size „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\nCheck if dropping last incomplete batch ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if sampler is exhausted ‚Ä¶\nGet total number of samples Á∑è„Çµ„É≥„Éó„É´Êï∞„ÇíÂèñÂæó\nCreate new batch sampler ‚Ä¶\nCreate sampler for subset of indices ‚Ä¶\nCreate weighted random sampler ‚Ä¶\nCreate new sequential sampler ‚Ä¶\nCreate new random sampler ‚Ä¶\nGet next batch of indices ‚Ä¶\nReset sampler for new epoch ‚Ä¶\nSample next index Ê¨°„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí„Çµ„É≥„Éó„É´\nCreate random sampler with replacement ‚Ä¶\nCreate seeded random sampler for reproducible results ‚Ä¶\nAnalysis Summary\nUnified Debug Framework\nComprehensive Debug Report\nDebug Session Information\nFramework Configuration\nLog Summary Statistics\nRAII Profile Guard for automatic timing\nQuick debug log\nDebug Utilities and System Information\nAnalysis configuration\nMemory tracking configuration\nProfiling configuration\nQuick error log\nFlush all pending data\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet comprehensive debug report\nGet current session information\nQuick info log\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLog structured message with metadata\nLog Analysis and Pattern Detection System\nLogging configuration\nStructured Logging System\nMemory Allocation Tracking System\nCreate new debug framework with default configuration\nCreate profiling guard for automatic timing\nPerformance Profiling System\nTrack memory allocation\nTrack memory deallocation\nQuick warning log\nCreate debug framework with custom configuration\nDebug utilities collection\nDiagnostic context for debugging\nMemory usage snapshot\nPerformance measurement helper\nStack trace information (simplified)\nSystem information for debugging\nAdd parameter\nCapture current stack trace (simplified implementation)\nCapture stack trace\nCapture stack trace\nAdd checkpoint\nCollect current system information\nCreate diagnostic context for operation\nSimple assertion with diagnostic context\nConditional debugging output\nGet current elapsed time\nFinish timing and get total duration\nFormat bytes for human reading\nFormat duration for human reading\nFormat system info as string\nFormat memory snapshot\nFormat stack trace as string\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate comprehensive diagnostic report\nGenerate environment report\nGenerate timing report\nCollect system information\nGet current thread information\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if running in debug mode\nCreate new diagnostic context\nSet memory snapshot\nStart new performance timer\nStart performance measurement\nTake memory snapshot (simplified)\nTake memory snapshot\nTime a code block and return result with timing\nCalculate memory utilization percentage\nAlert notification\nAlert rule for automated notifications\nLog analyzer with pattern detection and alerting\nLog pattern for detection\nPattern detection result\nAdd alert rule\nAdd log pattern\nAnalyze log entry for patterns\nCheck if rule can be triggered (not in cooldown)\nClear analysis data\nEnable/disable alert rule\nEnable/disable pattern\nExtract capture groups from log message\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate analysis report\nGet analysis summary\nGet matches for specific pattern\nGet recent pattern matches\nGet total alerts count\nGet all triggered alerts\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if analyzer is enabled\nMark rule as triggered\nCheck if log entry matches this pattern\nCreate new log analyzer\nCreate new log pattern\nCreate new alert rule\nEnable/disable analyzer\nLog entry with structured metadata\nLog severity levels\nLog output configuration\nCore logging system\nGet ANSI color code for console output\nGet emoji representation\nFilter entries by level\nFilter entries by metadata key-value pair\nFlush all pending writes\nFormat as colored console output\nFormat as human-readable string\nFormat as JSON\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nConvert from string representation\nGet log summary statistics\nGet total log count\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLog structured entry\nCreate new logger\nCreate new log entry\nSearch entries by message content\nCreate logger with specific output configuration\nMemory allocation information\nMemory usage statistics by component\nComprehensive memory report\nMemory allocation tracker\nAge of allocation\nClear all tracking data\nSet leak detection parameters\nCurrent usage in MB\nDetect potential memory leaks\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate comprehensive memory report\nGenerate memory usage summary\nGet current number of active allocations\nGet memory statistics for specific component\nGet current memory usage in MB\nGet large allocations\nGet peak memory usage in MB\nGet all tracked components\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if memory tracking is enabled\nCheck if usage exceeds threshold\nCreate new memory tracker\nCreate new allocation info\nPeak usage in MB\nEnable/disable memory tracking\nSet large allocation threshold\nSize in megabytes\nTrack memory allocation\nTrack memory deallocation\nPerformance profiler for operation timing\nStatistics for a specific operation type\nAggregated performance metrics\nPerformance profile entry\nRAII profiling scope guard\nAdd metadata to the profile\nClear all recorded data\nDuration in milliseconds\nDuration in microseconds\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate performance report\nGet entries for specific operation\nGet all operation names\nGet statistics for specific operation\nGet comprehensive performance metrics\nGet recent entries (last n entries)\nGet total number of recorded operations\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if profiling is enabled\nCreate new profiler\nCreate new profiling scope\nCreate new profile entry\nRecord operation timing\nRecord operation with metadata\nEnable/disable profiling\nCreate profiling scope with metadata\nCreate with metadata\nAverage reduction Âπ≥Âùá„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nDistributed backend types ÂàÜÊï£„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Çø„Ç§„Éó\nCommon trait for distributed data parallel implementations ‚Ä¶\nCommunication operations for distributed training ‚Ä¶\nType alias for distributed-compatible float types ‚Ä¶\nDistributed state management ÂàÜÊï£Áä∂ÊÖãÁÆ°ÁêÜ\nFacebook‚Äôs collective communications library ‚Ä¶\nMessage Passing Interface ‚Ä¶\nMaximum reduction ÊúÄÂ§ßÂÄ§„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nMinimum reduction ÊúÄÂ∞èÂÄ§„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nNVIDIA Collective Communications Library ‚Ä¶\nProcess group for distributed training ‚Ä¶\nProduct reduction Á©ç„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nReduction operations for collective communications ‚Ä¶\nSum reduction ÂêàË®à„É™„ÉÄ„ÇØ„Ç∑„Éß„É≥\nCustom TCP backend „Ç´„Çπ„Çø„É†TCP„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ\nAll-gather operation across all processes ‚Ä¶\nAll-reduce operation across all processes ‚Ä¶\nData parallel training module ‚Ä¶\nAsynchronous gradient synchronization system for ‚Ä¶\nBackend used for communication ‚Ä¶\nCommunication backends for distributed training ‚Ä¶\nBroadcast operation from root process ‚Ä¶\nMulti-machine cluster support for distributed training ‚Ä¶\nCommon utilities for distributed operations ‚Ä¶\nData parallel training implementation ‚Ä¶\nDistributedDataParallel (DDP) implementation for RusTorch ‚Ä¶\nGet device IDs for this DDP instance ‚Ä¶\nDevice mapping for each rank ‚Ä¶\nAvailable devices for distributed training ‚Ä¶\nPerform distributed forward pass ‚Ä¶\nFinalize distributed training ÂàÜÊï£Â≠¶Áøí„ÇíÁµÇ‰∫Ü\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather operation to root process ‚Ä¶\nGet global distributed state ‚Ä¶\nGet current rank in distributed training ‚Ä¶\nGet world size in distributed training ‚Ä¶\nInitialize the process group ‚Ä¶\nInitialize distributed training ÂàÜÊï£Â≠¶Áøí„ÇíÂàùÊúüÂåñ\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if distributed training is available ‚Ä¶\nCheck if distributed training is initialized ‚Ä¶\nMaster address for coordination ‚Ä¶\nMaster port for coordination Ë™øÊï¥Áî®„Éû„Çπ„Çø„Éº„Éù„Éº„Éà\nModel parallel training implementation ‚Ä¶\nMulti-GPU validation and benchmarking for distributed ‚Ä¶\nAdvanced NCCL integration for high-performance distributed ‚Ä¶\nCreate a new process group ‚Ä¶\nCreate new distributed state Êñ∞„Åó„ÅÑÂàÜÊï£Áä∂ÊÖã„Çí‰ΩúÊàê\nDistributed optimizers for distributed training ‚Ä¶\nPerformance optimizations for distributed learning ‚Ä¶\nCurrent process group ÁèæÂú®„ÅÆ„Éó„É≠„Çª„Çπ„Ç∞„É´„Éº„Éó\nGet current rank ÁèæÂú®„ÅÆ„É©„É≥„ÇØ„ÇíÂèñÂæó\nRank of current process ÁèæÂú®„ÅÆ„Éó„É≠„Çª„Çπ„ÅÆ„É©„É≥„ÇØ\nReduce operation to root process ‚Ä¶\nScatter operation from root process ‚Ä¶\nSet process group „Éó„É≠„Çª„Çπ„Ç∞„É´„Éº„Éó„ÇíË®≠ÂÆö\nSimplified DistributedDataParallel implementation ‚Ä¶\nSynchronize gradients across processes ‚Ä¶\nGet world size „ÉØ„Éº„É´„Éâ„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\nTotal number of processes „Éó„É≠„Çª„ÇπÁ∑èÊï∞\nAsynchronous distributed request handle ‚Ä¶\nProcess group management „Éó„É≠„Çª„Çπ„Ç∞„É´„Éº„ÉóÁÆ°ÁêÜ\nAll-gather operation All-gatherÊìç‰Ωú\nAll-reduce operation All-reduceÊìç‰Ωú\nBarrier synchronization „Éê„É™„Ç¢ÂêåÊúü\nBroadcast operation „Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„ÉàÊìç‰Ωú\nDestroy the distributed process group ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather operation GatherÊìç‰Ωú\nGet the rank of the current process ‚Ä¶\nGet the world size of the current process group ‚Ä¶\nInitialize distributed training process group ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if operation is completed ‚Ä¶\nCheck if the current process is initialized for ‚Ä¶\nMonitoring and debugging utilities ‚Ä¶\nCreate a new process group ‚Ä¶\nCreate a new distributed group ‚Ä¶\nGet the rank of this process in the group ‚Ä¶\nReduce operation ReduceÊìç‰Ωú\nScatter operation ScatterÊìç‰Ωú\nGet the size of this group ‚Ä¶\nWait for the operation to complete Êìç‰ΩúÂÆå‰∫Ü„ÇíÂæÖÊ©ü\nCommunication statistics structure ÈÄö‰ø°Áµ±Ë®àÊßãÈÄ†‰Ωì\nReturns the argument unchanged.\nGet communication statistics ÈÄö‰ø°Áµ±Ë®à„ÇíÂèñÂæó\nCalls <code>U::from(self)</code>.\nConfiguration for asynchronous gradient synchronization ‚Ä¶\nAsynchronous gradient synchronization context ‚Ä¶\nAsynchronous gradient synchronization coordinator ‚Ä¶\nIndividual gradient bucket ÂÄãÂà•ÂãæÈÖç„Éê„Ç±„ÉÉ„Éà\nGradient bucket manager for efficient communication ‚Ä¶\nGradient synchronization completion notification ‚Ä¶\nGradient synchronization request ‚Ä¶\nPriority levels for gradient synchronization ‚Ä¶\nAdd gradient to appropriate bucket ‚Ä¶\nBucket size (MB) „Éê„Ç±„ÉÉ„Éà„Çµ„Ç§„Ç∫ÔºàMBÔºâ\nCheck for completed synchronizations ‚Ä¶\nGradient compression utilities ‚Ä¶\nCompression threshold (bytes) ÂúßÁ∏ÆÈñæÂÄ§Ôºà„Éê„Ç§„ÉàÔºâ\nSynchronization duration ÂêåÊúüÊôÇÈñì\nEnable gradient bucketing ÂãæÈÖç„Éê„Ç±„ÉÉ„ÉàÂåñ„ÇíÊúâÂäπÂåñ\nEnable gradient compression ÂãæÈÖçÂúßÁ∏Æ„ÇíÊúâÂäπÂåñ\nError message if failed ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet buckets ready for synchronization ‚Ä¶\nGradient tensor to synchronize ‚Ä¶\nCombined gradient tensors ÁµêÂêàÂãæÈÖç„ÉÜ„É≥„ÇΩ„É´\nRequest ID for tracking ËøΩË∑°Áî®„É™„ÇØ„Ç®„Çπ„ÉàID\nBucket ID „Éê„Ç±„ÉÉ„ÉàID\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLast update timestamp ÊúÄÁµÇÊõ¥Êñ∞„Çø„Ç§„É†„Çπ„Çø„É≥„Éó\nMark bucket as synchronized and clear ‚Ä¶\nMaximum number of concurrent operations ‚Ä¶\nCreate a new asynchronous gradient synchronizer ‚Ä¶\nCreate new gradient bucket manager ‚Ä¶\nCreate new async gradient context ‚Ä¶\nParameter name „Éë„É©„É°„Éº„ÇøÂêç\nParameters in this bucket ‚Ä¶\nPriority level ÂÑ™ÂÖàÂ∫¶„É¨„Éô„É´\nReady for synchronization ÂêåÊúüÊ∫ñÂÇôÂÆå‰∫Ü\nReduction operation „É™„ÉÄ„ÇØ„Ç∑„Éß„É≥Êìç‰Ωú\nRequest ID that completed ÂÆå‰∫Ü„Åó„Åü„É™„ÇØ„Ç®„Çπ„ÉàID\nSubmit gradient for asynchronous synchronization ‚Ä¶\nSuccess status ÊàêÂäü„Çπ„ÉÜ„Éº„Çø„Çπ\nSynchronize all pending gradients ‚Ä¶\nSubmit parameter gradient for async sync ‚Ä¶\nTimeout for gradient synchronization ‚Ä¶\nWait for all pending operations to complete ‚Ä¶\nTotal size in bytes Á∑è„Çµ„Ç§„Ç∫Ôºà„Éê„Ç§„ÉàÔºâ\nWait for specific synchronization to complete ‚Ä¶\nCompressed gradient representation ÂúßÁ∏ÆÂãæÈÖçË°®Áèæ\nGradient compression algorithms ‚Ä¶\nCompression metadata ÂúßÁ∏Æ„É°„Çø„Éá„Éº„Çø\nError feedback compression ‚Ä¶\nNo compression ÂúßÁ∏Æ„Å™„Åó\nQuantization-based compression ÈáèÂ≠êÂåñ„Éô„Éº„ÇπÂúßÁ∏Æ\nTop-K sparsification Top-K„Çπ„Éë„Éº„ÇπÂåñ\nCompress gradient tensor ÂãæÈÖç„ÉÜ„É≥„ÇΩ„É´„ÇíÂúßÁ∏Æ\nCompressed data ÂúßÁ∏Æ„Éá„Éº„Çø\nDecompress gradient tensor ÂãæÈÖç„ÉÜ„É≥„ÇΩ„É´„ÇíÂ±ïÈñã\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCompression metadata ÂúßÁ∏Æ„É°„Çø„Éá„Éº„Çø\nOriginal tensor shape ÂÖÉ„ÅÆ„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nBackend factory „Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Éï„Ç°„ÇØ„Éà„É™\nGloo backend for CPU and GPU communication ‚Ä¶\nGloo communication context GlooÈÄö‰ø°„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà\nGloo transport options GlooËª¢ÈÄÅ„Ç™„Éó„Ç∑„Éß„É≥\nInfiniBand transport for high-performance clusters ‚Ä¶\nShared memory transport for single-node communication ‚Ä¶\nTCP transport for network communication ‚Ä¶\nTCP backend for simple distributed training ‚Ä¶\nTCP connection to remote process ‚Ä¶\nCreate backend instance based on process group ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new Gloo backend instance ‚Ä¶\nCreate new TCP backend instance ‚Ä¶\nNode is available „Éé„Éº„Éâ„ÅåÂà©Áî®ÂèØËÉΩ\nBest-fit scheduling ‚Ä¶\nNode is busy „Éé„Éº„Éâ„Åå„Éì„Ç∏„Éº\nCluster configuration for multi-machine training ‚Ä¶\nCluster manager for coordinating distributed training ‚Ä¶\nCluster status information ‚Ä¶\nCluster topology types ‚Ä¶\nCustom topology with explicit connections ‚Ä¶\nDistributed backend types ÂàÜÊï£„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Çø„Ç§„Éó\nNode has failed „Éé„Éº„Éâ„ÅåÊïÖÈöú\nFault tolerance configuration ÈöúÂÆ≥ËÄêÊÄßË®≠ÂÆö\nFirst-fit scheduling ‚Ä¶\nFlat topology - all nodes communicate directly ‚Ä¶\nFacebook Gloo collective communications Facebook ‚Ä¶\nHeartbeat monitor for node health checking ‚Ä¶\nLoad balancing Ë≤†Ëç∑ÂàÜÊï£\nLocality-aware scheduling ‚Ä¶\nMessage Passing Interface ‚Ä¶\nNVIDIA Collective Communications Library ‚Ä¶\nNode capabilities „Éé„Éº„ÉâÊ©üËÉΩ\nNode information in the cluster ‚Ä¶\nNode status „Éé„Éº„Éâ„Çπ„ÉÜ„Éº„Çø„Çπ\nNode is offline „Éé„Éº„Éâ„Åå„Ç™„Éï„É©„Ç§„É≥\nResource scheduler for optimal job placement ‚Ä¶\nCurrent resource usage on a node ‚Ä¶\nRing topology - ring-based communication ‚Ä¶\nScheduling strategies „Çπ„Ç±„Ç∏„É•„Éº„É™„É≥„Ç∞Êà¶Áï•\nTree topology - hierarchical communication ‚Ä¶\nActive jobs „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç∏„Éß„ÉñÊï∞\nActive jobs „Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Ç∏„Éß„ÉñÊï∞\nNode address „Éé„Éº„Éâ„Ç¢„Éâ„É¨„Çπ\nAvailable nodes Âà©Áî®ÂèØËÉΩ„Éé„Éº„ÉâÊï∞\nBusy nodes „Éì„Ç∏„Éº„Éé„Éº„ÉâÊï∞\nNode capabilities „Éé„Éº„ÉâÊ©üËÉΩ\nCheckpoint frequency for recovery ‚Ä¶\nCPU cores CPU„Ç≥„Ç¢Êï∞\nUsed CPU cores ‰ΩøÁî®CPU„Ç≥„Ç¢Êï∞\nCreate process group for distributed training ‚Ä¶\nEnable automatic failover ‚Ä¶\nFailed nodes ÊïÖÈöú„Éé„Éº„ÉâÊï∞\nFault tolerance settings ÈöúÂÆ≥ËÄêÊÄßË®≠ÂÆö\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cluster status „ÇØ„É©„Çπ„Çø„Éº„Çπ„ÉÜ„Éº„Çø„Çπ„ÇíÂèñÂæó\nNumber of GPUs on this node „Åì„ÅÆ„Éé„Éº„Éâ‰∏ä„ÅÆGPUÊï∞\nGPU memory per device (GB) ‚Ä¶\nUsed GPU memory (GB) ‰ΩøÁî®GPU„É°„É¢„É™ÔºàGBÔºâ\nHandle node failure „Éé„Éº„ÉâÈöúÂÆ≥„ÇíÂá¶ÁêÜ\nHeartbeat interval (seconds) ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaster node address „Éû„Çπ„Çø„Éº„Éé„Éº„Éâ„Ç¢„Éâ„É¨„Çπ\nMaster node port „Éû„Çπ„Çø„Éº„Éé„Éº„Éâ„Éù„Éº„Éà\nMaximum retry attempts ÊúÄÂ§ßÂÜçË©¶Ë°åÂõûÊï∞\nAvailable memory (GB) Âà©Áî®ÂèØËÉΩ„É°„É¢„É™ÔºàGBÔºâ\nUsed memory (GB) ‰ΩøÁî®„É°„É¢„É™ÔºàGBÔºâ\nNetwork bandwidth (Gbps) ‚Ä¶\nCreate new cluster manager ‚Ä¶\nCreate new heartbeat monitor ‚Ä¶\nCreate new resource scheduler ‚Ä¶\nNode ID „Éé„Éº„ÉâID\nNode timeout (seconds) „Éé„Éº„Éâ„Çø„Ç§„É†„Ç¢„Ç¶„ÉàÔºàÁßíÔºâ\nNode port „Éé„Éº„Éâ„Éù„Éº„Éà\nSchedule job on available nodes ‚Ä¶\nStart cluster services „ÇØ„É©„Çπ„Çø„Éº„Çµ„Éº„Éì„Çπ„ÇíÈñãÂßã\nStart resource monitoring ‚Ä¶\nNode status „Éé„Éº„Éâ„Çπ„ÉÜ„Éº„Çø„Çπ\nStop cluster services „ÇØ„É©„Çπ„Çø„Éº„Çµ„Éº„Éì„Çπ„ÇíÂÅúÊ≠¢\nStop monitoring „É¢„Éã„Çø„É™„É≥„Ç∞„ÇíÂÅúÊ≠¢\nStop resource monitoring ‚Ä¶\nCluster topology „ÇØ„É©„Çπ„Çø„Éº„Éà„Éù„É≠„Ç∏„Éº\nTotal GPUs Á∑èGPUÊï∞\nTotal number of nodes Á∑è„Éé„Éº„ÉâÊï∞\nUpdate heartbeat for node ‚Ä¶\nList of worker nodes „ÉØ„Éº„Ç´„Éº„Éé„Éº„Éâ„É™„Çπ„Éà\nMap from node ID to list of connected node IDs ‚Ä¶\nMaximum depth of the tree „ÉÑ„É™„Éº„ÅÆÊúÄÂ§ßÊ∑±Â∫¶\nTrait for backend-specific optimizations ‚Ä¶\nCommon distributed operation implementations ‚Ä¶\nDefault all_gather implementation for backends ‚Ä¶\nDefault all_reduce implementation for backends ‚Ä¶\nDefault broadcast implementation for backends ‚Ä¶\nDefault gather implementation for backends ‚Ä¶\nEnable gradient compression ÂãæÈÖçÂúßÁ∏Æ„ÇíÊúâÂäπÂåñ\nEnable pipeline parallelism for large tensors ‚Ä¶\nEnable zero-copy optimizations ‚Ä¶\nReturns the argument unchanged.\nGet optimal chunk size for communication ‚Ä¶\nCalls <code>U::from(self)</code>.\nGet memory pool size for tensor operations ‚Ä¶\nGet optimal bucket size for gradient bucketing ‚Ä¶\nGet optimal number of communication streams ‚Ä¶\nOptimize tensor for communication ‚Ä¶\nCheck if backend supports async operations ‚Ä¶\nCreate error for unsupported operations ‚Ä¶\nValidate rank for distributed operations ‚Ä¶\nValidate tensor for distributed operations ‚Ä¶\nValidate tensor shapes match across processes ‚Ä¶\nAsynchronous gradient updates ÈùûÂêåÊúüÂãæÈÖçÊõ¥Êñ∞\nData parallel wrapper for models ‚Ä¶\nIterator for distributed data loading ‚Ä¶\nDistributed data loader for data parallel training ‚Ä¶\nDistributed sampler for ensuring each process gets ‚Ä¶\nGradient synchronization strategies ÂãæÈÖçÂêåÊúüÊà¶Áï•\nLocal SGD with periodic synchronization ‚Ä¶\nSynchronize gradients after each backward pass ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather outputs from devices ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the dataloader has no data\nReturns true if there are no samples\nGet batch iterator „Éê„ÉÉ„ÉÅ„Ç§„ÉÜ„É¨„Éº„Çø„ÇíÂèñÂæó\nGet number of batches per epoch ‚Ä¶\nGet number of samples for current process ‚Ä¶\nCreate a new distributed data loader ‚Ä¶\nCreate a new distributed sampler ‚Ä¶\nCreate a new data parallel wrapper ‚Ä¶\nGet number of devices „Éá„Éê„Ç§„ÇπÊï∞„ÇíÂèñÂæó\nReplicate input across devices ‚Ä¶\nGenerate sample indices for current process ‚Ä¶\nSet epoch for deterministic shuffling ‚Ä¶\nSet epoch for deterministic sampling ‚Ä¶\nSet gradient synchronization strategy ‚Ä¶\nSynchronize gradients across devices ‚Ä¶\nFrequency of synchronization in steps ‚Ä¶\nDistributedDataParallel wrapper for PyTorch compatibility ‚Ä¶\nGet device IDs „Éá„Éê„Ç§„ÇπID„ÇíÂèñÂæó\nPerform forward pass with distributed synchronization ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if this is a DDP module ‚Ä¶\nGet the wrapped module ‚Ä¶\nCreate a new DistributedDataParallel wrapper ‚Ä¶\nSynchronize gradients across all processes ‚Ä¶\nConvenience function to wrap a module in DDP ‚Ä¶\nAll-reduce operation „Ç™„Éº„É´„É™„Éá„É•„Éº„ÇπÊìç‰Ωú\nAll-to-all communication „Ç™„Éº„É´„ÉÑ„Éº„Ç™„Éº„É´ÈÄö‰ø°\nBroadcast operation „Éñ„É≠„Éº„Éâ„Ç≠„É£„Çπ„ÉàÊìç‰Ωú\nCommunication operations between model partitions ‚Ä¶\nTypes of communication between partitions ‚Ä¶\nExpert parallel for mixture of experts models ‚Ä¶\nMemory usage statistics „É°„É¢„É™‰ΩøÁî®Áµ±Ë®à\nModel parallel wrapper for splitting models across devices ‚Ä¶\nPoint-to-point send/receive ‚Ä¶\nPipeline parallel configuration ‚Ä¶\nTensor parallel operations for splitting tensors across ‚Ä¶\nAll-reduce tensor across partitions ‚Ä¶\nCurrently allocated bytes ‚Ä¶\nBalance load across partitions ‚Ä¶\nCached bytes „Ç≠„É£„ÉÉ„Ç∑„É•„Åï„Çå„Åü„Éê„Ç§„ÉàÊï∞\nDestination partition index ‚Ä¶\nEnable pipeline parallelism ‚Ä¶\nExecute forward pass with model parallelism ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather tensors from all partitions ‚Ä¶\nGet expert assignment for current device ‚Ä¶\nGradient accumulation steps ÂãæÈÖçÁ¥ØÁ©ç„Çπ„ÉÜ„ÉÉ„Éó\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet memory usage statistics for each partition ‚Ä¶\nCreate new tensor parallel context ‚Ä¶\nCreate new expert parallel context ‚Ä¶\nCreate a new model parallel wrapper ‚Ä¶\nNumber of micro-batches „Éû„Ç§„ÇØ„É≠„Éê„ÉÉ„ÉÅÊï∞\nPipeline stages „Éë„Ç§„Éó„É©„Ç§„É≥„Çπ„ÉÜ„Éº„Ç∏\nCommunication type ÈÄö‰ø°„Çø„Ç§„Éó\nPeak allocated bytes „Éî„Éº„ÇØÂâ≤„ÇäÂΩì„Å¶„Éê„Ç§„ÉàÊï∞\nRoute tokens to appropriate experts ‚Ä¶\nSource partition index ‚Ä¶\nSplit tensor along parallel dimension ‚Ä¶\nTensor shape for communication ÈÄö‰ø°Áî®„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nWhether to use 1F1B schedule ‚Ä¶\nPerformance benchmark results ‚Ä¶\nGPU device information GPU„Éá„Éê„Ç§„ÇπÊÉÖÂ†±\nMemory usage statistics „É°„É¢„É™‰ΩøÁî®Áµ±Ë®à\nMulti-GPU validator „Éû„É´„ÉÅGPU„Éê„É™„Éá„Éº„Çø\nProcess group for distributed operations ‚Ä¶\nMulti-GPU validation metrics ‚Ä¶\nTotal accuracy Á∑èÁ≤æÂ∫¶\nAvailable memory in bytes ‚Ä¶\nBackend type for distributed communication ‚Ä¶\nRun performance benchmark ‚Ä¶\nClear metrics history „É°„Éà„É™„ÇØ„ÇπÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢\nCommunication overhead percentage ‚Ä¶\nCommunication time ÈÄö‰ø°ÊôÇÈñì\nCompute capability (for CUDA) Ë®àÁÆóËÉΩÂäõÔºàCUDAÁî®Ôºâ\nCurrent usage ÁèæÂú®„ÅÆ‰ΩøÁî®Èáè\nPer-device accuracies „Éá„Éê„Ç§„Çπ„Åî„Å®„ÅÆÁ≤æÂ∫¶\nDevice ID „Éá„Éê„Ç§„ÇπID\nPer-device losses „Éá„Éê„Ç§„Çπ„Åî„Å®„ÅÆÊêçÂ§±\nValidation time per device ‚Ä¶\nDevice type „Éá„Éê„Ç§„Çπ„Çø„Ç§„Éó\nMemory fragmentation percentage ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cached benchmark results ‚Ä¶\nGet device information „Éá„Éê„Ç§„ÇπÊÉÖÂ†±„ÇíÂèñÂæó\nGet validation history Ê§úË®ºÂ±•Ê≠¥„ÇíÂèñÂæó\nInitialize multi-GPU environment ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nIs device available for training ‚Ä¶\nMemory usage per device ‚Ä¶\nMulti-GPU throughput „Éû„É´„ÉÅGPU„Çπ„É´„Éº„Éó„ÉÉ„Éà\nDevice name „Éá„Éê„Ç§„ÇπÂêç\nCreate a new multi-GPU validator ‚Ä¶\nOptimal batch size per GPU ‚Ä¶\nPeak memory usage „Éî„Éº„ÇØ„É°„É¢„É™‰ΩøÁî®Èáè\nProcess rank in the distributed group ‚Ä¶\nScaling efficiency (multi/single) ‚Ä¶\nSingle GPU baseline performance ‚Ä¶\nSamples processed per second ‚Ä¶\nTotal validation loss Á∑èÊ§úË®ºÊêçÂ§±\nTotal memory in bytes Á∑è„É°„É¢„É™Ôºà„Éê„Ç§„ÉàÔºâ\nTotal validation time Á∑èÊ§úË®ºÊôÇÈñì\nValidate model across multiple GPUs ‚Ä¶\nTotal number of processes in the group ‚Ä¶\nTiming statistics „Çø„Ç§„Éü„É≥„Ç∞Áµ±Ë®à\nFallback implementations when NCCL is not available ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAdam optimizer Adam„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Éº\nAsynchronous gradient updates ÈùûÂêåÊúüÂãæÈÖçÊõ¥Êñ∞\nAverage all values across processes ‚Ä¶\nGradient compression for bandwidth efficiency ‚Ä¶\nDistributed optimizer wrapper ‚Ä¶\nDistributed optimizer builder for easy configuration ‚Ä¶\nGradient bucket for batching communications ‚Ä¶\nGradient synchronization strategies ÂãæÈÖçÂêåÊúüÊà¶Áï•\nHierarchical all-reduce for large clusters ‚Ä¶\nLocal SGD with periodic synchronization ‚Ä¶\nTake maximum value across processes ‚Ä¶\nTake minimum value across processes ‚Ä¶\nOptimizer types for builder pattern ‚Ä¶\nReduction operation types „É™„ÉÄ„ÇØ„Ç∑„Éß„É≥Êìç‰Ωú„Çø„Ç§„Éó\nStochastic Gradient Descent optimizer ‚Ä¶\nSum all values across processes ‚Ä¶\nSynchronous all-reduce after each backward pass ‚Ä¶\nCreate distributed Adam optimizer ‚Ä¶\nCreate new builder with Adam ‚Ä¶\nAdd tensor to gradient bucket ‚Ä¶\nSet communication backend ÈÄö‰ø°„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÇíË®≠ÂÆö\nSet gradient bucket size ‚Ä¶\nBuild the distributed optimizer ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitialize gradient buckets for efficient communication ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new distributed optimizer ‚Ä¶\nReset step counter ‚Ä¶\nSet communication frequency for local SGD ‚Ä¶\nCreate distributed SGD optimizer ‚Ä¶\nCreate new builder with SGD ‚Ä¶\nGet current step count ÁèæÂú®„ÅÆ„Çπ„ÉÜ„ÉÉ„ÉóÊï∞„ÇíÂèñÂæó\nSynchronize gradients across all processes ‚Ä¶\nSet gradient synchronization strategy ‚Ä¶\nCompression ratio (0.0 to 1.0) ÂúßÁ∏ÆÁéáÔºà0.0„Åã„Çâ1.0Ôºâ\nFrequency of synchronization in steps ‚Ä¶\nBeta1 parameter Beta1„Éë„É©„É°„Éº„Çø\nBeta2 parameter Beta2„Éë„É©„É°„Éº„Çø\nEpsilon for numerical stability ‚Ä¶\nLearning rate for optimization ÊúÄÈÅ©Âåñ„ÅÆÂ≠¶ÁøíÁéá\nLearning rate for optimization ÊúÄÈÅ©Âåñ„ÅÆÂ≠¶ÁøíÁéá\nMomentum factor „É¢„É°„É≥„Çø„É†‰øÇÊï∞\nWeight decay for L2 regularization ‚Ä¶\nWeight decay for L2 regularization ‚Ä¶\nAll-gather operation (gather data from all processes to ‚Ä¶\nAll-reduce operation (combine values from all processes) ‚Ä¶\nBroadcast operation (send data from one process to all) ‚Ä¶\nCommunication scheduler for batching operations ‚Ä¶\nCompressed gradient representation ÂúßÁ∏ÆÂãæÈÖçË°®Áèæ\nGradient compression algorithms ‚Ä¶\nGather operation (collect data from all processes to one) ‚Ä¶\nGradient compressor for reducing communication overhead ‚Ä¶\nMemory pool statistics „É°„É¢„É™„Éó„Éº„É´Áµ±Ë®à\nNo compression ÂúßÁ∏Æ„Å™„Åó\nOperation metadata Êìç‰Ωú„É°„Çø„Éá„Éº„Çø\nType of communication operation ÈÄö‰ø°Êìç‰Ωú„ÅÆÁ®ÆÈ°û\nPending communication operation ‰øùÁïô‰∏≠„ÅÆÈÄö‰ø°Êìç‰Ωú\nQuantization ÈáèÂ≠êÂåñ\nRandom sparsification „É©„É≥„ÉÄ„É†„Çπ„Éë„Éº„ÇπÂåñ\nReduce operation (combine values to one process) ‚Ä¶\nScatter operation (distribute data from one process to all)\nMemory pool for efficient tensor allocation ‚Ä¶\nTop-K sparsification Top-K „Çπ„Éë„Éº„ÇπÂåñ\nCompression algorithm used ‚Ä¶\nClear all pools ÂÖ®„Éó„Éº„É´„Çí„ÇØ„É™„Ç¢\nCompress gradient tensor ÂãæÈÖç„ÉÜ„É≥„ÇΩ„É´„ÇíÂúßÁ∏Æ\nCompressed gradient data ÂúßÁ∏Æ„Åï„Çå„ÅüÂãæÈÖç„Éá„Éº„Çø\nDecompress gradient tensor ÂãæÈÖç„ÉÜ„É≥„ÇΩ„É´„ÇíÂ±ïÈñã\nForce execution of pending operations ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet memory pool statistics ‚Ä¶\nGet tensor from pool or allocate new one ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum pool size allowed ‚Ä¶\nOperation metadata Êìç‰Ωú„É°„Çø„Éá„Éº„Çø\nCreate new gradient compressor ‚Ä¶\nCreate new memory pool Êñ∞„Åó„ÅÑ„É°„É¢„É™„Éó„Éº„É´„Çí‰ΩúÊàê\nCreate new communication scheduler ‚Ä¶\nType of the communication operation ‚Ä¶\nOriginal tensor shape before compression ‚Ä¶\nPriority of the operation (0-255, higher values = higher ‚Ä¶\nReturn tensor to pool „ÉÜ„É≥„ÇΩ„É´„Çí„Éó„Éº„É´„Å´ËøîÂç¥\nRoot rank for operations that require a root process ‚Ä¶\nSchedule operation for batched execution ‚Ä¶\nTensor data for the operation ‚Ä¶\nTimestamp when the operation was created ‚Ä¶\nTotal number of tensors in the pool ‚Ä¶\nNumber of unique tensor shapes ‚Ä¶\nNumber of bits for quantization ÈáèÂ≠êÂåñ„ÅÆ„Éì„ÉÉ„ÉàÊï∞\nNumber of top elements to keep ‰øùÊåÅ„Åô„Çã‰∏ä‰ΩçË¶ÅÁ¥†Êï∞\nCompression ratio (0.0 to 1.0) ÂúßÁ∏ÆÁéáÔºà0.0„Åã„Çâ1.0Ôºâ\nSimplified DistributedDataParallel for RusTorch ‚Ä¶\nGet device IDs „Éá„Éê„Ç§„ÇπID„ÇíÂèñÂæó\nForward pass with distributed synchronization ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nGet the wrapped module ‚Ä¶\nCreate a new simplified DDP wrapper ‚Ä¶\nEnable or disable automatic gradient synchronization ‚Ä¶\nSynchronize gradients across all processes ‚Ä¶\nConvenience function to wrap a module in simplified DDP ‚Ä¶\nDistribution trait for all statistical distributions ‚Ä¶\nValidation utilities for distributions ‚Ä¶\nBernoulli distribution implementation\nBeta distribution implementation\nCategorical distribution implementation\nCompute cumulative distribution function ‚Ä¶\nStatistical Distributions - torch.distributions.* ‚Ä¶\nGet distribution entropy ‚Ä¶\nExponential distribution implementation\nReturns the argument unchanged.\nGamma distribution implementation\nCompute inverse cumulative distribution function (quantile ‚Ä¶\nCalls <code>U::from(self)</code>.\nCompute log probability density/mass function ‚Ä¶\nGet distribution mean ÂàÜÂ∏É„ÅÆÂπ≥Âùá„ÇíÂèñÂæó\nNormal (Gaussian) distribution implementation\nCompute probability density/mass function ‚Ä¶\nGenerate random normal samples using Box-Muller transform ‚Ä¶\nGenerate single random normal scalar ‚Ä¶\nGenerate random uniform samples ‚Ä¶\nGenerate single random uniform scalar ‚Ä¶\nSample from the distribution ‚Ä¶\nSample with replacement Âæ©ÂÖÉÊäΩÂá∫„Åß„Çµ„É≥„Éó„É´\nGet distribution standard deviation ‚Ä¶\nUniform distribution implementation\nValidate non-negative parameter (&gt;= 0) ‚Ä¶\nValidate positive parameter (&gt; 0) ‚Ä¶\nValidate probability parameter (0 &lt;= p &lt;= 1) ‚Ä¶\nGet distribution variance ÂàÜÂ∏É„ÅÆÂàÜÊï£„ÇíÂèñÂæó\nBernoulli Distribution „Éô„É´„Éå„Éº„Ç§ÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nBinary cross entropy for Bernoulli distributions ‚Ä¶\nCreate fair coin (p = 0.5) ÂÖ¨Ê≠£„Å™„Ç≥„Ç§„É≥Ôºàp = 0.5Ôºâ\nReturns the argument unchanged.\nCreate a new Bernoulli distribution from logits ‚Ä¶\nCreate a new Bernoulli distribution from probability ‚Ä¶\nCreate Bernoulli distribution with scalar probability ‚Ä¶\nGet logits (convert from probs if necessary) ‚Ä¶\nGet probabilities (convert from logits if necessary) ‚Ä¶\nCalls <code>U::from(self)</code>.\nLogits parameter (log-odds) - optional ‚Ä¶\nProbability parameter (p) - optional Á¢∫Áéá„Éë„É©„É°„Éº„Çø ‚Ä¶\nBeta Distribution „Éô„Éº„ÇøÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nSecond concentration parameter (Œ≤) ‚Ä¶\nFirst concentration parameter (Œ±) ‚Ä¶\nReturns the argument unchanged.\nCreate Beta distribution with scalar parameters ‚Ä¶\nCalls <code>U::from(self)</code>.\nCreate a new Beta distribution ‚Ä¶\nSymmetric Beta distribution with equal parameters (Œ±=Œ≤) ‚Ä¶\nUniform Beta distribution (Œ±=1, Œ≤=1) - equivalent to ‚Ä¶\nCategorical Distribution „Ç´„ÉÜ„Ç¥„É™„Ç´„É´ÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nCompute cross entropy loss ‚Ä¶\nReturns the argument unchanged.\nCreate a new Categorical distribution from logits ‚Ä¶\nCreate a new Categorical distribution from probabilities ‚Ä¶\nGet logits (convert from probs if necessary) ‚Ä¶\nGet probabilities (convert from logits if necessary) ‚Ä¶\nCalls <code>U::from(self)</code>.\nLogits parameters (unnormalized log probabilities) - ‚Ä¶\nNumber of categories „Ç´„ÉÜ„Ç¥„É™Êï∞\nProbability parameters - optional Á¢∫Áéá„Éë„É©„É°„Éº„Çø - ‚Ä¶\nCreate uniform categorical distribution ‚Ä¶\nBase Distribution struct for common functionality ‚Ä¶\nDistribution registry for factory pattern ‚Ä¶\nList all available distribution types ‚Ä¶\nBatch shape of the distribution ÂàÜÂ∏É„ÅÆ„Éê„ÉÉ„ÉÅÂΩ¢Áä∂\nBroadcast two shapes together ‚Ä¶\nEvent shape of the distribution ÂàÜÂ∏É„ÅÆ„Ç§„Éô„É≥„ÉàÂΩ¢Áä∂\nExpand batch dimensions for sampling ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet distribution name for a given type ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new Distribution Êñ∞„Åó„ÅÑÂàÜÂ∏É„Çí‰ΩúÊàê\nGet the shape of a single sample ‚Ä¶\nWhether to validate parameters ‚Ä¶\nValidate tensor shape matches expected shape ‚Ä¶\nExponential Distribution ÊåáÊï∞ÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nReturns the argument unchanged.\nCreate Exponential distribution with scalar rate ‚Ä¶\nCreate exponential distribution from scale parameter ‚Ä¶\nCreate exponential distribution from scalar scale parameter\nCalls <code>U::from(self)</code>.\nCreate a new Exponential distribution ‚Ä¶\nRate parameter (Œª) „É¨„Éº„Éà„Éë„É©„É°„Éº„Çø (Œª)\nGet the scale parameter (1/rate) ‚Ä¶\nStandard exponential distribution (Œª=1) ‚Ä¶\nGamma Distribution „Ç¨„É≥„ÉûÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nShape parameter (Œ±, concentration) ‚Ä¶\nCreate standard Gamma distribution (shape=1, scale=1) - ‚Ä¶\nReturns the argument unchanged.\nCreate a new Gamma distribution with concentration and rate\nCreate a new Gamma distribution with concentration and ‚Ä¶\nGet rate parameter (convert from scale if necessary) ‚Ä¶\nGet scale parameter (convert from rate if necessary) ‚Ä¶\nCalls <code>U::from(self)</code>.\nRate parameter (Œ≤) - optional if scale is provided ‚Ä¶\nScale parameter (Œ∏ = 1/Œ≤) - optional if rate is provided ‚Ä¶\nNormal Distribution Ê≠£Ë¶èÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nReturns the argument unchanged.\nCreate Normal distribution with scalar parameters ‚Ä¶\nCalls <code>U::from(self)</code>.\nMean parameter (Œº) Âπ≥Âùá„Éë„É©„É°„Éº„Çø (Œº)\nCreate a new Normal distribution ‚Ä¶\nStandard deviation parameter (œÉ) ‚Ä¶\nStandard normal distribution (Œº=0, œÉ=1) ‚Ä¶\nUniform Distribution ‰∏ÄÊßòÂàÜÂ∏É\nBase distribution properties Âü∫Êú¨ÂàÜÂ∏ÉÁâπÊÄß\nReturns the argument unchanged.\nCreate Uniform distribution with scalar parameters ‚Ä¶\nUpper bound parameter (b) ‰∏äÈôê„Éë„É©„É°„Éº„Çø (b)\nCalls <code>U::from(self)</code>.\nLower bound parameter (a) ‰∏ãÈôê„Éë„É©„É°„Éº„Çø (a)\nCreate a new Uniform distribution ‚Ä¶\nStandard uniform distribution on [0, 1) [0, ‚Ä¶\nSymmetric uniform distribution around zero [-a, a) ‚Ä¶\n16-bit brain floating point (bfloat16)\nBoolean\nComplex 128-bit (64-bit real + 64-bit imaginary)\nComplex 64-bit (32-bit real + 32-bit imaginary)\nData type enumeration\n16-bit floating point (half precision)\n32-bit floating point (single precision)\n64-bit floating point (double precision)\n16-bit signed integer\n32-bit signed integer\n64-bit signed integer\n8-bit signed integer\n16-bit unsigned integer\n32-bit unsigned integer\n64-bit unsigned integer\n8-bit unsigned integer\nGet the common dtype for two dtypes (promotion)\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if this is a boolean type\nCheck if two dtypes are compatible for operations\nCheck if this is a complex type\nCheck if this is a floating point type\nCheck if this is an integer type\nCheck if this is a signed integer type\nCheck if this is an unsigned integer type\nGet the maximum value for this data type (if applicable)\nGet the minimum value for this data type (if applicable)\nParse dtype from string\nGet the size in bytes of this data type\nGet the corresponding floating point type for this dtype\nGet the corresponding integer type for this dtype\nAutomatic differentiation error Ëá™ÂãïÂæÆÂàÜ„Ç®„É©„Éº\nBackend not available „Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅåÂà©Áî®‰∏çÂèØ\nCreate cluster error\nCreate communication error\nCreate computation error\nCreate config error\nCreate configuration error\nCreate convergence error\nData loading error „Éá„Éº„ÇøË™≠„ÅøËæº„Åø„Ç®„É©„Éº\nDataLoader system errors (Phase 5) ‚Ä¶\nCreate dataset error\nDebug system and logging errors ‚Ä¶\nCreate deserialization error\nDevice operation failed „Éá„Éê„Ç§„ÇπÊìç‰ΩúÂ§±Êïó\nCreate device not available error\nDistributed computing errors ‚Ä¶\nCreate domain error\nCreate download error\nContains the error value\nCreate file not found error\nGPU-specific errors GPUÂõ∫Êúâ„Ç®„É©„Éº\nInput/Output and serialization errors ‚Ä¶\nImport-specific error „Ç§„É≥„Éù„Éº„ÉàÂõ∫Êúâ„Ç®„É©„Éº\nCreate incompatible shapes error\nCreate invalid data format error\nCreate invalid dimension error\nCreate invalid dimensions error\nCreate invalid image shape error\nCreate invalid model error\nCreate invalid operation error\nInvalid operation ÁÑ°Âäπ„Å™Êìç‰Ωú\nInvalid operation parameters ÁÑ°Âäπ„Å™Êìç‰Ωú„Éë„É©„É°„Éº„Çø\nCreate invalid rank error\nCreate invalid transform params error\nCreate IO error\nKernel compilation error for GPU operations ‚Ä¶\nCreate kernel error\nCreate kernel execution error\nMemory allocation failed „É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶Â§±Êïó\nCreate memory error\nCreate mismatched dimensions error\nModel import/export error ‚Ä¶\nCreate model not found error\nNeural network layer error ‚Ä¶\nFeature not implemented yet Ê©üËÉΩÊú™ÂÆüË£Ö\nCreate not singleton dimension error\nContains the success value\nOut of memory error „É°„É¢„É™‰∏çË∂≥„Ç®„É©„Éº\nCreate overflow error\nCreate parse error\nCreate plotting error\nCreate process group error\nProfiling and performance analysis errors ‚Ä¶\nCreate reshape error\nMain error type for RusTorch operations ‚Ä¶\nUnified Result type for all RusTorch operations - the ONLY ‚Ä¶\nSerialization error „Ç∑„É™„Ç¢„É©„Ç§„Çº„Éº„Ç∑„Éß„É≥„Ç®„É©„Éº\nCreate serialization error\nSerialization and model I/O errors (Phase 9) ‚Ä¶\nShape mismatch between tensors ‚Ä¶\nTensor operation failed „ÉÜ„É≥„ÇΩ„É´Êìç‰ΩúÂ§±Êïó\nCreate unsupported device error\nCreate unsupported operation error\nData validation and quality assurance errors ‚Ä¶\nCreate validation error\nCreate verification error\nVision processing errors Ë¶ñË¶öÂá¶ÁêÜ„Ç®„É©„Éº\nVisualization errors ÂèØË¶ñÂåñ„Ç®„É©„Éº\nCreate autograd error\nCreate backend unavailable error\nCreate backward pass error\nError context and diagnostic information ‚Ä¶\nCreate data loading error\nCreate dataset error\nCreate a device error\nCreate device not available error\nCreate distributed error\nCreate empty tensor error\nCreate forward pass error\nReturns the argument unchanged.\nCreate a GPU error\nCreate gradient computation error\nCreate import error\nCreate index out of bounds error\nCalls <code>U::from(self)</code>.\nCreate invalid configuration error\nCreate invalid dimension error\nCreate invalid image format error\nCreate invalid image shape error\nCreate invalid parameter error (simple)\nCreate invalid parameters error\nCreate kernel compilation error\nCreate memory error (simple)\nCreate memory allocation error\nCreate model I/O error\nCreate neural network error\nCreate not implemented error\nCreate numeric error\nCreate out of memory error\nCreate parallel error\nCreate plotting error\nCreate process group error\nCreate quantization calibration failed error\nCreate quantization hardware not supported error\nCreate incompatible quantization schemes error\nCreate invalid quantization parameters error\nCreate quantization range overflow error\nCreate serialization error\nCreate a shape mismatch error\nCreate a tensor operation error\nCreate type error\nCreate unsupported format error\nCreate vision processing error\nCreate visualization error\nActual tensor shape that was provided ‚Ä¶\nAvailable memory in bytes ‚Ä¶\nName of the unavailable backend ‚Ä¶\nDevice identifier where the error occurred ‚Ä¶\nDevice where allocation failed ‚Ä¶\nExpected tensor shape ÊúüÂæÖ„Åï„Çå„Çã„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nFeature that is not yet implemented Êú™ÂÆüË£Ö„ÅÆÊ©üËÉΩ\nName of the neural network layer ‚Ä¶\nError message describing the tensor operation failure ‚Ä¶\nError message describing the device issue ‚Ä¶\nDescription of the parameter issue ‚Ä¶\nDetailed error message Ë©≥Á¥∞„Å™„Ç®„É©„Éº„É°„ÉÉ„Çª„Éº„Ç∏\nError message from the layer ‚Ä¶\nAutograd error description Ëá™ÂãïÂæÆÂàÜ„Ç®„É©„Éº„ÅÆË™¨Êòé\nModel I/O error description „É¢„Éá„É´I/O„Ç®„É©„Éº„ÅÆË™¨Êòé\nImport operation error message ‚Ä¶\nData loading error description ‚Ä¶\nGPU operation error description GPUÊìç‰Ωú„Ç®„É©„Éº„ÅÆË™¨Êòé\nVision processing error description ‚Ä¶\nDistributed computing error description ‚Ä¶\nVisualization error description ÂèØË¶ñÂåñ„Ç®„É©„Éº„ÅÆË™¨Êòé\nProfiling error description ‚Ä¶\nData validation error description ‚Ä¶\nDebug system error description ‚Ä¶\nKernel compilation error description ‚Ä¶\nDataLoader error description DataLoader„Ç®„É©„Éº„ÅÆË™¨Êòé\nSerialization error description ‚Ä¶\nSerialization error description ‚Ä¶\nName of the operation with invalid parameters ‚Ä¶\nName of the invalid operation ÁÑ°Âäπ„Å™Êìç‰ΩúÂêç\nOperation that caused the serialization error ‚Ä¶\nRequested memory in bytes ‚Ä¶\nSize of the failed allocation in bytes ‚Ä¶\nOptional underlying error cause ‚Ä¶\nOptional underlying I/O error ‚Ä¶\nOptional underlying import error ‚Ä¶\nOptional underlying data loading error ‚Ä¶\nOptional underlying GPU error ‚Ä¶\nOptional underlying vision error ‚Ä¶\nOptional underlying distributed error ‚Ä¶\nOptional underlying visualization error ‚Ä¶\nError context for providing additional diagnostic ‚Ä¶\nFile location information „Éï„Ç°„Ç§„É´‰ΩçÁΩÆÊÉÖÂ†±\nTrait for adding context to errors ‚Ä¶\nColumn number ÂàóÁï™Âè∑\nSource file name „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´Âêç\nGet formatted context information ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLine number Ë°åÁï™Âè∑\nFile location where error occurred ‚Ä¶\nAdditional metadata ËøΩÂä†„ÅÆ„É°„Çø„Éá„Éº„Çø\nCreate a new error context ‚Ä¶\nOperation that was being performed ‚Ä¶\nAdd operation to stack trace ‚Ä¶\nStack trace of operations Êìç‰Ωú„ÅÆ„Çπ„Çø„ÉÉ„ÇØ„Éà„É¨„Éº„Çπ\nAdd context to the error ‚Ä¶\nAdd location information ‰ΩçÁΩÆÊÉÖÂ†±„ÇíËøΩÂä†\nAdd metadata to the context ‚Ä¶\nAdd simple operation context ‚Ä¶\nDynamic computation graph execution engine ‚Ä¶\nRuntime execution engine with JIT compilation and ‚Ä¶\nElement-wise addition\nBatch normalization\nConvolution operation\nCustom operation with name\nDropout\nDynamic execution context for runtime graph management ‚Ä¶\nDynamic execution statistics ÂãïÁöÑÂÆüË°åÁµ±Ë®à\nDynamic execution node containing operation and runtime ‚Ä¶\nDynamic operation types for runtime execution ‚Ä¶\nExecution plan for optimized graph execution ‚Ä¶\nFused operation that combines multiple operations ‚Ä¶\nJIT compilation context for dynamic operations ‚Ä¶\nJIT compilation statistics JIT„Ç≥„É≥„Éë„Ç§„É´Áµ±Ë®à\nLinear transformation\nMatrix multiplication\nMemory allocation entry „É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶„Ç®„É≥„Éà„É™\nMemory allocation plan „É°„É¢„É™Ââ≤„ÇäÂΩì„Å¶„Éó„É©„É≥\nElement-wise multiplication\nPlanned operation with optimization metadata ‚Ä¶\nReLU activation\nReshape operation\nSigmoid activation\nAdd a leaf node (input/parameter)\nAdd a dynamic operation node\nAdd operation to plan\nMemory allocation schedule\nAverage execution speedup\nCache hit rate\nCache hits\nCached output tensor\nClear all cached outputs and force recomputation\nTotal compilation time\nNumber of compilations\nCompile a sequence of operations into optimized function\nCreate execution plan with memory optimization\nWhether this node needs recomputation\nGet estimated total execution time\nEstimated execution time\nExecute the graph and return output of specified node\nExecute a single node\nExecution time tracking\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cached output if available\nGet dynamic node by ID\nGet execution statistics\nGet compilation statistics\nNode ID for tracking\nInput node IDs\nInput node references\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if node is dirty\nJIT compilations performed\nLifetime (when this memory can be freed)\nMark this node as dirty (needs recomputation)\nMemory allocations\nMemory allocation plan\nMemory requirements\nMemory usage tracking\nCreate a new dynamic node\nCreate new dynamic execution context\nCreate new JIT compiler\nCreate new fused operation\nCreate new execution plan\nNode ID\nOperation type\nOperation type\nOperation that needs this memory\nOrdered operations\nOptimize execution order for parallelism\nOptimize memory usage by analyzing lifetimes\nParallel execution opportunities\nCan be executed in parallel with previous operations\nPeak memory usage\nGet peak memory usage\nCan reuse memory from previous allocation\nMemory reuse opportunities\nSet cached output\nSize in bytes\nTotal execution time\nTotal operations executed\nBottleneck information „Éú„Éà„É´„Éç„ÉÉ„ÇØÊÉÖÂ†±\nCached execution for pattern reuse ‚Ä¶\nGraph builder for fluent API ‚Ä¶\nJIT compilation metrics JIT„Ç≥„É≥„Éë„Ç§„É´„É°„Éà„É™„ÇØ„Çπ\nMemory usage metrics „É°„É¢„É™‰ΩøÁî®Èáè„É°„Éà„É™„ÇØ„Çπ\nParallel execution metrics ‰∏¶ÂàóÂÆüË°å„É°„Éà„É™„ÇØ„Çπ\nProfiling result with performance analysis ‚Ä¶\nRuntime configuration ÂÆüË°åÊôÇË®≠ÂÆö\nRuntime execution engine for dynamic graph execution ‚Ä¶\nRuntime performance metrics ‚Ä¶\nAdd element-wise addition\nAdd execution time measurement\nAdd input tensor\nAdd operation\nAdd parameter tensor\nAllocation count\nAnalyze performance and generate recommendations\nAverage compilation time\nAverage execution time\nAverage parallelism factor\nAverage speedup from JIT\nCache hit rate\nClean up old cache entries\nRuntime configuration\nDynamic execution context\nAdd conv2d layer\nCurrent memory usage\nDeallocation count\nEnable operation fusion\nEnable JIT compilation\nEnable memory optimization\nEnable parallel execution\nExecute a computation graph with runtime optimization\nExecution cache for common patterns\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet runtime metrics\nHit count\nExpected input shapes\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nJIT compilation statistics\nJIT compilation threshold (operations)\nLast used timestamp\nAdd linear layer\nAdd matrix multiplication\nMaximum cache size\nMemory efficiency (reuse rate)\nMemory statistics\nCreate new runtime engine\nCreate new graph builder\nCreate new profile result\nOperation type causing bottleneck\nOutput shape\nParallel efficiency\nParallel executions performed\nParallel execution opportunities\nParallel execution statistics\nPeak memory usage\nExecution plan\nProfile execution and suggest optimizations\nRecommended optimization\nAdd ReLU activation\nReset all metrics\nAdd reshape operation\nAdd sigmoid activation\nSuccessful compilations\nGet performance summary\nPercentage of total time\nTotal compilations\nTotal executions\nWarm up the engine with common operations\nPyTorch compatibility format for RusTorch ‚Ä¶\nPyTorch model wrapper for RusTorch ‚Ä¶\nPyTorch state dict format for RusTorch ‚Ä¶\nSerializable tensor data ‚Ä¶\nAdd metadata „É°„Çø„Éá„Éº„Çø„ÇíËøΩÂä†\nAdd tensor to state dict ‚Ä¶\nOptional architecture description ‚Ä¶\nTensor data as f64 for maximum precision ‚Ä¶\nData type string identifier „Éá„Éº„ÇøÂûãÊñáÂ≠óÂàóË≠òÂà•Â≠ê\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nLoad model from PyTorch state dict ‚Ä¶\nGet layer bias „É¨„Ç§„É§„Éº„Éê„Ç§„Ç¢„Çπ„ÇíÂèñÂæó\nGet layer weights „É¨„Ç§„É§„ÉºÈáç„Åø„ÇíÂèñÂæó\nGet metadata „É°„Çø„Éá„Éº„Çø„ÇíÂèñÂæó\nGet tensor from state dict ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet layer names „É¨„Ç§„É§„ÉºÂêç„ÇíÂèñÂæó\nLoad model from file ‚Ä¶\nLoad state dict from file (JSON format) ‚Ä¶\nOptional metadata for the state dict ‚Ä¶\nCreate new PyTorch model Êñ∞„Åó„ÅÑPyTorch„É¢„Éá„É´„Çí‰ΩúÊàê\nCreate new empty state dict ‚Ä¶\nSave model to file „É¢„Éá„É´„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò\nSave state dict to file (JSON format) ‚Ä¶\nSet model architecture description ‚Ä¶\nSet layer bias „É¨„Ç§„É§„Éº„Éê„Ç§„Ç¢„Çπ„ÇíË®≠ÂÆö\nSet layer weights „É¨„Ç§„É§„ÉºÈáç„Åø„ÇíË®≠ÂÆö\nShape of the tensor „ÉÜ„É≥„ÇΩ„É´„ÅÆÂΩ¢Áä∂\nThe model‚Äôs state dictionary containing parameters ‚Ä¶\nGet all tensor names ÂÖ®„ÉÜ„É≥„ÇΩ„É´Âêç„ÇíÂèñÂæó\nMap of tensor names to tensor data ‚Ä¶\nUtility functions for PyTorch compatibility ‚Ä¶\nConvert layer name from PyTorch to RusTorch convention ‚Ä¶\nExtract model statistics „É¢„Éá„É´Áµ±Ë®à„ÇíÊäΩÂá∫\nConvert layer name from RusTorch to PyTorch convention ‚Ä¶\nValidate model structure „É¢„Éá„É´ÊßãÈÄ†„ÇíÊ§úË®º\nActivation functions\nAuto-select best available device ‚Ä¶\nColumn-major (Fortran-style) layout ‚Ä¶\nComplex number operations (CoreML unsupported)\nConvolution operations\nCPU device CPU„Éá„Éê„Ç§„Çπ\nCUDA GPU device CUDA GPU„Éá„Éê„Ç§„Çπ\nCustom kernel operations (CoreML unsupported)\nDevice capability information „Éá„Éê„Ç§„ÇπËÉΩÂäõÊÉÖÂ†±\nGPU device manager GPU„Éá„Éê„Ç§„Çπ„Éû„Éç„Éº„Ç∏„É£„Éº\nGPU device types with CoreML and hybrid support GPU ‚Ä¶\nDistributed operations (CoreML unsupported)\nStatistical distributions (CoreML unsupported)\nGPU operation context GPUÊºîÁÆó„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà\nGPU device types for fallback ‚Ä¶\nLinear algebra operations (matmul, etc.)\nGPU memory layout GPU„É°„É¢„É™„É¨„Ç§„Ç¢„Ç¶„Éà\nMetal GPU device (for Apple Silicon) Metal ‚Ä¶\nNormalization operations\nOperation types for device capability checking ‚Ä¶\nOpenCL GPU device OpenCL GPU„Éá„Éê„Ç§„Çπ\nReduction operations (sum, mean, etc.)\nRow-major (C-style) layout ‚Ä¶\nGPU activation operations and optimization ‚Ä¶\nGet available devices Âà©Áî®ÂèØËÉΩ„Å™„Éá„Éê„Ç§„Çπ„ÇíÂèñÂæó\nPerformance benchmark suite for GPU operations ‚Ä¶\nGPU convolution operations and cuDNN/MPS integration ‚Ä¶\nEnhanced CUDA cuBLAS integration for high-performance ‚Ä¶\nCUDA kernel implementations CUDA„Ç´„Éº„Éç„É´ÂÆüË£Ö CUDA ‚Ä¶\nGet current context ÁèæÂú®„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÂèñÂæó\nGet the current device ÁèæÂú®„ÅÆ„Éá„Éê„Ç§„Çπ„ÇíÂèñÂæó\nGet current device ÁèæÂú®„ÅÆ„Éá„Éê„Ç§„Çπ„ÇíÂèñÂæó\nCustom GPU kernels for specialized tensor operations ‚Ä¶\nDevice management module for GPU operations ‚Ä¶\nGet the device type „Éá„Éê„Ç§„Çπ„Çø„Ç§„Éó„ÇíÂèñÂæó\nDevice caching module for optimized initialization ‚Ä¶\nDistributed training infrastructure for multi-GPU learning ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet the global device manager ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if the device is available ‚Ä¶\nCheck if CUDA is available ‚Ä¶\nCheck if any GPU is available ‚Ä¶\nCheck if GPU is available GPU„ÅåÂà©Áî®ÂèØËÉΩ„Åã„ÉÅ„Çß„ÉÉ„ÇØ\nCheck if Metal is available ‚Ä¶\nGPU kernel execution and management ‚Ä¶\nGPU matrix operations and BLAS integration ‚Ä¶\nGPU memory management and allocation ‚Ä¶\nGPU memory operations (modular implementation) ‚Ä¶\nGet memory pool size „É°„É¢„É™„Éó„Éº„É´„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\nGPU memory transfer operations GPU„É°„É¢„É™Ëª¢ÈÄÅÊìç‰Ωú ‚Ä¶\nMetal Performance Shaders kernel implementations for GPU ‚Ä¶\nMulti-GPU distributed processing and communication ‚Ä¶\nMulti-GPU performance profiling and benchmarking ‚Ä¶\nCreate a new GPU context ‚Ä¶\nCreate a new device manager ‚Ä¶\nOpenCL kernel implementations for GPU acceleration ‚Ä¶\nOptimized OpenCL implementation for cross-platform GPU ‚Ä¶\nPerformance Benchmark Suite for GPU Kernels ‚Ä¶\nGPU performance optimizer ‚Ä¶\nGPU reduction operations and optimizations ‚Ä¶\nSet the current device globally ‚Ä¶\nSet current device ÁèæÂú®„ÅÆ„Éá„Éê„Ç§„Çπ„ÇíË®≠ÂÆö\nSimple Metal GPU testing and benchmarking ‚Ä¶\nSmart device selection module for optimized operation ‚Ä¶\nGet number of streams „Çπ„Éà„É™„Éº„É†Êï∞„ÇíÂèñÂæó\nCheck if device supports specific operation ‚Ä¶\nGPU synchronization primitives for multi-GPU operations ‚Ä¶\nUnified kernel interface for cross-platform GPU ‚Ä¶\nGPU kernel validation and testing ‚Ä¶\nGPU vs CPU verification tests GPU vs CPUÊ§úË®º„ÉÜ„Çπ„Éà ‚Ä¶\nGPU activation operations trait ‚Ä¶\nGPU ELU activation GPU ELUÊ¥ªÊÄßÂåñ\nGPU GELU activation GPU GELUÊ¥ªÊÄßÂåñ\nGPU Leaky ReLU activation GPU Leaky ReLUÊ¥ªÊÄßÂåñ\nGPU ReLU activation GPU ReLUÊ¥ªÊÄßÂåñ\nGPU Sigmoid activation GPU SigmoidÊ¥ªÊÄßÂåñ\nGPU Softmax activation GPU SoftmaxÊ¥ªÊÄßÂåñ\nGPU Swish activation GPU SwishÊ¥ªÊÄßÂåñ\nGPU Tanh activation GPU TanhÊ¥ªÊÄßÂåñ\nBenchmark configuration for GPU performance testing ‚Ä¶\nBenchmark result data structures and utilities ‚Ä¶\nPerformance benchmark suite implementation ‚Ä¶\nBenchmark configuration „Éô„É≥„ÉÅ„Éû„Éº„ÇØË®≠ÂÆö\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nMaximum benchmark duration in milliseconds ‚Ä¶\nEnable FLOPS measurements FLOPSÊ∏¨ÂÆö„ÇíÊúâÂäπÂåñ\nEnable memory bandwidth measurements ‚Ä¶\nNumber of measurement iterations Ê∏¨ÂÆöÂèçÂæ©ÂõûÊï∞\nMinimum benchmark duration in milliseconds ‚Ä¶\nNumber of warmup iterations ‚Ä¶\nBenchmark result for a single operation ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate a new benchmark result ‚Ä¶\nAdd CPU timing information to the benchmark result ‚Ä¶\nAdd error message to the benchmark result ‚Ä¶\nAdd FLOPS count to the benchmark result ‚Ä¶\nAdd GPU timing information to the benchmark result ‚Ä¶\nAdd memory bytes to the benchmark result ‚Ä¶\nPerformance benchmark suite for GPU operations ‚Ä¶\nClear all results „Åô„Åπ„Å¶„ÅÆÁµêÊûú„Çí„ÇØ„É™„Ç¢\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate a new performance benchmark ‚Ä¶\nGet benchmark results „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„ÇíÂèñÂæó\nRun comprehensive benchmark suite ‚Ä¶\nGPU Convolution trait for convolution operations with ‚Ä¶\nGPU-accelerated 2D convolution with automatic device ‚Ä¶\nGPU-accelerated 3D convolution GPUÂä†ÈÄü3DÁï≥„ÅøËæº„Åø\nGPU-accelerated transposed convolution (deconvolution) ‚Ä¶\nGPU-accelerated depthwise separable convolution ‚Ä¶\nGPU-accelerated grouped convolution ‚Ä¶\nNon-CUDA fallback implementation ‚Ä¶\nPerform batch matrix multiplication using CUDA cuBLAS CUDA ‚Ä¶\nExecute batch CUDA matrix multiplication ‚Ä¶\nPublic interface functions for CUDA operations ‚Ä¶\nReturns the argument unchanged.\nGet CUDA compute capability (major, minor) ‚Ä¶\nGet CUDA device memory information (free, total) ‚Ä¶\nCalls <code>U::from(self)</code>.\nPerform matrix multiplication using CUDA cuBLAS CUDA ‚Ä¶\nCreate a new CUDA matrix executor for the specified device ‚Ä¶\nBatch normalization „Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñ\nConvolution Áï≥„ÅøËæº„Åø\nCUDA memory buffer CUDA„É°„É¢„É™„Éê„ÉÉ„Éï„Ç°\nNon-CUDA fallback executor for compatibility ‚Ä¶\nCUDA kernel parameters CUDA„Ç´„Éº„Éç„É´„Éë„É©„É°„Éº„Çø#[‚Ä¶\nCUDA kernel types CUDA„Ç´„Éº„Éç„É´„Çø„Ç§„Éó#[derive(Debug, ‚Ä¶\nCUDA device properties CUDA„Éá„Éê„Ç§„Çπ„Éó„É≠„Éë„ÉÜ„Ç£#[‚Ä¶\nElement-wise operations Ë¶ÅÁ¥†„Åî„Å®ÊºîÁÆó\nMatrix multiplication Ë°åÂàó‰πóÁÆó\nReduction operations „É™„ÉÄ„ÇØ„Ç∑„Éß„É≥ÊºîÁÆó\nBlock dimensions „Éñ„É≠„ÉÉ„ÇØÊ¨°ÂÖÉ\nCompute capability version (major, minor) ‚Ä¶\nPerform 2D convolution using CUDA ‚Ä¶\nExecute CUDA 2D convolution CUDA 2DÁï≥„ÅøËæº„Åø„ÇíÂÆüË°å\nExecute CUDA element-wise addition ‚Ä¶\nPublic interface functions for CUDA operations ‚Ä¶\nExecute CUDA reduction sum ‚Ä¶\nCUDA kernel optimization utilities ‚Ä¶\nDevice ID „Éá„Éê„Ç§„ÇπID\nPerform element-wise addition using CUDA ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGrid dimensions „Ç∞„É™„ÉÉ„ÉâÊ¨°ÂÖÉ\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nPerform matrix multiplication using CUDA ‚Ä¶\nMaximum shared memory size ÊúÄÂ§ßÂÖ±Êúâ„É°„É¢„É™„Çµ„Ç§„Ç∫\nMaximum threads per block ‚Ä¶\nTotal memory size Á∑è„É°„É¢„É™„Çµ„Ç§„Ç∫\nDevice name „Éá„Éê„Ç§„ÇπÂêç\nCreate a new CUDA kernel executor for the specified device ‚Ä¶\nPerform reduction sum using CUDA ‚Ä¶\nShared memory size ÂÖ±Êúâ„É°„É¢„É™„Çµ„Ç§„Ç∫\nSize in elements Ë¶ÅÁ¥†Êï∞\nStream ID „Çπ„Éà„É™„Éº„É†ID\nWarp size „ÉØ„Éº„Éó„Çµ„Ç§„Ç∫\nCalculate optimal grid and block dimensions ‚Ä¶\nGet device properties „Éá„Éê„Ç§„Çπ„Éó„É≠„Éë„ÉÜ„Ç£„ÇíÂèñÂæó\nAttention mechanism kernel ‚Ä¶\nBatch normalization kernel „Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñ„Ç´„Éº„Éç„É´\nBoolean parameter ÁúüÂÅΩÂÄ§„Éë„É©„É°„Éº„Çø\nCompiled kernel representation ‚Ä¶\nCustom activation functions „Ç´„Çπ„Çø„É†Ê¥ªÊÄßÂåñÈñ¢Êï∞\nCustom kernel manager ‚Ä¶\nCustom kernel types for specialized operations ‚Ä¶\nFast Fourier Transform kernel ‚Ä¶\nFloat parameter ÊµÆÂãïÂ∞èÊï∞ÁÇπ„Éë„É©„É°„Éº„Çø\nFloat array parameter ÊµÆÂãïÂ∞èÊï∞ÁÇπÈÖçÂàó„Éë„É©„É°„Éº„Çø\nInteger parameter Êï¥Êï∞„Éë„É©„É°„Éº„Çø\nInteger array parameter Êï¥Êï∞ÈÖçÂàó„Éë„É©„É°„Éº„Çø\nCustom kernel configuration „Ç´„Çπ„Çø„É†„Ç´„Éº„Éç„É´Ë®≠ÂÆö#‚Ä¶\nKernel parameter types ‚Ä¶\nKernel performance statistics ‚Ä¶\nOptimized convolution kernel ‚Ä¶\nMemory-optimized reduction ‚Ä¶\nSparse matrix operations „Çπ„Éë„Éº„ÇπË°åÂàóÊºîÁÆó\nString parameter ÊñáÂ≠óÂàó„Éë„É©„É°„Éº„Çø\nTensor fusion operations „ÉÜ„É≥„ÇΩ„É´ËûçÂêàÊºîÁÆó\nCompiled binary data ‚Ä¶\nSize of the compiled binary in bytes ‚Ä¶\nBlock size for kernel execution (x, y, z) ‚Ä¶\nTime taken to compile the kernel ‚Ä¶\nTime when the kernel was compiled ‚Ä¶\nCompile and cache custom kernel ‚Ä¶\nKernel entry point function name ‚Ä¶\nExecute custom kernel „Ç´„Çπ„Çø„É†„Ç´„Éº„Éç„É´„ÇíÂÆüË°å\nNumber of times the kernel has been executed ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet kernel performance statistics ‚Ä¶\nGrid size for kernel execution (x, y, z) ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nType of custom kernel to execute ‚Ä¶\nType of the kernel „Ç´„Éº„Éç„É´„ÅÆ„Çø„Ç§„Éó\nType of the compiled kernel ‚Ä¶\nCreate new custom kernel manager ‚Ä¶\nKernel parameters „Ç´„Éº„Éç„É´„Éë„É©„É°„Éº„Çø\nShared memory size in bytes ‚Ä¶\nSource code of the kernel „Ç´„Éº„Éç„É´„ÅÆ„ÇΩ„Éº„Çπ„Ç≥„Éº„Éâ\nTotal time spent executing the kernel ‚Ä¶\nCPU device implementation#[derive(Debug)]\nCPU stream implementation#[derive(Debug)]\nGPU device capabilities GPU„Éá„Éê„Ç§„ÇπÊ©üËÉΩ#[‚Ä¶\nDevice information and management ‚Ä¶\nDevice registry for managing multiple devices ‚Ä¶\nGPU backend for managing multiple devices\nGPU device trait\nGPU stream trait for asynchronous operations\nGet allocated memory in bytes\nAvailable memory in bytes ‚Ä¶\nGet best device for operation ‚Ä¶\nGet device capabilities „Éá„Éê„Ç§„ÇπÊ©üËÉΩ„ÇíÂèñÂæó\nGet compute capability (for CUDA)\nCompute capability major version ‚Ä¶\nCompute capability minor version ‚Ä¶\nCreate device info for CPU ‚Ä¶\nCreate a stream for asynchronous operations\nCreate device info for CUDA device ‚Ä¶\nGet device type\nGet device type „Éá„Éê„Ç§„Çπ„Çø„Ç§„Éó„ÇíÂèñÂæó\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet device by ID\nGet device info „Éá„Éê„Ç§„ÇπÊÉÖÂ†±„ÇíÂèñÂæó\nGet device ID\nGet stream ID\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if device is available\nCheck if device is available ‚Ä¶\nCheck if device is CPU\nList all available devices\nList all available devices ‚Ä¶\nMaximum block dimensions ÊúÄÂ§ß„Éñ„É≠„ÉÉ„ÇØÊ¨°ÂÖÉ\nMaximum grid dimensions ÊúÄÂ§ß„Ç∞„É™„ÉÉ„ÉâÊ¨°ÂÖÉ\nMaximum threads per block ‚Ä¶\nGet memory usage information ‚Ä¶\nCreate device info for Metal device ‚Ä¶\nGet device name\nDevice name „Éá„Éê„Ç§„ÇπÂêç\nCreate new GPU backend\nCreate a new CPU device Êñ∞„Åó„ÅÑCPU„Éá„Éê„Ç§„Çπ„Çí‰ΩúÊàê\nCreate a new CPU stream ‚Ä¶\nCreate a new device registry ‚Ä¶\nGet optimal block size for given problem size ‚Ä¶\nGet optimal grid size for given problem size and block size\nShared memory per block ‚Ä¶\nSupports double precision ÂÄçÁ≤æÂ∫¶„Çµ„Éù„Éº„Éà\nSupports half precision ÂçäÁ≤æÂ∫¶„Çµ„Éù„Éº„Éà\nCheck if device supports operation ‚Ä¶\nSupports tensor cores „ÉÜ„É≥„Çµ„Éº„Ç≥„Ç¢„Çµ„Éù„Éº„Éà\nSynchronize device\nSynchronize this stream\nGet total memory in bytes\nTotal memory in bytes Á∑è„É°„É¢„É™Ôºà„Éê„Ç§„ÉàÔºâ\nWarp size „ÉØ„Éº„Éó„Çµ„Ç§„Ç∫\nCache performance statistics ‚Ä¶\nCached device information ‚Ä¶\nCoreML-specific initialization cache ‚Ä¶\nDevice availability cache with lazy initialization ‚Ä¶\nDevice initialization result „Éá„Éê„Ç§„ÇπÂàùÊúüÂåñÁµêÊûú\nClear expired cache entries ‚Ä¶\nInitialize CoreML with caching ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cached device status ‚Ä¶\nGet cache statistics „Ç≠„É£„ÉÉ„Ç∑„É•Áµ±Ë®à„ÇíÂèñÂæó\nGet global device cache instance ‚Ä¶\nGet global CoreML cache instance ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if device is available (with caching) ‚Ä¶\nCheck if cache entry is still valid (within 30 seconds) ‚Ä¶\nCreate new device cache ‚Ä¶\nReset initialization state (for testing) ‚Ä¶\nUpdate device cache „Éá„Éê„Ç§„Çπ„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊõ¥Êñ∞\nWarmup device cache by checking all common devices ‚Ä¶\nConstant learning rate\nCosine annealing\nDistributed training coordinator\nServer encountered an error\nExponential decay\nFault tolerance configuration\nLearning rate scheduling strategies\nLinear decay\nParameter server for distributed training\nParameter update record\nServer is running normally\nParameter server status\nServer is shutting down\nTraining configuration\nTraining performance metrics\nServer is temporarily unavailable\nWarm-up followed by decay\nApply parameter updates from all GPUs\nGet average GPU utilization\nAverage step time\nSynchronize all GPUs before critical operations\nCheckpoint frequency (in steps)\nEnable checkpointing\nCommunication overhead\nCommunication timeout\nGradient compression method\nGet efficiency ratio (useful computation vs total time)\nEnable performance profiling\nFault tolerance settings\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet GPU count\nGet training performance metrics\nGet profiling report\nGPU utilization per device\nEnable gradient accumulation on failure\nGradient synchronization time\nHandle GPU failure and recovery\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLearning rate scheduling\nMaximum retry attempts\nMemory usage per GPU\nParameter name\nCreate new distributed trainer\nCreate new parameter server\nCreate new metrics instance\nRetry delay\nSource GPU\nGet server status\nTotal training steps completed\nGradient synchronization frequency\nThroughput (samples/second)\nUpdate timestamp\nTotal training time\nExecute training step across all GPUs\nUpdate tensor\nVersion number\nElement-wise addition kernel Ë¶ÅÁ¥†„Åî„Å®Âä†ÁÆó„Ç´„Éº„Éç„É´\nConvolution kernel Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´\nGPU kernel trait for different operations ‚Ä¶\nKernel executor for managing and running kernels ‚Ä¶\nGPU kernel execution parameters ‚Ä¶\nMatrix multiplication kernel Ë°åÂàó‰πóÁÆó„Ç´„Éº„Éç„É´\nModern GPU kernel trait without generic type parameter\nBlock/workgroup size ‚Ä¶\nCompile kernel from source\nGet device „Éá„Éê„Ç§„Çπ„ÇíÂèñÂæó\nExecute the kernel „Ç´„Éº„Éç„É´„ÇíÂÆüË°å\nExecute a kernel with automatic parameter optimization ‚Ä¶\nExecute a kernel with custom parameters ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGrid size „Ç∞„É™„ÉÉ„Éâ„Çµ„Ç§„Ç∫\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSize of the convolution kernel ‚Ä¶\nLaunch kernel with given parameters\nGet kernel name\nCreate a new kernel executor ‚Ä¶\nGet optimal parameters for given problem size ‚Ä¶\nPadding applied to the input ‚Ä¶\nSet kernel parameter\nShared memory size ÂÖ±Êúâ„É°„É¢„É™„Çµ„Ç§„Ç∫\nStream/queue ID „Çπ„Éà„É™„Éº„É†/„Ç≠„É•„ÉºID\nStride of the convolution operation ‚Ä¶\nBatch matrix multiplication for multiple matrices with GPU ‚Ä¶\nGPU-accelerated Linear Algebra operations\nGPU matrix multiplication executor\nPerform batch matrix multiplication with GPU acceleration ‚Ä¶\nGet the device type being used\nReturns the argument unchanged.\nReturns the argument unchanged.\nGPU batch matrix multiplication\nGPU matrix multiplication\nGPU matrix-vector multiplication\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if GPU acceleration is available\nCreate new matrix executor with device validation\nCreate new batch matrix executor with device validation\nData transfer operations between devices ‚Ä¶\nGPU memory manager for multiple devices ‚Ä¶\nGPU memory pool for efficient allocation ‚Ä¶\nGPU memory allocation information ‚Ä¶\nMemory utilization statistics „É°„É¢„É™‰ΩøÁî®Áµ±Ë®à#[‚Ä¶\nAllocate memory from the pool with optimized alignment ‚Ä¶\nAllocate memory on specific device ‚Ä¶\nCurrently allocated size in bytes\nGet basic memory usage statistics (legacy interface) ‚Ä¶\nGet memory statistics for all devices (legacy format) ‚Ä¶\nClear all pools ÂÖ®„Éó„Éº„É´„Çí„ÇØ„É™„Ç¢\nDeallocate memory back to the pool ‚Ä¶\nDeallocate memory „É°„É¢„É™„ÇíËß£Êîæ\nGet device „Éá„Éê„Ç§„Çπ„ÇíÂèñÂæó\nDevice where memory is allocated ‚Ä¶\nCopy data between devices ‚Ä¶\nCopy data from device to host ‚Ä¶\nFragmentation ratio (lower is better)\nFree memory size in bytes\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet or create memory pool for device ‚Ä¶\nCopy data from host to device ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet current memory utilization statistics ‚Ä¶\nGet detailed memory statistics for all devices ‚Ä¶\nCreate a new GPU memory pool ‚Ä¶\nCreate a new GPU memory manager ‚Ä¶\nNumber of active allocations\nNumber of free blocks\nMemory pointer (platform-specific) ‚Ä¶\nSize in bytes „Çµ„Ç§„Ç∫Ôºà„Éê„Ç§„ÉàÔºâ\nAllocation timestamp Ââ≤„ÇäÂΩì„Å¶„Çø„Ç§„É†„Çπ„Çø„É≥„Éó\nTotal pool size in bytes\nUtilization ratio (0.0 to 1.0)\nGPU Memory Buffer Abstraction ‚Ä¶\nCPU Fallback Operations for GPU Memory ‚Ä¶\nCUDA Memory Operations CUDA„É°„É¢„É™Êìç‰Ωú\nGPU Memory Manager GPU„É°„É¢„É™„Éû„Éç„Éº„Ç∏„É£„Éº\nMetal Memory Operations Metal„É°„É¢„É™Êìç‰Ωú\nOpenCL Memory Operations OpenCL„É°„É¢„É™Êìç‰Ωú\nGPU Memory Transfer Operations GPU„É°„É¢„É™Ëª¢ÈÄÅÊìç‰Ωú\nCPU fallback\nGPU memory buffer abstraction ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if buffer is on CPU\nCheck if buffer is on CUDA device\nCheck if buffer is empty\nCheck if buffer is on Metal device\nCheck if buffer is on OpenCL device\nGet buffer size (number of elements)\nCPU fallback operations for GPU memory operations ‚Ä¶\nExecute attention mechanism using CPU fallback ‚Ä¶\nExecute batch normalization using CPU fallback ‚Ä¶\nExecute element-wise operation using CPU fallback ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStub for CUDA operations when CUDA is not available ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nGPU memory manager for tensor operations ‚Ä¶\nExecute attention operation (CPU fallback)\nExecute batch normalization on GPU buffer (CPU fallback)\nExecute element-wise operation on GPU buffers (CPU ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate new GPU memory manager (CPU fallback) ‚Ä¶\nTransfer data from GPU to CPU (fallback implementation)\nTransfer tensor from CPU to GPU (fallback implementation)\nStub for Metal operations when Metal is not available ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStub for OpenCL operations when OpenCL is not available ‚Ä¶\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nBatch normalization operations „Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñÊºîÁÆó\nConvolution operations Áï≥„ÅøËæº„ÅøÊºîÁÆó\nElement-wise operations (add, mul, etc.) ‚Ä¶\nMatrix multiplication operations Ë°åÂàó‰πóÁÆóÊºîÁÆó\nMetal buffer wrapper Metal„Éê„ÉÉ„Éï„Ç°„É©„ÉÉ„Éë„Éº\nNon-Metal fallback executor for compatibility ‚Ä¶\nMetal kernel parameters Metal„Ç´„Éº„Éç„É´„Éë„É©„É°„Éº„Çø\nMetal kernel types Metal„Ç´„Éº„Éç„É´„Çø„Ç§„Éó\nReduction operations (sum, mean, etc.) ‚Ä¶\nCopy data from host to Metal buffer ‚Ä¶\nCopy data from Metal buffer to host ‚Ä¶\nPerform element-wise addition using Metal ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nPerform matrix multiplication using Metal ‚Ä¶\nExecute Metal 2D convolution Metal 2DÁï≥„ÅøËæº„Åø„ÇíÂÆüË°å\nExecute Metal element-wise addition ‚Ä¶\nPublic interface functions for Metal operations ‚Ä¶\nExecute Metal reduction sum ‚Ä¶\nCreate a new Metal buffer with specified size ‚Ä¶\nCreate a new Metal kernel executor (fallback ‚Ä¶\nPerform reduction sum using Metal ‚Ä¶\nThreadgroups per grid for Metal kernel execution ‚Ä¶\nThreads per threadgroup for Metal kernel execution ‚Ä¶\nGradient aggregation methods\nAll-reduce algorithms\nSimple average\nBackward pass for micro-batch\nLoad balancing strategies\nBatch splitting strategy\nButterfly all-reduce\nColumn-wise partitioning\nCommunication backend types\nCommunication optimization settings\nCommunication group for collective operations\nCommunication manager for GPU-to-GPU transfers\nData parallelism - replicate model, split data\nData parallel trainer\nDelayed aggregation\nDouble binary tree\nDynamic based on current load\nDynamic based on processing speed\nError feedback compression\nEven split across GPUs\nExpert parallelism - for mixture of experts\nForward pass for micro-batch\nGPipe schedule\nGPU topology information\nGradient aggregation handler\nGradient compression methods\nHierarchical aggregation\nHost-staged transfers\nHybrid parallelism - combination of strategies\nHybrid partitioning\nInterleaved 1F1B\nLayer-wise partitioning\nLoad balancer for work distribution\nModel parallelism - split model across GPUs\nModel parallel partitioner\nMulti-GPU context manager\nNVIDIA NCCL\n1F1B (one forward one backward)\nDirect peer-to-peer\nMulti-GPU parallelism strategies\nModel partitioning strategies\nPipeDream schedule\nPipeline operation type\nPipeline parallelism - pipeline stages across GPUs\nPipeline scheduling algorithms\nPipeline parallel scheduler\nPredictive based on history\nQuantization\nAMD RCCL\nRandom sparsification\nRing all-reduce\nRow-wise partitioning\nSplit strategies for data parallelism\nStatic even distribution\nTensor-wise partitioning\nTop-K sparsification\nTransfer statistics\nTree all-reduce\nWeighted by GPU capability\nWeighted average\nWork stealing\nIntel XeLink\nAggregate gradients across GPUs\nAll-reduce operation across GPUs\nAll-reduce operation\nAverage bandwidth achieved\nNVLink/Interconnect bandwidth matrix (GB/s)\nBroadcast tensor from root GPU\nBroadcast operation\nEnable compression\nGPU compute capabilities\nCreate communication group\nGPU device IDs\nExecute operation across multiple GPUs\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nEnable fusion of small transfers\nGather tensors from all GPUs\nGather operation\nGenerate pipeline schedule\nGet device IDs\nGet GPU assignment for layer\nGet GPU IDs (alias for device IDs)\nGet recommended GPU for next operation\nGet number of GPUs\nParticipating GPU IDs\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if GPU is available\nAvailable memory per GPU\nGroup name\nCheck if rebalancing is needed\nSimple constructor for testing\nCreate new communication manager\nCreate new load balancer\nCreate new data parallel trainer\nCreate new model parallel partitioner\nCreate new pipeline scheduler\nConstructor with strategy (renamed from original new)\nNumber of GPUs\nEnable overlap of computation and communication\nPeer-to-peer connectivity matrix\nPartition model across GPUs\nPeak bandwidth achieved\nGroup rank mapping\nRing buffer size for async operations\nRoot rank for broadcast operations\nScatter tensors across GPUs\nScatter operation\nSplit batch across GPUs\nTest P2P communication between two GPUs\nTotal bytes transferred\nTotal transfers\nUpdate load for GPU\nBatch size optimization\nTypes of performance bottlenecks\nCommunication bound\nCommunication performance metrics\nImprove communication patterns\nGPU compute bound\nGPU-specific performance metrics\nHardware configuration\nLoad balancing adjustment\nLoad imbalance\nMemory bandwidth bound\nMemory fragmentation\nOptimize memory usage\nMulti-GPU performance profiler\nOptimization recommendations\nAdjust parallelism strategy\nPerformance bottleneck identification\nPerformance analysis report\nPerformance snapshot for trend analysis\nPerformance trends analysis\nTypes of optimization recommendations\nSynchronization bound\nTraining session performance metrics\nSpecific actions to take\nAffected components\nAll-reduce operation times\nAnalyze performance and generate report\nAverage step time\nAverage step time\nBackward pass times\nNetwork bandwidth utilization\nBarrier synchronization times\nBottleneck type\nBottleneck identification\nBroadcast operation times\nCommunication metrics snapshot\nCommunication efficiency\nCommunication overhead ratio\nCommunication overhead trend\nImplementation complexity (1-10)\nCompute utilization percentage\nDescription\nDescription\nGPU device ID\nDisable profiling\nGPU efficiency score\nEnable profiling\nExpected performance gain percentage\nExport profiling data for external analysis\nForward pass times\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate comprehensive performance report\nGPU efficiency scores\nGPU metrics snapshot\nGPU utilization trend\nGradient synchronization times\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nKernel execution times\nMemory bandwidth utilization\nMemory efficiency trend\nMemory utilization percentage\nCreate new multi-GPU profiler\nOverall performance score (0-100)\nCommunication overhead percentage\nP2P transfer times\nImpact on overall performance\nPower consumption in watts\nPriority level (1-10)\nConvenience function to profile multi-GPU operations\nRecommendation type\nOptimization recommendations\nRecord communication operation\nRecord GPU kernel execution\nRecord training step metrics\nSession duration\nSeverity (0-100)\nStart profiling a multi-GPU operation\nStream synchronization times\nTemperature in Celsius\nThroughput (samples per second)\nThroughput trend (samples/sec over time)\nTimestamp\nTotal training steps\nTotal training steps\nTraining metrics snapshot\nMemory transfer times\nPerformance trends\nParameter update times\nBatch normalization operations „Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñÊºîÁÆó\nConvolution operations Áï≥„ÅøËæº„ÅøÊºîÁÆó\nElement-wise operations (add, mul, etc.) ‚Ä¶\nMatrix multiplication operations Ë°åÂàó‰πóÁÆóÊºîÁÆó\nOpenCL buffer wrapper OpenCL„Éê„ÉÉ„Éï„Ç°„É©„ÉÉ„Éë„Éº\nNon-OpenCL fallback executor for compatibility ‚Ä¶\nOpenCL kernel parameters OpenCL„Ç´„Éº„Éç„É´„Éë„É©„É°„Éº„Çø#‚Ä¶\nOpenCL kernel types OpenCL„Ç´„Éº„Éç„É´„Çø„Ç§„Éó\nReduction operations (sum, mean, etc.) ‚Ä¶\nCopy buffer data to host memory (fallback when OpenCL not ‚Ä¶\nPerform elementwise addition on f32 arrays (fallback) ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate buffer from host data (fallback when OpenCL not ‚Ä¶\nGlobal work size for OpenCL kernel execution ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLocal work size for OpenCL kernel execution ‚Ä¶\nPerform matrix multiplication on f32 arrays (fallback) ‚Ä¶\nCreate a new OpenCL buffer (fallback when OpenCL not ‚Ä¶\nCreate a new OpenCL kernel executor (fallback when OpenCL ‚Ä¶\nExecute OpenCL element-wise addition ‚Ä¶\nPublic interface functions for OpenCL operations ‚Ä¶\nExecute OpenCL reduction sum ‚Ä¶\nQueue index for OpenCL command queue ‚Ä¶\nPerform reduction sum on f32 array (fallback) ‚Ä¶\nGet buffer size „Éê„ÉÉ„Éï„Ç°„Çµ„Ç§„Ç∫„ÇíÂèñÂæó\nOpenCL device information for optimization selection ‚Ä¶\nNon-OpenCL fallback implementation ‚Ä¶\nCompute units (cores) Ë®àÁÆó„É¶„Éã„ÉÉ„ÉàÔºà„Ç≥„Ç¢Ôºâ\nDevice type (CPU/GPU/Accelerator) ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nGlobal memory size in bytes ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLocal memory size in bytes ‚Ä¶\nPerform matrix multiplication using OpenCL with ‚Ä¶\nMaximum clock frequency in MHz ‚Ä¶\nMaximum work group size ‚Ä¶\nDevice name „Éá„Éê„Ç§„ÇπÂêç\nCreate a new OpenCL matrix executor with automatic device ‚Ä¶\nPublic interface functions for OpenCL operations ‚Ä¶\nVendor (AMD, Intel, NVIDIA) „Éô„É≥„ÉÄ„Éº (AMD, Intel, ‚Ä¶\nBenchmark configuration „Éô„É≥„ÉÅ„Éû„Éº„ÇØË®≠ÂÆö#[‚Ä¶\nBenchmark result for a single operation ‚Ä¶\nPerformance benchmark suite for GPU operations ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum benchmark duration in milliseconds ‚Ä¶\nEnable FLOPS measurements FLOPSÊ∏¨ÂÆö„ÇíÊúâÂäπÂåñ\nEnable memory bandwidth measurements ‚Ä¶\nNumber of measurement iterations Ê∏¨ÂÆöÂèçÂæ©ÂõûÊï∞\nMinimum benchmark duration in milliseconds ‚Ä¶\nCreate a new benchmark result ‚Ä¶\nCreate a new performance benchmark ‚Ä¶\nRun comprehensive benchmark suite ‚Ä¶\nNumber of warmup iterations ‚Ä¶\nAdd CPU timing information to the benchmark result ‚Ä¶\nAdd error information to the benchmark result ‚Ä¶\nAdd FLOPS count to the benchmark result ‚Ä¶\nAdd GPU timing information to the benchmark result ‚Ä¶\nAdd memory bytes to the benchmark result ‚Ä¶\nPerformance profiler for device capabilities ‚Ä¶\nOptimization strategies for different scenarios ‚Ä¶\nPerformance benchmarking utilities ‚Ä¶\nPerformance metrics for GPU operations ‚Ä¶\nPerformance optimizer for GPU operations ‚Ä¶\nPerformance statistics summary ‚Ä¶\nDevice thermal state monitoring „Éá„Éê„Ç§„ÇπÁÜ±Áä∂ÊÖãÁõ£Ë¶ñ\nBenchmark operation on different devices ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance statistics ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new performance optimizer ‚Ä¶\nRecord performance metrics for an operation ‚Ä¶\nRun comprehensive performance suite ‚Ä¶\nSelect optimal device for a given operation ‚Ä¶\nSet optimization strategy ÊúÄÈÅ©ÂåñÊà¶Áï•„ÇíË®≠ÂÆö\nUpdate device profile with new measurements ‚Ä¶\nGPU reduction operations trait\nGPU reduction executor\nMaximum value reduction\nMean (average) reduction\nMinimum value reduction\nProduct reduction\nReduction operation types\nStandard deviation reduction\nSum reduction\nVariance reduction\nReturns the argument unchanged.\nReturns the argument unchanged.\nGPU max reduction\nGPU mean reduction\nGPU min reduction\nGPU standard deviation\nGPU sum reduction\nGPU variance\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new GPU reduction executor\nPerform reduction operation on GPU\nBenchmark CPU vs Metal performance ‚Ä¶\nTest Metal GPU availability and basic functionality Metal ‚Ä¶\nTest basic tensor operations with Metal backend ‚Ä¶\nDevice selection thresholds „Éá„Éê„Ç§„ÇπÈÅ∏ÊäûÈñæÂÄ§\nOperation characteristics for smart selection ‚Ä¶\nOperation type for device selection ‚Ä¶\nSmart device selector with operation-specific logic ‚Ä¶\nMaximum tensor size for CoreML (elements) ‚Ä¶\nMinimum tensor size for CoreML (elements) ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet fallback chain for operation ‚Ä¶\nMinimum memory footprint for GPU operations (bytes) ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalculate memory footprint in bytes ‚Ä¶\nMinimum tensor size for Metal GPU (elements) Metal ‚Ä¶\nSelect optimal device for the given operation ‚Ä¶\nSynchronization barrier\nEvent recording\nWait for event\nGPU event for synchronization\nGPU stream for asynchronous operations\nCompute kernel execution\nMemory copy operation\nMulti-GPU barrier for synchronization\nStream manager for coordinating GPU operations\nStream operations\nStream priority levels\nCompletion status\nCompletion notifier\nCreate new event\nCreate new stream on device\nCreation timestamp\nGPU device ID\nDevice ID\nAssociated events\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nEvent ID\nStream ID\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new multi-GPU barrier\nCreate new stream manager\nOperation queue\nPriority level\nQuery event status\nRecord event on stream\nReset barrier state\nSynchronize all streams on device\nWait for all GPUs to reach barrier\nWait for event completion\nElement-wise addition Ë¶ÅÁ¥†„Åî„Å®„ÅÆÂä†ÁÆó\nBatch normalization „Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñ\n2D convolution 2DÁï≥„ÅøËæº„Åø\nElement-wise division Ë¶ÅÁ¥†„Åî„Å®„ÅÆÈô§ÁÆó\nKernel performance metrics ‚Ä¶\nUnified kernel operation types ‚Ä¶\nKernel execution parameters ‚Ä¶\nSimplified kernel selector Á∞°ÊΩî„Å™„Ç´„Éº„Éç„É´ÈÅ∏ÊäûÂô®\nMatrix multiplication Ë°åÂàó‰πóÁÆó\nElement-wise multiplication Ë¶ÅÁ¥†„Åî„Å®„ÅÆ‰πóÁÆó\nReLU activation ReLUÊ¥ªÊÄßÂåñ\nReduction mean „É™„ÉÄ„ÇØ„Ç∑„Éß„É≥Âπ≥Âùá\nReduction sum „É™„ÉÄ„ÇØ„Ç∑„Éß„É≥ÂêàË®à\nSoftmax activation SoftmaxÊ¥ªÊÄßÂåñ\nElement-wise subtraction Ë¶ÅÁ¥†„Åî„Å®„ÅÆÊ∏õÁÆó\nUnified kernel executor Áµ±‰∏Ä„Ç´„Éº„Éç„É´ÂÆüË°åËÄÖ\nAdd executor ÂÆüË°åËÄÖ„ÇíËøΩÂä†\nGet available devices Âà©Áî®ÂèØËÉΩ„Éá„Éê„Ç§„Çπ„ÇíÂèñÂæó\nGet device type „Éá„Éê„Ç§„Çπ„Çø„Ç§„Éó„ÇíÂèñÂæó\nExecute kernel operation on f32 tensors ‚Ä¶\nExecute operation with best executor ‚Ä¶\nExecute kernel operation on f64 tensors ‚Ä¶\nExecute operation with best executor ‚Ä¶\nKernel execution time „Ç´„Éº„Éç„É´ÂÆüË°åÊôÇÈñì\nAdditional kernel parameters ‚Ä¶\nFloating point operations per second ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance metrics ‚Ä¶\nInput tensor shapes for the kernel ‚Ä¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMemory bandwidth utilization (GB/s) ‚Ä¶\nCreate new unified kernel executor ‚Ä¶\nCreate new kernel selector ‚Ä¶\nGPU occupancy percentage GPUÂç†ÊúâÁéá„Éë„Éº„Çª„É≥„Éà\nOutput tensor shape Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ÂΩ¢Áä∂\nSelect best executor for operation ‚Ä¶\nCheck if operation is supported ‚Ä¶\nGPU kernel validator GPU„Ç´„Éº„Éç„É´Ê§úË®ºÂô®\nValidation results for GPU operations ‚Ä¶\nThe GPU device type used for validation\nError message if validation failed\nExecution time in milliseconds\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate a validation report Ê§úË®º„É¨„Éù„Éº„Éà„ÇíÁîüÊàê\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum error between expected and actual results\nCreate a new GPU validator Êñ∞„Åó„ÅÑGPUÊ§úË®ºÂô®„Çí‰ΩúÊàê\nName of the operation being validated\nWhether the validation passed\nPrint GPU validation report GPUÊ§úË®º„É¨„Éù„Éº„Éà„ÇíÂá∫Âäõ\nRun comprehensive GPU validation ‚Ä¶\nValidate all available GPU devices ‚Ä¶\nValidate element-wise addition operation ‚Ä¶\nValidate matrix multiplication operation ‚Ä¶\nValidate memory operations (allocation, copy, deallocation)\nTest tolerance for double precision floating point ‚Ä¶\nTest tolerance for floating point comparisons ‚Ä¶\nTest result structure „ÉÜ„Çπ„ÉàÁµêÊûúÊßãÈÄ†‰Ωì\nVerification test suite for GPU operations ‚Ä¶\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new verification test suite ‚Ä¶\nCreate a new verification result ‚Ä¶\nRun all verification tests ‚Ä¶\nAdd error metrics to the verification result ‚Ä¶\nMark the verification as failed with error message ‚Ä¶\nAdd timing information to the verification result ‚Ä¶\nHigh-Performance BLAS Integration for RusTorch ‚Ä¶\nBenchmark different matrix multiplication implementations ‚Ä¶\nMulti-threaded matrix multiplication using Rayon ‚Ä¶\nFallback implementation when BLAS is not available ‚Ä¶\nComprehensive Memory Management System ‚Ä¶")