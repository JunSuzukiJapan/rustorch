searchState.loadedDescShard("rustorch", 0, "RusTorch ğŸš€\nAutomatic Mixed Precision (AMP) training support â€¦\nMacro for autocast-aware operations\nAutomatic differentiation module â€¦\nUnified compute backend abstraction layer â€¦\nCommon utilities and shared functionality â€¦\nPyTorch to RusTorch conversion system â€¦\nData loading and processing utilities (Phase 5 API) â€¦\nDebug and logging system ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  â€¦\nMacro for easy diagnostic context creation\nDistributed training support for multi-GPU and â€¦\nMacro for creating distributed errors easily â€¦\nStatistical distributions module providing â€¦\nData types for tensors ãƒ†ãƒ³ã‚½ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿å‹\nUnified error handling system â€¦\nMacro for creating error context with file location â€¦\nDynamic execution engine for runtime graph optimization â€¦\nModel format support and conversion utilities â€¦\nGPU acceleration support (CUDA, Metal, OpenCL) â€¦\nMacro for creating GPU errors easily â€¦\nHigh-performance linear algebra with BLAS integration â€¦\nConvenience macro for structured logging\nMemory management and pooling utilities â€¦\nModel import functionality for PyTorch and ONNX models â€¦\nPre-built models and architectures â€¦\nNeural network layers and building blocks â€¦\nOptimization algorithms æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \nCross-platform optimization module â€¦\nParallel processing utilities â€¦\nMacro for performance timing\nRe-exports of commonly used items\nProfile a code block â€¦\nProfile a function é–¢æ•°ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«\nPerformance profiler â€¦\nQuantization support for model compression and â€¦\nSerialization and model I/O system (Phase 9) â€¦\nConvenience macro for chaining shape operations â€¦\nSIMD vectorized operations for performance optimization â€¦\nSparse tensor support and operations (Phase 12) â€¦\nSpecial mathematical functions (gamma, Bessel, error â€¦\nMacro for easy TensorBoard logging â€¦\nTensor operations and data structures â€¦\nConvenient macro for creating tensors with literal syntax\nHelper macros for error creation â€¦\nCreates an N-dimensional tensor with compile-time shape â€¦\nTensorBoard integration TensorBoardçµ±åˆ TensorBoard â€¦\nMacro for timed code blocks\nConvenience macro for tracking allocations\nTraining loop abstractions and utilities â€¦\nUtility functions ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° Utility â€¦\nData validation and quality assurance system â€¦\nComputer vision module providing image transforms, data â€¦\nVisualization tools for plots, graphs, and data analysis â€¦\nMacro for adding context to results â€¦\nGlobal AMP configuration ã‚°ãƒ­ãƒ¼ãƒãƒ«AMPè¨­å®š\nAMP-aware optimizer wrapper that handles mixed precision â€¦\nAutocast context manager\nAutocast mode for mixed precision training â€¦\nUse BF16 for reduced precision\nUse FP16 for reduced precision\nGradient scaler for automatic mixed precision training\nStep skipped due to inf/nan in gradients\nMixed precision tensor operations\nNo autocasting\nStep skipped due to gradient overflow\nParameter group configuration for AMP\nState of the gradient scaler\nStatistics about gradient scaling â€¦\nResult of a gradient scaling step\nStep completed successfully\nTraining statistics for AMP optimization â€¦\nGradually adjust growth interval based on overflow â€¦\nAdd a parameter group with specific AMP settings\nCreate an autocast context\nBackoff factor for loss scale\nBackoff factor for scale\nBackoff factor when overflow occurs â€¦\nCreate config for BF16\nCheck if tensor can be safely cast to target dtype\nCast BF16 tensor to FP32 (passthrough for compatibility)\nGeneric tensor casting function\nCast tensor to target dtype\nCast tensor to BF16 (simulated - converts to BF16 and back â€¦\nCast tensor to FP16 (simulated - converts to FP16 and back â€¦\nCast tensor to FP32 (passthrough for compatibility)\nCheck for gradient overflow/underflow\nWhether to use gradient clipping for this group\nNumber of consecutive non-overflowed iterations\nNumber of consecutive non-overflow steps â€¦\nCurrent scale factor ç¾åœ¨ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°\nCreate a default scaler\nDisable AMP globally\nPreferred reduced precision dtype (FP16 or BF16)\nWhether to use dynamic loss scaling\nEnable AMP globally\nWhether to enable autocast\nWhether dynamic scaling is enabled\nWhether scaling is enabled â€¦\nEnter the autocast context\nExit the autocast context\nCreate config for FP16 with static scaling\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet current AMP configuration\nGet recommended actions based on statistics\nGet current scale\nGet detailed statistics about scaling\nGet training statistics\nGrowth factor for loss scale\nGrowth factor for scale\nGrowth factor for scale updates â€¦\nGrowth interval\nNumber of iterations between scale updates\nInterval between growth updates æˆé•·æ›´æ–°ã®é–“éš”\nNumber of iterations since last scale update\nCurrent growth tracking counter â€¦\nWhether overflow was detected â€¦\nInitial loss scale\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if AMP is enabled\nCheck if scaling is enabled\nCheck if training is stable (low overflow rate)\nLoad scaler state\nMaximum gradient norm for clipping\nCast tensor to autocast dtype if enabled\nGet memory footprint in bytes\nGet memory footprint for target dtype\nCreate a new autocast context\nCreate a new gradient scaler\nCreate a new AMP optimizer wrapper\nGet the underlying optimizer\nGet mutable reference to the underlying optimizer\nNumber of overflow occurrences â€¦\nRate of overflow occurrences ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ç™ºç”Ÿç‡\nParameter IDs in this group\nReset scaler state (useful after loading checkpoints)\nReset optimizer state\nScale the loss\nCurrent scale factor\nScale tensor directly\nGet the gradient scaler\nGet mutable reference to the gradient scaler\nGradient scaler statistics å‹¾é…ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼çµ±è¨ˆ\nEnable or disable scaling\nSet scale manually\nSet scale bounds\nGet scaler state\nPerform optimizer step with scaling\nPerform an optimization step with AMP\nAdvanced step with gradient clipping and inf/nan detection\nGet success rate\nNumber of successful steps æˆåŠŸã—ãŸã‚¹ãƒ†ãƒƒãƒ—æ•°\nTotal number of optimization steps â€¦\nUnscale gradients\nUpdate the scale factor\nUpdate learning rate schedule and adaptive scaling\nWhether to use mixed precision for this group\nMixed precision training utilities\nZero gradients (delegate to underlying optimizer)\nGradient norm before clipping (if clipping was applied)\nNew scale factor after backoff\nScale factor used\nCurrent scale factor\nCurrent scale factor\nGet optimal dtype for current hardware\nCheck if hardware supports BF16\nCheck if hardware supports FP16\nCheck if operation should use reduced precision\nGradient function trait for backward computation â€¦\nA variable that supports automatic differentiation. â€¦\nApply the gradient function to compute input gradients â€¦\nBatch matrix multiplication for 4D tensors (batch_size, â€¦\nPerforms backward pass to compute gradients. â€¦\nPerforms backward pass with a specific gradient. â€¦\nGradient context management for automatic differentiation â€¦\nReturns the data tensor. â€¦\nReturns the argument unchanged.\nAutomatic differentiation functions è‡ªå‹•å¾®åˆ†é–¢æ•°\nReturns the gradient tensor. â€¦\nGradient functions for automatic differentiation â€¦\nReturns the gradient function if any. â€¦\nAdvanced gradient computation utilities â€¦\nGradient checking utilities for numerical validation â€¦\nComputation graph for automatic differentiation â€¦\nHigher-order derivatives: Jacobian and Hessian computation â€¦\nReturns the unique identifier for this Variable â€¦\nCalls <code>U::from(self)</code>.\nLinear layer gradient functions â€¦\nMatrix multiplication with automatic differentiation â€¦\nCompute the mean of all elements with proper gradient â€¦\nMean of all elements with automatic differentiation support\nCreates a new variable with the given tensor. â€¦\nCreates a new variable with gradient function â€¦\nPower function with automatic differentiation support â€¦\nReturns whether this variable requires gradients. â€¦\nSum all elements with automatic differentiation support â€¦\nSum along a specific dimension (simplified implementation) â€¦\nTranspose the last two dimensions æœ€å¾Œã®2æ¬¡å…ƒã‚’è»¢ç½®\nGradient flow visualization for computational graphs â€¦\nZeros out the gradient. â€¦\nRAII guard for temporarily enabling anomaly detection â€¦\nRAII guard for temporarily enabling gradient computation â€¦\nGlobal gradient context state â€¦\nRAII guard for temporarily disabling gradient computation â€¦\nConvenience function for anomaly detection context â€¦\nConvenience function for enable_grad context â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if anomaly detection is enabled â€¦\nCheck if gradient computation is currently enabled â€¦\nCreate a new NoGradGuard, disabling gradient computation â€¦\nCreate a new EnableGradGuard, enabling gradient computation\nCreate a new AnomalyDetectionGuard, enabling anomaly â€¦\nConvenience function for no_grad context â€¦\nSet anomaly detection state ç•°å¸¸æ¤œå‡ºçŠ¶æ…‹ã‚’è¨­å®š\nSet gradient computation state å‹¾é…è¨ˆç®—çŠ¶æ…‹ã‚’è¨­å®š\nAddition function åŠ ç®—é–¢æ•°\nFunction trait for automatic differentiation â€¦\nMatrix multiplication function è¡Œåˆ—ä¹—ç®—é–¢æ•°\nMultiplication function ä¹—ç®—é–¢æ•°\nSubtraction function æ¸›ç®—é–¢æ•°\nSum function ç·å’Œé–¢æ•°\nApply the backward pass é€†ä¼æ’­ã‚’é©ç”¨\nApply the forward pass é †ä¼æ’­ã‚’é©ç”¨\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nAddition backward function åŠ ç®—ã®é€†ä¼æ’­é–¢æ•°\nMatrix multiplication backward function â€¦\nMean backward function å¹³å‡ã®é€†ä¼æ’­é–¢æ•°\nMultiplication backward function ä¹—ç®—ã®é€†ä¼æ’­é–¢æ•°\nSubtraction backward function æ¸›ç®—ã®é€†ä¼æ’­é–¢æ•°\nSum backward function ç·å’Œã®é€†ä¼æ’­é–¢æ•°\nPhantom data for type parameter â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nFirst input tensor data â€¦\nFirst input tensor data â€¦\nFirst input tensor data for gradient computation â€¦\nFirst input tensor data for gradient computation â€¦\nFirst input variable æœ€åˆã®å…¥åŠ›å¤‰æ•°\nFirst input variable æœ€åˆã®å…¥åŠ›å¤‰æ•°\nFirst input variable æœ€åˆã®å…¥åŠ›å¤‰æ•°\nFirst input variable æœ€åˆã®å…¥åŠ›å¤‰æ•°\nSecond input tensor data â€¦\nSecond input tensor data â€¦\nSecond input tensor data for gradient computation â€¦\nSecond input tensor data for gradient computation â€¦\nSecond input variable 2ç•ªç›®ã®å…¥åŠ›å¤‰æ•°\nSecond input variable 2ç•ªç›®ã®å…¥åŠ›å¤‰æ•°\nSecond input variable 2ç•ªç›®ã®å…¥åŠ›å¤‰æ•°\nSecond input variable 2ç•ªç›®ã®å…¥åŠ›å¤‰æ•°\nOriginal input tensor shape for gradient broadcasting â€¦\nInput variable for gradient propagation â€¦\nInput variable å…¥åŠ›å¤‰æ•°\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nNumber of elements è¦ç´ æ•°\nAutomatic differentiation error è‡ªå‹•å¾®åˆ†ã‚¨ãƒ©ãƒ¼\nBackend not available ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒåˆ©ç”¨ä¸å¯\nData loading error ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼\nDataLoader system errors (Phase 5) â€¦\nDebug system and logging errors â€¦\nDevice operation failed ãƒ‡ãƒã‚¤ã‚¹æ“ä½œå¤±æ•—\nDistributed computing errors â€¦\nGPU-specific errors GPUå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nType alias for gradient errors using unified RusTorchError â€¦\nInput/Output and serialization errors â€¦\nImport-specific error ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nInvalid operation ç„¡åŠ¹ãªæ“ä½œ\nInvalid operation parameters ç„¡åŠ¹ãªæ“ä½œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nKernel compilation error for GPU operations â€¦\nMemory allocation failed ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦å¤±æ•—\nModel import/export error â€¦\nNeural network layer error â€¦\nFeature not implemented yet æ©Ÿèƒ½æœªå®Ÿè£…\nOut of memory error ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼\nProfiling and performance analysis errors â€¦\nSerialization error ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼\nSerialization and model I/O errors (Phase 9) â€¦\nShape mismatch between tensors â€¦\nTensor operation failed ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œå¤±æ•—\nData validation and quality assurance errors â€¦\nVision processing errors è¦–è¦šå‡¦ç†ã‚¨ãƒ©ãƒ¼\nVisualization errors å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼\nCompute gradients of outputs with respect to inputs â€¦\nCompute the gradient of a scalar function â€¦\nUtility function to check if a variable is in the â€¦\nValidate gradient computation setup â€¦\nActual tensor shape that was provided â€¦\nAvailable memory in bytes â€¦\nName of the unavailable backend â€¦\nDevice identifier where the error occurred â€¦\nDevice where allocation failed â€¦\nExpected tensor shape æœŸå¾…ã•ã‚Œã‚‹ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nFeature that is not yet implemented æœªå®Ÿè£…ã®æ©Ÿèƒ½\nName of the neural network layer â€¦\nError message describing the tensor operation failure â€¦\nError message describing the device issue â€¦\nDescription of the parameter issue â€¦\nDetailed error message è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\nError message from the layer â€¦\nAutograd error description è‡ªå‹•å¾®åˆ†ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nModel I/O error description ãƒ¢ãƒ‡ãƒ«I/Oã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nImport operation error message â€¦\nData loading error description â€¦\nGPU operation error description GPUæ“ä½œã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nVision processing error description â€¦\nDistributed computing error description â€¦\nVisualization error description å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nProfiling error description â€¦\nData validation error description â€¦\nDebug system error description â€¦\nKernel compilation error description â€¦\nDataLoader error description DataLoaderã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nSerialization error description â€¦\nSerialization error description â€¦\nName of the operation with invalid parameters â€¦\nName of the invalid operation ç„¡åŠ¹ãªæ“ä½œå\nOperation that caused the serialization error â€¦\nRequested memory in bytes â€¦\nSize of the failed allocation in bytes â€¦\nOptional underlying error cause â€¦\nOptional underlying I/O error â€¦\nOptional underlying import error â€¦\nOptional underlying data loading error â€¦\nOptional underlying GPU error â€¦\nOptional underlying vision error â€¦\nOptional underlying distributed error â€¦\nOptional underlying visualization error â€¦\nGradient checking configuration å‹¾é…ãƒã‚§ãƒƒã‚¯è¨­å®š\nResult of gradient checking å‹¾é…ãƒã‚§ãƒƒã‚¯ã®çµæœ\nReturns the argument unchanged.\nReturns the argument unchanged.\nCheck gradients using finite differences â€¦\nSimplified gradient checking function with default â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nComputation graph for automatic differentiation â€¦\nNode in the computation graph è¨ˆç®—ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰\nAccumulate gradient for this node â€¦\nAdd a function node to the graph â€¦\nAdd a leaf node to the graph â€¦\nPerform backward pass from the given node â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nFunction that created this node â€¦\nGet a node by ID IDã«ã‚ˆã£ã¦ãƒãƒ¼ãƒ‰ã‚’å–å¾—\nGet a mutable node by ID â€¦\nGradient accumulator å‹¾é…ã‚¢ã‚­ãƒ¥ãƒ ãƒ¬ãƒ¼ã‚¿\nInput tensors (kept for backward pass) â€¦\nInput nodes å…¥åŠ›ãƒãƒ¼ãƒ‰\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new computation graph â€¦\nCreate a new function node â€¦\nCreate a new leaf node (no function, no inputs) â€¦\nWhether this node requires gradient computation â€¦\nClear gradient for this node â€¦\nCompute the Hessian matrix of a scalar-valued function â€¦\nCompute Hessian-vector product (HVP) efficiently â€¦\nCompute the Jacobian matrix of a vector-valued function â€¦\nLinear layer backward function â€¦\nBias variable for gradient propagation (optional) â€¦\nReturns the argument unchanged.\nInput tensor for gradient computation â€¦\nInput variable for gradient propagation â€¦\nCalls <code>U::from(self)</code>.\nWeight tensor for gradient computation â€¦\nWeight variable for gradient propagation â€¦\nGradient is decreasing\nDisconnected parameter (no gradient) â€¦\nGradient is exploding\nExploding gradient detected å‹¾é…çˆ†ç™ºãŒæ¤œå‡ºã•ã‚ŒãŸ\nInteractive gradient flow analyzer â€¦\nGradient flow issues detected â€¦\nSummary of gradient flow statistics â€¦\nGradient flow visualizer â€¦\nGradient trend analysis result â€¦\nGradient is increasing\nInput variable å…¥åŠ›å¤‰æ•°\nNode information for visualization â€¦\nNode types in the computation graph â€¦\nOperation node æ¼”ç®—ãƒãƒ¼ãƒ‰\nLoss/output node æå¤±/å‡ºåŠ›ãƒãƒ¼ãƒ‰\nParameter (trainable) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¨“ç·´å¯èƒ½ï¼‰\nGradient is stable\nGradient is vanishing\nVanishing gradient detected å‹¾é…æ¶ˆå¤±ãŒæ¤œå‡ºã•ã‚ŒãŸ\nAdd an operation node æ¼”ç®—ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ \nAnalyze gradient trends å‹¾é…ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ\nAverage gradient norm å¹³å‡å‹¾é…ãƒãƒ«ãƒ \nClear the visualizer for reuse â€¦\nClear history å±¥æ­´ã‚’ã‚¯ãƒªã‚¢\nDetect potential gradient flow issues â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet gradient history for a parameter â€¦\nGenerate a summary of gradient flow statistics â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum gradient norm æœ€å¤§å‹¾é…ãƒãƒ«ãƒ \nMinimum gradient norm æœ€å°å‹¾é…ãƒãƒ«ãƒ \nCreate a new gradient flow visualizer â€¦\nCreate a new gradient flow analyzer â€¦\nNumber of nodes with gradients â€¦\nNumber of parameter nodes ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¼ãƒ‰ã®æ•°\nRecord gradient norm for a parameter â€¦\nSave visualization to file â€¦\nGenerate DOT format for Graphviz â€¦\nTotal number of edges ã‚¨ãƒƒã‚¸ã®ç·æ•°\nTotal number of nodes ãƒãƒ¼ãƒ‰ã®ç·æ•°\nTrace gradient flow from a variable â€¦\nGradient norm value\nGradient norm value\nNode label\nNode label\nNode label\nBackend factory for creating and managing compute backends â€¦\nResult type for backend operations (now unified) â€¦\nCore compute backend trait that all backends must implement\nParameters for convolution operations â€¦\nCPU computation CPUè¨ˆç®—\nNVIDIA CUDA GPU NVIDIA CUDA GPU\nDevice memory buffer abstraction â€¦\nDevice information and capabilities â€¦\nTypes of compute devices è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ã®ç¨®é¡\nContains the error value\nApple Metal GPU Apple Metal GPU\nContains the success value\nOpenCL compatible device OpenCLäº’æ›ãƒ‡ãƒã‚¤ã‚¹\nElement-wise addition: a + b è¦ç´ ã”ã¨åŠ ç®—: a + b\nAllocate memory on the device â€¦\nGet raw pointer for device operations (unsafe) â€¦\nGet all available backends on the current system â€¦\nAvailable memory in bytes â€¦\nGet backend-specific context for advanced operations â€¦\nBatch normalization ãƒãƒƒãƒæ­£è¦åŒ–\nUnified Compute Backend Abstraction Layer\nConvolution operation ç•³ã¿è¾¼ã¿æ“ä½œ\nCopy data from host to device â€¦\nCopy data from device to host â€¦\nUnified CPU backend implementation for RusTorch â€¦\nCreate the best available CPU backend for the current â€¦\nGet device information ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—\nDevice type ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—\nDilation [height, width] or [depth, height, width] è†¨å¼µ [â€¦\nElement-wise division: a / b è¦ç´ ã”ã¨é™¤ç®—: a / b\nExecute custom kernel/operation (backend-specific) â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nNumber of groups for grouped convolution â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if this backend is available on the current system â€¦\nKernel size [height, width] or [depth, height, width] â€¦\nMatrix multiplication: a @ b è¡Œåˆ—ä¹—ç®—: a @ b\nMaximum element æœ€å¤§è¦ç´ \nMaximum number of threads/cores â€¦\nMean of all elements å…¨è¦ç´ ã®å¹³å‡\nMinimum element æœ€å°è¦ç´ \nElement-wise multiplication: a * b è¦ç´ ã”ã¨ä¹—ç®—: a * â€¦\nDevice name ãƒ‡ãƒã‚¤ã‚¹å\nPadding [height, width] or [depth, height, width] â€¦\nActivation functions æ´»æ€§åŒ–é–¢æ•°\nReshape tensor to new shape â€¦\nApply sigmoid activation function â€¦\nSize of the buffer in bytes â€¦\nStride [height, width] or [depth, height, width] â€¦\nElement-wise subtraction: a - b è¦ç´ ã”ã¨æ¸›ç®—: a - b\nSum all elements å…¨è¦ç´ ã®åˆè¨ˆ\nSupports half precision åŠç²¾åº¦ã‚µãƒãƒ¼ãƒˆ\nSupports double precision å€ç²¾åº¦ã‚µãƒãƒ¼ãƒˆ\nSynchronize device execution (wait for all operations to â€¦\nApply hyperbolic tangent activation function â€¦\nTotal memory in bytes ç·ãƒ¡ãƒ¢ãƒªï¼ˆãƒã‚¤ãƒˆï¼‰\nTranspose tensor along specified axes â€¦\nElement-wise addition è¦ç´ ã”ã¨ã®åŠ ç®—\nBalance between performance and memory â€¦\nUnified compute backend trait (non-generic methods) â€¦\nGeneric backend operations â€¦\nConvolution operation ç•³ã¿è¾¼ã¿æ“ä½œ\nCPU computation using optimized algorithms â€¦\nNVIDIA CUDA GPU acceleration NVIDIA CUDA GPUåŠ é€Ÿ\nDevice manager for unified backend selection â€¦\nDevice to Host (GPU â†’ CPU) â€¦\nDevice types available for computation â€¦\nHost to Device (CPU â†’ GPU) â€¦\nUser-specified device priority â€¦\nMatrix multiplication è¡Œåˆ—ä¹—ç®—\nMaximum value reduction æœ€å¤§å€¤ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nMean (average) reduction å¹³å‡ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nAlways prefer backends with most available memory â€¦\nApple Metal GPU acceleration Apple Metal GPUåŠ é€Ÿ\nMinimum value reduction æœ€å°å€¤ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nElement-wise multiplication è¦ç´ ã”ã¨ã®ä¹—ç®—\nOpenCL cross-platform GPU acceleration â€¦\nGeneric operation type for backend execution â€¦\nAlways prefer the fastest available backend â€¦\nPerformance metrics for operations â€¦\nProduct reduction ç©ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nReduction operations (sum, mean, etc.) â€¦\nReduction operation types ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ“ä½œç¨®åˆ¥\nBackend selection strategy ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠæˆ¦ç•¥\nSum reduction åˆè¨ˆãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nMemory transfer direction between host and device â€¦\nGet available device types â€¦\nGet available memory in bytes â€¦\nReturns the device type this backend handles â€¦\nDevice utilization percentage â€¦\nExecute a generic operation on this backend â€¦\nExecute operation on the best available backend â€¦\nExecution time in nanoseconds å®Ÿè¡Œæ™‚é–“ï¼ˆãƒŠãƒç§’ï¼‰\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance statistics for a device â€¦\nGet backend-specific information â€¦\nGet performance metrics for the last operation â€¦\nGet the global device manager â€¦\nInitialize the backend ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’åˆæœŸåŒ–\nInitialize all available backends â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns whether this backend is available on the current â€¦\nMemory bandwidth in GB/s ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…ï¼ˆGB/sï¼‰\nTransfer memory between host and device â€¦\nMemory usage in bytes ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒã‚¤ãƒˆï¼‰\nCreate a new device manager â€¦\nRegister a compute backend (temporarily CPU-only) â€¦\nSelect the best backend for an operation â€¦\nSet device-specific configuration â€¦\nBackend selection strategy ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠæˆ¦ç•¥\nSynchronize device operations (wait for completion) â€¦\nFirst input tensor ç¬¬1å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\nFirst input tensor ç¬¬1å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\nLeft matrix tensor å·¦è¡Œåˆ—ãƒ†ãƒ³ã‚½ãƒ«\nAxes along which to reduce (None for all axes) â€¦\nSecond input tensor ç¬¬2å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\nSecond input tensor ç¬¬2å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\nRight matrix tensor å³è¡Œåˆ—ãƒ†ãƒ³ã‚½ãƒ«\nInput tensor for convolution â€¦\nInput tensor for reduction â€¦\nConvolution kernel/filter â€¦\nType of reduction operation â€¦\nPadding for convolution (height, width) â€¦\nStride for convolution (height, width) â€¦\nUnified CPU backend implementation with SIMD optimizations â€¦\nCPU SIMD feature detection CPU SIMDæ©Ÿèƒ½æ¤œå‡º\nAVX2 support\nAVX512F support\nDetect available SIMD features â€¦\nFMA support\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new unified CPU backend â€¦\nSSE4.1 support\nUnified error handling for RusTorch â€¦\nActivation function error æ´»æ€§åŒ–é–¢æ•°ã‚¨ãƒ©ãƒ¼\nMemory alignment error ãƒ¡ãƒ¢ãƒªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼\nMemory allocation failed ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦å¤±æ•—\nDistributed backend not supported â€¦\nBackward pass error é€†ä¼æ’­ã‚¨ãƒ©ãƒ¼\nBatch processing error ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼\nCommunication failed é€šä¿¡å¤±æ•—\nGPU context creation failed â€¦\nConvergence error åæŸã‚¨ãƒ©ãƒ¼\nData loading errors ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼\nData loading errors ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼\nData loader error ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¨ãƒ©ãƒ¼\nData type error ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼\nDataset error ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼\nMemory deallocation failed ãƒ¡ãƒ¢ãƒªè§£æ”¾å¤±æ•—\nGPU device not found GPUãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„\nGPU device not supported â€¦\nTensor dimension mismatch error â€¦\nDistributed computing errors â€¦\nDistributed operation errors åˆ†æ•£æ“ä½œã‚¨ãƒ©ãƒ¼\nGPU driver error GPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚¨ãƒ©ãƒ¼\nEmpty tensor error ç©ºã®ãƒ†ãƒ³ã‚½ãƒ«ã‚¨ãƒ©ãƒ¼\nContains the error value\nFile operation error ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚¨ãƒ©ãƒ¼\nForward pass error é †ä¼æ’­ã‚¨ãƒ©ãƒ¼\nGeneric errors æ±ç”¨ã‚¨ãƒ©ãƒ¼\nGPU-specific errors GPUå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nGPU operation errors GPUæ“ä½œã‚¨ãƒ©ãƒ¼\nGradient calculation error å‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼\nInsufficient tensor dimensions error â€¦\nInvalid GPU device ç„¡åŠ¹ãªGPUãƒ‡ãƒã‚¤ã‚¹\nInvalid tensor index â€¦\nInvalid tensor operation ç„¡åŠ¹ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œ\nInvalid memory pointer ç„¡åŠ¹ãªãƒ¡ãƒ¢ãƒªãƒã‚¤ãƒ³ã‚¿\nInvalid rank ç„¡åŠ¹ãªãƒ©ãƒ³ã‚¯\nInvalid tensor shape ç„¡åŠ¹ãªãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nInvalid world size ç„¡åŠ¹ãªãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚º\nI/O errors I/Oã‚¨ãƒ©ãƒ¼\nGPU kernel compilation failed â€¦\nGPU kernel execution failed GPUã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œå¤±æ•—\nNeural network layer error â€¦\nLearning rate error å­¦ç¿’ç‡ã‚¨ãƒ©ãƒ¼\nLoss function error æå¤±é–¢æ•°ã‚¨ãƒ©ãƒ¼\nGPU memory allocation failed GPUãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦å¤±æ•—\nMemory management errors ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¨ãƒ©ãƒ¼\nMemory management errors ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¨ãƒ©ãƒ¼\nMemory leak detected ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º\nGPU memory transfer failed GPUãƒ¡ãƒ¢ãƒªè»¢é€å¤±æ•—\nModel error ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼\nNetwork error ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼\nNeural network errors â€¦\nNeural network operation errors â€¦\nNode connection failed ãƒãƒ¼ãƒ‰æ¥ç¶šå¤±æ•—\nContains the success value\nOptimization errors æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼\nOptimization errors æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼\nOptimizer error ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚¨ãƒ©ãƒ¼\nGPU out of memory GPUãƒ¡ãƒ¢ãƒªä¸è¶³\nParameter error ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼\nMemory pool exhausted ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«æ¯æ¸‡\nProcess group error ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼\nUnified error type for all RusTorch operations â€¦\nCommon result type for RusTorch operations (çµ±ä¸€æ¸ˆã¿) â€¦\nLearning rate scheduler error â€¦\nTensor shape mismatch error â€¦\nSynchronization failed åŒæœŸå¤±æ•—\nTensor-specific errors ãƒ†ãƒ³ã‚½ãƒ«å›ºæœ‰ã‚¨ãƒ©ãƒ¼\nTensor operation errors ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã‚¨ãƒ©ãƒ¼\nTimeout error ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼\nData transformation error ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ©ãƒ¼\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nActual shape å®Ÿéš›ã®å½¢çŠ¶\nActual dimensions å®Ÿéš›ã®æ¬¡å…ƒæ•°\nExpected shape æœŸå¾…ã•ã‚Œã‚‹å½¢çŠ¶\nLeft tensor shape å·¦ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\nRequired dimensions å¿…è¦ãªæ¬¡å…ƒæ•°\nRight tensor shape å³ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\nModel architecture parsing and analysis (legacy interface) â€¦\nModel architecture parsing and analysis (modular structure)\nSimplified PyTorch to RusTorch conversion implementation â€¦\nCore model parsing logic ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«è§£æãƒ­ã‚¸ãƒƒã‚¯\nError types for model parsing â€¦\nArchitecture description formats and serialization â€¦\nCore data types for model parsing â€¦\nModel graph validation functions â€¦\nPyTorch model parser PyTorchãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ¼ã‚µãƒ¼\nCompute execution order from architecture description â€¦\nExtract layer information from state dictionary â€¦\nReturns the argument unchanged.\nInfer layer type from name and parameters â€¦\nCalls <code>U::from(self)</code>.\nCreate new model parser â€¦\nParse architecture string as JSON or YAML â€¦\nParse PyTorch model into model graph â€¦\nParse parameter name to extract layer name and parameter â€¦\nParse simple architecture format (e.g., â€œconv2d -&gt; relu â€¦\nCircular dependency detected å¾ªç’°ä¾å­˜ã‚’æ¤œå‡º\nContains the error value\nType alias for execution result containing layer order and â€¦\nIncompatible layer dimensions â€¦\nInvalid architecture format â€¦\nMissing layer connection â€¦\nContains the success value\nModel parsing errors ãƒ¢ãƒ‡ãƒ«è§£æã‚¨ãƒ©ãƒ¼\nType alias for parsing result containing layer order and â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nFirst layer name\nSecond layer name\nArchitecture description format for explicit model â€¦\nConnection definition between layers â€¦\nLayer definition in architecture description â€¦\nModel metadata information ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±\nConnection type (optional) â€¦\nLayer connections ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¥ç¶š\nDescription èª¬æ˜\nFramework (pytorch, tensorflow, etc.) ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nSource layer name ã‚½ãƒ¼ã‚¹ãƒ¬ã‚¤ãƒ¤ãƒ¼å\nInput shape hint å…¥åŠ›å½¢çŠ¶ãƒ’ãƒ³ãƒˆ\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLayer type specification ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—ä»•æ§˜\nLayer definitions ãƒ¬ã‚¤ãƒ¤ãƒ¼å®šç¾©\nModel metadata ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nModel name ãƒ¢ãƒ‡ãƒ«å\nLayer name/id ãƒ¬ã‚¤ãƒ¤ãƒ¼å/ID\nOutput shape hint å‡ºåŠ›å½¢çŠ¶ãƒ’ãƒ³ãƒˆ\nLayer parameters ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nTarget layer name ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼å\nModel version ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³\n2D Average Pooling 2Då¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°\n2D Batch Normalization 2Dãƒãƒƒãƒæ­£è¦åŒ–\n1D Convolution layer 1Dç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼\n2D Convolution layer 2Dç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼\n3D Convolution layer 3Dç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼\nDropout layer Dropoutãƒ¬ã‚¤ãƒ¤ãƒ¼\nFlatten layer Flattenãƒ¬ã‚¤ãƒ¤ãƒ¼\nLayer information extracted from PyTorch model â€¦\nSupported layer types â€¦\nLinear/Dense layer Linear/Denseãƒ¬ã‚¤ãƒ¤ãƒ¼\n2D Max Pooling 2Dæœ€å¤§ãƒ—ãƒ¼ãƒªãƒ³ã‚°\nModel architecture graph â€¦\nReLU activation ReLUæ´»æ€§åŒ–\nUnknown layer type ä¸æ˜ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—\nLayer connections (from -&gt; to) ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¥ç¶šï¼ˆfrom â€¦\nLayer execution order ãƒ¬ã‚¤ãƒ¤ãƒ¼å®Ÿè¡Œé †åº\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nModel input layers ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ãƒ¬ã‚¤ãƒ¤ãƒ¼\nInput shape å…¥åŠ›å½¢çŠ¶\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLayer type (Linear, Conv2d, etc.) â€¦\nLayers in the model ãƒ¢ãƒ‡ãƒ«å†…ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼\nLayer name ãƒ¬ã‚¤ãƒ¤ãƒ¼å\nNumber of parameters ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\nModel output layers ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãƒ¬ã‚¤ãƒ¤ãƒ¼\nOutput shape å‡ºåŠ›å½¢çŠ¶\nLayer parameters ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nInput channels\nInput channels\nInput channels\nInput features\nKernel size\nKernel size\nKernel size\nKernel size\nKernel size\nNumber of features\nOutput channels\nOutput channels\nOutput channels\nOutput features\nDropout probability\nPadding\nPadding\nPadding\nStride\nStride\nStride\nStride\nStride\nModel validation functionality\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nValidate model graph for consistency â€¦\nValidate that all referenced layers exist â€¦\nInvalid parameter format ç„¡åŠ¹ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼\nLayer description for model conversion â€¦\nMissing parameter ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„\nSimplified conversion error ç°¡ç•¥åŒ–å¤‰æ›ã‚¨ãƒ©ãƒ¼\nSimplified layer information ç°¡ç•¥åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼æƒ…å ±\nSimplified converter ç°¡ç•¥åŒ–å¤‰æ›å™¨\nSimplified PyTorch model representation â€¦\nLayer not supported in simplified version â€¦\nConvert PyTorch model to simplified representation â€¦\nExecution order å®Ÿè¡Œé †åº\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet layer by name åå‰ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å–å¾—\nInput tensor shape\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet all layer names å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å–å¾—\nType of layer\nLayer type as string â€¦\nModel layers ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¤ãƒ¤ãƒ¼\nLayer name\nLayer name ãƒ¬ã‚¤ãƒ¤ãƒ¼å\nTotal number of parameters ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\nOutput tensor shape\nParameter shapes ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢çŠ¶\nPrint model summary ãƒ¢ãƒ‡ãƒ«è¦ç´„ã‚’è¡¨ç¤º\nSimulate forward pass (placeholder) â€¦\nConverted tensors å¤‰æ›ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«\nModel statistics ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ\nDataLoader implementation for Phase 5 - PyTorch-compatible â€¦\nDataset traits and implementations for Phase 5 â€¦\nSampling strategies for Phase 5 DataLoader ãƒ•ã‚§ãƒ¼ã‚º5 â€¦\nMain DataLoader implementation using Phase 5 Dataset trait â€¦\nGet batch size ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if exhausted æ¯æ¸‡ã—ãŸã‹ãƒã‚§ãƒƒã‚¯\nCreate new DataLoader æ–°ã—ã„DataLoaderã‚’ä½œæˆ\nGet next batch æ¬¡ã®ãƒãƒƒãƒã‚’å–å¾—\nGet number of workers ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—\nReset the sampler ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ\nCreate DataLoader with all options â€¦\nAutomatic differentiation error è‡ªå‹•å¾®åˆ†ã‚¨ãƒ©ãƒ¼\nBackend not available ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒåˆ©ç”¨ä¸å¯\nConcatenated dataset implementation â€¦\nData loading error type â€¦\nData loading error ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼\nDataLoader system errors (Phase 5) â€¦\nCore Dataset trait following PyTorch API (Phase 5 - Main â€¦\nDebug system and logging errors â€¦\nDevice operation failed ãƒ‡ãƒã‚¤ã‚¹æ“ä½œå¤±æ•—\nDistributed computing errors â€¦\nGPU-specific errors GPUå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nInput/Output and serialization errors â€¦\nImport-specific error ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nInvalid operation ç„¡åŠ¹ãªæ“ä½œ\nInvalid operation parameters ç„¡åŠ¹ãªæ“ä½œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nIterable Dataset trait for streaming data â€¦\nKernel compilation error for GPU operations â€¦\nMemory allocation failed ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦å¤±æ•—\nModel import/export error â€¦\nNeural network layer error â€¦\nFeature not implemented yet æ©Ÿèƒ½æœªå®Ÿè£…\nOut of memory error ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼\nProfiling and performance analysis errors â€¦\nSerialization error ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼\nSerialization and model I/O errors (Phase 9) â€¦\nShape mismatch between tensors â€¦\nSimple tensor dataset implementation â€¦\nTensor operation failed ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œå¤±æ•—\nData validation and quality assurance errors â€¦\nVision processing errors è¦–è¦šå‡¦ç†ã‚¨ãƒ©ãƒ¼\nVisualization errors å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼\nAdd tensor to dataset â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate tensor dataset from features and targets â€¦\nGet item at index â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if dataset is empty â€¦\nCreate iterator over dataset â€¦\nReturns the number of samples in the dataset â€¦\nCreate new concatenated dataset â€¦\nCreate new tensor dataset â€¦\nGet reference to tensors ãƒ†ãƒ³ã‚½ãƒ«ã®å‚ç…§ã‚’å–å¾—\nActual tensor shape that was provided â€¦\nAvailable memory in bytes â€¦\nName of the unavailable backend â€¦\nDevice identifier where the error occurred â€¦\nDevice where allocation failed â€¦\nExpected tensor shape æœŸå¾…ã•ã‚Œã‚‹ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nFeature that is not yet implemented æœªå®Ÿè£…ã®æ©Ÿèƒ½\nName of the neural network layer â€¦\nError message describing the tensor operation failure â€¦\nError message describing the device issue â€¦\nDescription of the parameter issue â€¦\nDetailed error message è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\nError message from the layer â€¦\nAutograd error description è‡ªå‹•å¾®åˆ†ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nModel I/O error description ãƒ¢ãƒ‡ãƒ«I/Oã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nImport operation error message â€¦\nData loading error description â€¦\nGPU operation error description GPUæ“ä½œã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nVision processing error description â€¦\nDistributed computing error description â€¦\nVisualization error description å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nProfiling error description â€¦\nData validation error description â€¦\nDebug system error description â€¦\nKernel compilation error description â€¦\nDataLoader error description DataLoaderã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nSerialization error description â€¦\nSerialization error description â€¦\nName of the operation with invalid parameters â€¦\nName of the invalid operation ç„¡åŠ¹ãªæ“ä½œå\nOperation that caused the serialization error â€¦\nRequested memory in bytes â€¦\nSize of the failed allocation in bytes â€¦\nOptional underlying error cause â€¦\nOptional underlying I/O error â€¦\nOptional underlying import error â€¦\nOptional underlying data loading error â€¦\nOptional underlying GPU error â€¦\nOptional underlying vision error â€¦\nOptional underlying distributed error â€¦\nOptional underlying visualization error â€¦\nBatch sampler - wraps another sampler to yield batches of â€¦\nRandom sampler - returns shuffled indices â€¦\nCore Sampler trait following PyTorch API PyTorch â€¦\nSequential sampler - returns indices in order â€¦\nSubset random sampler - samples from a subset of indices â€¦\nWeighted random sampler â€¦\nGet batch size ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\nCheck if dropping last incomplete batch â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if sampler is exhausted â€¦\nGet total number of samples ç·ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å–å¾—\nCreate new batch sampler â€¦\nCreate sampler for subset of indices â€¦\nCreate weighted random sampler â€¦\nCreate new sequential sampler â€¦\nCreate new random sampler â€¦\nGet next batch of indices â€¦\nReset sampler for new epoch â€¦\nSample next index æ¬¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚µãƒ³ãƒ—ãƒ«\nCreate random sampler with replacement â€¦\nCreate seeded random sampler for reproducible results â€¦\nAnalysis Summary\nUnified Debug Framework\nComprehensive Debug Report\nDebug Session Information\nFramework Configuration\nLog Summary Statistics\nRAII Profile Guard for automatic timing\nQuick debug log\nDebug Utilities and System Information\nAnalysis configuration\nMemory tracking configuration\nProfiling configuration\nQuick error log\nFlush all pending data\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet comprehensive debug report\nGet current session information\nQuick info log\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLog structured message with metadata\nLog Analysis and Pattern Detection System\nLogging configuration\nStructured Logging System\nMemory Allocation Tracking System\nCreate new debug framework with default configuration\nCreate profiling guard for automatic timing\nPerformance Profiling System\nTrack memory allocation\nTrack memory deallocation\nQuick warning log\nCreate debug framework with custom configuration\nDebug utilities collection\nDiagnostic context for debugging\nMemory usage snapshot\nPerformance measurement helper\nStack trace information (simplified)\nSystem information for debugging\nAdd parameter\nCapture current stack trace (simplified implementation)\nCapture stack trace\nCapture stack trace\nAdd checkpoint\nCollect current system information\nCreate diagnostic context for operation\nSimple assertion with diagnostic context\nConditional debugging output\nGet current elapsed time\nFinish timing and get total duration\nFormat bytes for human reading\nFormat duration for human reading\nFormat system info as string\nFormat memory snapshot\nFormat stack trace as string\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate comprehensive diagnostic report\nGenerate environment report\nGenerate timing report\nCollect system information\nGet current thread information\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if running in debug mode\nCreate new diagnostic context\nSet memory snapshot\nStart new performance timer\nStart performance measurement\nTake memory snapshot (simplified)\nTake memory snapshot\nTime a code block and return result with timing\nCalculate memory utilization percentage\nAlert notification\nAlert rule for automated notifications\nLog analyzer with pattern detection and alerting\nLog pattern for detection\nPattern detection result\nAdd alert rule\nAdd log pattern\nAnalyze log entry for patterns\nCheck if rule can be triggered (not in cooldown)\nClear analysis data\nEnable/disable alert rule\nEnable/disable pattern\nExtract capture groups from log message\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate analysis report\nGet analysis summary\nGet matches for specific pattern\nGet recent pattern matches\nGet total alerts count\nGet all triggered alerts\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if analyzer is enabled\nMark rule as triggered\nCheck if log entry matches this pattern\nCreate new log analyzer\nCreate new log pattern\nCreate new alert rule\nEnable/disable analyzer\nLog entry with structured metadata\nLog severity levels\nLog output configuration\nCore logging system\nGet ANSI color code for console output\nGet emoji representation\nFilter entries by level\nFilter entries by metadata key-value pair\nFlush all pending writes\nFormat as colored console output\nFormat as human-readable string\nFormat as JSON\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nConvert from string representation\nGet log summary statistics\nGet total log count\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLog structured entry\nCreate new logger\nCreate new log entry\nSearch entries by message content\nCreate logger with specific output configuration\nMemory allocation information\nMemory usage statistics by component\nComprehensive memory report\nMemory allocation tracker\nAge of allocation\nClear all tracking data\nSet leak detection parameters\nCurrent usage in MB\nDetect potential memory leaks\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate comprehensive memory report\nGenerate memory usage summary\nGet current number of active allocations\nGet memory statistics for specific component\nGet current memory usage in MB\nGet large allocations\nGet peak memory usage in MB\nGet all tracked components\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if memory tracking is enabled\nCheck if usage exceeds threshold\nCreate new memory tracker\nCreate new allocation info\nPeak usage in MB\nEnable/disable memory tracking\nSet large allocation threshold\nSize in megabytes\nTrack memory allocation\nTrack memory deallocation\nPerformance profiler for operation timing\nStatistics for a specific operation type\nAggregated performance metrics\nPerformance profile entry\nRAII profiling scope guard\nAdd metadata to the profile\nClear all recorded data\nDuration in milliseconds\nDuration in microseconds\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate performance report\nGet entries for specific operation\nGet all operation names\nGet statistics for specific operation\nGet comprehensive performance metrics\nGet recent entries (last n entries)\nGet total number of recorded operations\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if profiling is enabled\nCreate new profiler\nCreate new profiling scope\nCreate new profile entry\nRecord operation timing\nRecord operation with metadata\nEnable/disable profiling\nCreate profiling scope with metadata\nCreate with metadata\nAverage reduction å¹³å‡ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nDistributed backend types åˆ†æ•£ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—\nCommon trait for distributed data parallel implementations â€¦\nCommunication operations for distributed training â€¦\nType alias for distributed-compatible float types â€¦\nDistributed state management åˆ†æ•£çŠ¶æ…‹ç®¡ç†\nFacebookâ€™s collective communications library â€¦\nMessage Passing Interface â€¦\nMaximum reduction æœ€å¤§å€¤ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nMinimum reduction æœ€å°å€¤ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nNVIDIA Collective Communications Library â€¦\nProcess group for distributed training â€¦\nProduct reduction ç©ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nReduction operations for collective communications â€¦\nSum reduction åˆè¨ˆãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³\nCustom TCP backend ã‚«ã‚¹ã‚¿ãƒ TCPãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰\nAll-gather operation across all processes â€¦\nAll-reduce operation across all processes â€¦\nData parallel training module â€¦\nAsynchronous gradient synchronization system for â€¦\nBackend used for communication â€¦\nCommunication backends for distributed training â€¦\nBroadcast operation from root process â€¦\nMulti-machine cluster support for distributed training â€¦\nCommon utilities for distributed operations â€¦\nData parallel training implementation â€¦\nDistributedDataParallel (DDP) implementation for RusTorch â€¦\nGet device IDs for this DDP instance â€¦\nDevice mapping for each rank â€¦\nAvailable devices for distributed training â€¦\nPerform distributed forward pass â€¦\nFinalize distributed training åˆ†æ•£å­¦ç¿’ã‚’çµ‚äº†\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather operation to root process â€¦\nGet global distributed state â€¦\nGet current rank in distributed training â€¦\nGet world size in distributed training â€¦\nInitialize the process group â€¦\nInitialize distributed training åˆ†æ•£å­¦ç¿’ã‚’åˆæœŸåŒ–\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if distributed training is available â€¦\nCheck if distributed training is initialized â€¦\nMaster address for coordination â€¦\nMaster port for coordination èª¿æ•´ç”¨ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒˆ\nModel parallel training implementation â€¦\nMulti-GPU validation and benchmarking for distributed â€¦\nAdvanced NCCL integration for high-performance distributed â€¦\nCreate a new process group â€¦\nCreate new distributed state æ–°ã—ã„åˆ†æ•£çŠ¶æ…‹ã‚’ä½œæˆ\nDistributed optimizers for distributed training â€¦\nPerformance optimizations for distributed learning â€¦\nCurrent process group ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—\nGet current rank ç¾åœ¨ã®ãƒ©ãƒ³ã‚¯ã‚’å–å¾—\nRank of current process ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ©ãƒ³ã‚¯\nReduce operation to root process â€¦\nScatter operation from root process â€¦\nSet process group ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¨­å®š\nSimplified DistributedDataParallel implementation â€¦\nSynchronize gradients across processes â€¦\nGet world size ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã‚’å–å¾—\nTotal number of processes ãƒ—ãƒ­ã‚»ã‚¹ç·æ•°\nAsynchronous distributed request handle â€¦\nProcess group management ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ç®¡ç†\nAll-gather operation All-gatheræ“ä½œ\nAll-reduce operation All-reduceæ“ä½œ\nBarrier synchronization ãƒãƒªã‚¢åŒæœŸ\nBroadcast operation ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆæ“ä½œ\nDestroy the distributed process group â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather operation Gatheræ“ä½œ\nGet the rank of the current process â€¦\nGet the world size of the current process group â€¦\nInitialize distributed training process group â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if operation is completed â€¦\nCheck if the current process is initialized for â€¦\nMonitoring and debugging utilities â€¦\nCreate a new process group â€¦\nCreate a new distributed group â€¦\nGet the rank of this process in the group â€¦\nReduce operation Reduceæ“ä½œ\nScatter operation Scatteræ“ä½œ\nGet the size of this group â€¦\nWait for the operation to complete æ“ä½œå®Œäº†ã‚’å¾…æ©Ÿ\nCommunication statistics structure é€šä¿¡çµ±è¨ˆæ§‹é€ ä½“\nReturns the argument unchanged.\nGet communication statistics é€šä¿¡çµ±è¨ˆã‚’å–å¾—\nCalls <code>U::from(self)</code>.\nConfiguration for asynchronous gradient synchronization â€¦\nAsynchronous gradient synchronization context â€¦\nAsynchronous gradient synchronization coordinator â€¦\nIndividual gradient bucket å€‹åˆ¥å‹¾é…ãƒã‚±ãƒƒãƒˆ\nGradient bucket manager for efficient communication â€¦\nGradient synchronization completion notification â€¦\nGradient synchronization request â€¦\nPriority levels for gradient synchronization â€¦\nAdd gradient to appropriate bucket â€¦\nBucket size (MB) ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºï¼ˆMBï¼‰\nCheck for completed synchronizations â€¦\nGradient compression utilities â€¦\nCompression threshold (bytes) åœ§ç¸®é–¾å€¤ï¼ˆãƒã‚¤ãƒˆï¼‰\nSynchronization duration åŒæœŸæ™‚é–“\nEnable gradient bucketing å‹¾é…ãƒã‚±ãƒƒãƒˆåŒ–ã‚’æœ‰åŠ¹åŒ–\nEnable gradient compression å‹¾é…åœ§ç¸®ã‚’æœ‰åŠ¹åŒ–\nError message if failed â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet buckets ready for synchronization â€¦\nGradient tensor to synchronize â€¦\nCombined gradient tensors çµåˆå‹¾é…ãƒ†ãƒ³ã‚½ãƒ«\nRequest ID for tracking è¿½è·¡ç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆID\nBucket ID ãƒã‚±ãƒƒãƒˆID\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLast update timestamp æœ€çµ‚æ›´æ–°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—\nMark bucket as synchronized and clear â€¦\nMaximum number of concurrent operations â€¦\nCreate a new asynchronous gradient synchronizer â€¦\nCreate new gradient bucket manager â€¦\nCreate new async gradient context â€¦\nParameter name ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å\nParameters in this bucket â€¦\nPriority level å„ªå…ˆåº¦ãƒ¬ãƒ™ãƒ«\nReady for synchronization åŒæœŸæº–å‚™å®Œäº†\nReduction operation ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ“ä½œ\nRequest ID that completed å®Œäº†ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆID\nSubmit gradient for asynchronous synchronization â€¦\nSuccess status æˆåŠŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\nSynchronize all pending gradients â€¦\nSubmit parameter gradient for async sync â€¦\nTimeout for gradient synchronization â€¦\nWait for all pending operations to complete â€¦\nTotal size in bytes ç·ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰\nWait for specific synchronization to complete â€¦\nCompressed gradient representation åœ§ç¸®å‹¾é…è¡¨ç¾\nGradient compression algorithms â€¦\nCompression metadata åœ§ç¸®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nError feedback compression â€¦\nNo compression åœ§ç¸®ãªã—\nQuantization-based compression é‡å­åŒ–ãƒ™ãƒ¼ã‚¹åœ§ç¸®\nTop-K sparsification Top-Kã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–\nCompress gradient tensor å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã‚’åœ§ç¸®\nCompressed data åœ§ç¸®ãƒ‡ãƒ¼ã‚¿\nDecompress gradient tensor å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã‚’å±•é–‹\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCompression metadata åœ§ç¸®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nOriginal tensor shape å…ƒã®ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nBackend factory ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¯ãƒˆãƒª\nGloo backend for CPU and GPU communication â€¦\nGloo communication context Glooé€šä¿¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\nGloo transport options Glooè»¢é€ã‚ªãƒ—ã‚·ãƒ§ãƒ³\nInfiniBand transport for high-performance clusters â€¦\nShared memory transport for single-node communication â€¦\nTCP transport for network communication â€¦\nTCP backend for simple distributed training â€¦\nTCP connection to remote process â€¦\nCreate backend instance based on process group â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new Gloo backend instance â€¦\nCreate new TCP backend instance â€¦\nNode is available ãƒãƒ¼ãƒ‰ãŒåˆ©ç”¨å¯èƒ½\nBest-fit scheduling â€¦\nNode is busy ãƒãƒ¼ãƒ‰ãŒãƒ“ã‚¸ãƒ¼\nCluster configuration for multi-machine training â€¦\nCluster manager for coordinating distributed training â€¦\nCluster status information â€¦\nCluster topology types â€¦\nCustom topology with explicit connections â€¦\nDistributed backend types åˆ†æ•£ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ—\nNode has failed ãƒãƒ¼ãƒ‰ãŒæ•…éšœ\nFault tolerance configuration éšœå®³è€æ€§è¨­å®š\nFirst-fit scheduling â€¦\nFlat topology - all nodes communicate directly â€¦\nFacebook Gloo collective communications Facebook â€¦\nHeartbeat monitor for node health checking â€¦\nLoad balancing è² è·åˆ†æ•£\nLocality-aware scheduling â€¦\nMessage Passing Interface â€¦\nNVIDIA Collective Communications Library â€¦\nNode capabilities ãƒãƒ¼ãƒ‰æ©Ÿèƒ½\nNode information in the cluster â€¦\nNode status ãƒãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\nNode is offline ãƒãƒ¼ãƒ‰ãŒã‚ªãƒ•ãƒ©ã‚¤ãƒ³\nResource scheduler for optimal job placement â€¦\nCurrent resource usage on a node â€¦\nRing topology - ring-based communication â€¦\nScheduling strategies ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥\nTree topology - hierarchical communication â€¦\nActive jobs ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¸ãƒ§ãƒ–æ•°\nActive jobs ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¸ãƒ§ãƒ–æ•°\nNode address ãƒãƒ¼ãƒ‰ã‚¢ãƒ‰ãƒ¬ã‚¹\nAvailable nodes åˆ©ç”¨å¯èƒ½ãƒãƒ¼ãƒ‰æ•°\nBusy nodes ãƒ“ã‚¸ãƒ¼ãƒãƒ¼ãƒ‰æ•°\nNode capabilities ãƒãƒ¼ãƒ‰æ©Ÿèƒ½\nCheckpoint frequency for recovery â€¦\nCPU cores CPUã‚³ã‚¢æ•°\nUsed CPU cores ä½¿ç”¨CPUã‚³ã‚¢æ•°\nCreate process group for distributed training â€¦\nEnable automatic failover â€¦\nFailed nodes æ•…éšœãƒãƒ¼ãƒ‰æ•°\nFault tolerance settings éšœå®³è€æ€§è¨­å®š\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cluster status ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—\nNumber of GPUs on this node ã“ã®ãƒãƒ¼ãƒ‰ä¸Šã®GPUæ•°\nGPU memory per device (GB) â€¦\nUsed GPU memory (GB) ä½¿ç”¨GPUãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰\nHandle node failure ãƒãƒ¼ãƒ‰éšœå®³ã‚’å‡¦ç†\nHeartbeat interval (seconds) â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaster node address ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‚¢ãƒ‰ãƒ¬ã‚¹\nMaster node port ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ãƒãƒ¼ãƒˆ\nMaximum retry attempts æœ€å¤§å†è©¦è¡Œå›æ•°\nAvailable memory (GB) åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰\nUsed memory (GB) ä½¿ç”¨ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰\nNetwork bandwidth (Gbps) â€¦\nCreate new cluster manager â€¦\nCreate new heartbeat monitor â€¦\nCreate new resource scheduler â€¦\nNode ID ãƒãƒ¼ãƒ‰ID\nNode timeout (seconds) ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰\nNode port ãƒãƒ¼ãƒ‰ãƒãƒ¼ãƒˆ\nSchedule job on available nodes â€¦\nStart cluster services ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹\nStart resource monitoring â€¦\nNode status ãƒãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\nStop cluster services ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢\nStop monitoring ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’åœæ­¢\nStop resource monitoring â€¦\nCluster topology ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒˆãƒãƒ­ã‚¸ãƒ¼\nTotal GPUs ç·GPUæ•°\nTotal number of nodes ç·ãƒãƒ¼ãƒ‰æ•°\nUpdate heartbeat for node â€¦\nList of worker nodes ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\nMap from node ID to list of connected node IDs â€¦\nMaximum depth of the tree ãƒ„ãƒªãƒ¼ã®æœ€å¤§æ·±åº¦\nTrait for backend-specific optimizations â€¦\nCommon distributed operation implementations â€¦\nDefault all_gather implementation for backends â€¦\nDefault all_reduce implementation for backends â€¦\nDefault broadcast implementation for backends â€¦\nDefault gather implementation for backends â€¦\nEnable gradient compression å‹¾é…åœ§ç¸®ã‚’æœ‰åŠ¹åŒ–\nEnable pipeline parallelism for large tensors â€¦\nEnable zero-copy optimizations â€¦\nReturns the argument unchanged.\nGet optimal chunk size for communication â€¦\nCalls <code>U::from(self)</code>.\nGet memory pool size for tensor operations â€¦\nGet optimal bucket size for gradient bucketing â€¦\nGet optimal number of communication streams â€¦\nOptimize tensor for communication â€¦\nCheck if backend supports async operations â€¦\nCreate error for unsupported operations â€¦\nValidate rank for distributed operations â€¦\nValidate tensor for distributed operations â€¦\nValidate tensor shapes match across processes â€¦\nAsynchronous gradient updates éåŒæœŸå‹¾é…æ›´æ–°\nData parallel wrapper for models â€¦\nIterator for distributed data loading â€¦\nDistributed data loader for data parallel training â€¦\nDistributed sampler for ensuring each process gets â€¦\nGradient synchronization strategies å‹¾é…åŒæœŸæˆ¦ç•¥\nLocal SGD with periodic synchronization â€¦\nSynchronize gradients after each backward pass â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather outputs from devices â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if the dataloader has no data\nReturns true if there are no samples\nGet batch iterator ãƒãƒƒãƒã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’å–å¾—\nGet number of batches per epoch â€¦\nGet number of samples for current process â€¦\nCreate a new distributed data loader â€¦\nCreate a new distributed sampler â€¦\nCreate a new data parallel wrapper â€¦\nGet number of devices ãƒ‡ãƒã‚¤ã‚¹æ•°ã‚’å–å¾—\nReplicate input across devices â€¦\nGenerate sample indices for current process â€¦\nSet epoch for deterministic shuffling â€¦\nSet epoch for deterministic sampling â€¦\nSet gradient synchronization strategy â€¦\nSynchronize gradients across devices â€¦\nFrequency of synchronization in steps â€¦\nDistributedDataParallel wrapper for PyTorch compatibility â€¦\nGet device IDs ãƒ‡ãƒã‚¤ã‚¹IDã‚’å–å¾—\nPerform forward pass with distributed synchronization â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if this is a DDP module â€¦\nGet the wrapped module â€¦\nCreate a new DistributedDataParallel wrapper â€¦\nSynchronize gradients across all processes â€¦\nConvenience function to wrap a module in DDP â€¦\nAll-reduce operation ã‚ªãƒ¼ãƒ«ãƒªãƒ‡ãƒ¥ãƒ¼ã‚¹æ“ä½œ\nAll-to-all communication ã‚ªãƒ¼ãƒ«ãƒ„ãƒ¼ã‚ªãƒ¼ãƒ«é€šä¿¡\nBroadcast operation ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆæ“ä½œ\nCommunication operations between model partitions â€¦\nTypes of communication between partitions â€¦\nExpert parallel for mixture of experts models â€¦\nMemory usage statistics ãƒ¡ãƒ¢ãƒªä½¿ç”¨çµ±è¨ˆ\nModel parallel wrapper for splitting models across devices â€¦\nPoint-to-point send/receive â€¦\nPipeline parallel configuration â€¦\nTensor parallel operations for splitting tensors across â€¦\nAll-reduce tensor across partitions â€¦\nCurrently allocated bytes â€¦\nBalance load across partitions â€¦\nCached bytes ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒã‚¤ãƒˆæ•°\nDestination partition index â€¦\nEnable pipeline parallelism â€¦\nExecute forward pass with model parallelism â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGather tensors from all partitions â€¦\nGet expert assignment for current device â€¦\nGradient accumulation steps å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet memory usage statistics for each partition â€¦\nCreate new tensor parallel context â€¦\nCreate new expert parallel context â€¦\nCreate a new model parallel wrapper â€¦\nNumber of micro-batches ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒæ•°\nPipeline stages ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¸\nCommunication type é€šä¿¡ã‚¿ã‚¤ãƒ—\nPeak allocated bytes ãƒ”ãƒ¼ã‚¯å‰²ã‚Šå½“ã¦ãƒã‚¤ãƒˆæ•°\nRoute tokens to appropriate experts â€¦\nSource partition index â€¦\nSplit tensor along parallel dimension â€¦\nTensor shape for communication é€šä¿¡ç”¨ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nWhether to use 1F1B schedule â€¦\nPerformance benchmark results â€¦\nGPU device information GPUãƒ‡ãƒã‚¤ã‚¹æƒ…å ±\nMemory usage statistics ãƒ¡ãƒ¢ãƒªä½¿ç”¨çµ±è¨ˆ\nMulti-GPU validator ãƒãƒ«ãƒGPUãƒãƒªãƒ‡ãƒ¼ã‚¿\nProcess group for distributed operations â€¦\nMulti-GPU validation metrics â€¦\nTotal accuracy ç·ç²¾åº¦\nAvailable memory in bytes â€¦\nBackend type for distributed communication â€¦\nRun performance benchmark â€¦\nClear metrics history ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã‚’ã‚¯ãƒªã‚¢\nCommunication overhead percentage â€¦\nCommunication time é€šä¿¡æ™‚é–“\nCompute capability (for CUDA) è¨ˆç®—èƒ½åŠ›ï¼ˆCUDAç”¨ï¼‰\nCurrent usage ç¾åœ¨ã®ä½¿ç”¨é‡\nPer-device accuracies ãƒ‡ãƒã‚¤ã‚¹ã”ã¨ã®ç²¾åº¦\nDevice ID ãƒ‡ãƒã‚¤ã‚¹ID\nPer-device losses ãƒ‡ãƒã‚¤ã‚¹ã”ã¨ã®æå¤±\nValidation time per device â€¦\nDevice type ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—\nMemory fragmentation percentage â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cached benchmark results â€¦\nGet device information ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—\nGet validation history æ¤œè¨¼å±¥æ­´ã‚’å–å¾—\nInitialize multi-GPU environment â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nIs device available for training â€¦\nMemory usage per device â€¦\nMulti-GPU throughput ãƒãƒ«ãƒGPUã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ\nDevice name ãƒ‡ãƒã‚¤ã‚¹å\nCreate a new multi-GPU validator â€¦\nOptimal batch size per GPU â€¦\nPeak memory usage ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\nProcess rank in the distributed group â€¦\nScaling efficiency (multi/single) â€¦\nSingle GPU baseline performance â€¦\nSamples processed per second â€¦\nTotal validation loss ç·æ¤œè¨¼æå¤±\nTotal memory in bytes ç·ãƒ¡ãƒ¢ãƒªï¼ˆãƒã‚¤ãƒˆï¼‰\nTotal validation time ç·æ¤œè¨¼æ™‚é–“\nValidate model across multiple GPUs â€¦\nTotal number of processes in the group â€¦\nTiming statistics ã‚¿ã‚¤ãƒŸãƒ³ã‚°çµ±è¨ˆ\nFallback implementations when NCCL is not available â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAdam optimizer Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼\nAsynchronous gradient updates éåŒæœŸå‹¾é…æ›´æ–°\nAverage all values across processes â€¦\nGradient compression for bandwidth efficiency â€¦\nDistributed optimizer wrapper â€¦\nDistributed optimizer builder for easy configuration â€¦\nGradient bucket for batching communications â€¦\nGradient synchronization strategies å‹¾é…åŒæœŸæˆ¦ç•¥\nHierarchical all-reduce for large clusters â€¦\nLocal SGD with periodic synchronization â€¦\nTake maximum value across processes â€¦\nTake minimum value across processes â€¦\nOptimizer types for builder pattern â€¦\nReduction operation types ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ“ä½œã‚¿ã‚¤ãƒ—\nStochastic Gradient Descent optimizer â€¦\nSum all values across processes â€¦\nSynchronous all-reduce after each backward pass â€¦\nCreate distributed Adam optimizer â€¦\nCreate new builder with Adam â€¦\nAdd tensor to gradient bucket â€¦\nSet communication backend é€šä¿¡ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è¨­å®š\nSet gradient bucket size â€¦\nBuild the distributed optimizer â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInitialize gradient buckets for efficient communication â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new distributed optimizer â€¦\nReset step counter â€¦\nSet communication frequency for local SGD â€¦\nCreate distributed SGD optimizer â€¦\nCreate new builder with SGD â€¦\nGet current step count ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å–å¾—\nSynchronize gradients across all processes â€¦\nSet gradient synchronization strategy â€¦\nCompression ratio (0.0 to 1.0) åœ§ç¸®ç‡ï¼ˆ0.0ã‹ã‚‰1.0ï¼‰\nFrequency of synchronization in steps â€¦\nBeta1 parameter Beta1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nBeta2 parameter Beta2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nEpsilon for numerical stability â€¦\nLearning rate for optimization æœ€é©åŒ–ã®å­¦ç¿’ç‡\nLearning rate for optimization æœ€é©åŒ–ã®å­¦ç¿’ç‡\nMomentum factor ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¿‚æ•°\nWeight decay for L2 regularization â€¦\nWeight decay for L2 regularization â€¦\nAll-gather operation (gather data from all processes to â€¦\nAll-reduce operation (combine values from all processes) â€¦\nBroadcast operation (send data from one process to all) â€¦\nCommunication scheduler for batching operations â€¦\nCompressed gradient representation åœ§ç¸®å‹¾é…è¡¨ç¾\nGradient compression algorithms â€¦\nGather operation (collect data from all processes to one) â€¦\nGradient compressor for reducing communication overhead â€¦\nMemory pool statistics ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«çµ±è¨ˆ\nNo compression åœ§ç¸®ãªã—\nOperation metadata æ“ä½œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nType of communication operation é€šä¿¡æ“ä½œã®ç¨®é¡\nPending communication operation ä¿ç•™ä¸­ã®é€šä¿¡æ“ä½œ\nQuantization é‡å­åŒ–\nRandom sparsification ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–\nReduce operation (combine values to one process) â€¦\nScatter operation (distribute data from one process to all)\nMemory pool for efficient tensor allocation â€¦\nTop-K sparsification Top-K ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–\nCompression algorithm used â€¦\nClear all pools å…¨ãƒ—ãƒ¼ãƒ«ã‚’ã‚¯ãƒªã‚¢\nCompress gradient tensor å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã‚’åœ§ç¸®\nCompressed gradient data åœ§ç¸®ã•ã‚ŒãŸå‹¾é…ãƒ‡ãƒ¼ã‚¿\nDecompress gradient tensor å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã‚’å±•é–‹\nForce execution of pending operations â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet memory pool statistics â€¦\nGet tensor from pool or allocate new one â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum pool size allowed â€¦\nOperation metadata æ“ä½œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nCreate new gradient compressor â€¦\nCreate new memory pool æ–°ã—ã„ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ\nCreate new communication scheduler â€¦\nType of the communication operation â€¦\nOriginal tensor shape before compression â€¦\nPriority of the operation (0-255, higher values = higher â€¦\nReturn tensor to pool ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ—ãƒ¼ãƒ«ã«è¿”å´\nRoot rank for operations that require a root process â€¦\nSchedule operation for batched execution â€¦\nTensor data for the operation â€¦\nTimestamp when the operation was created â€¦\nTotal number of tensors in the pool â€¦\nNumber of unique tensor shapes â€¦\nNumber of bits for quantization é‡å­åŒ–ã®ãƒ“ãƒƒãƒˆæ•°\nNumber of top elements to keep ä¿æŒã™ã‚‹ä¸Šä½è¦ç´ æ•°\nCompression ratio (0.0 to 1.0) åœ§ç¸®ç‡ï¼ˆ0.0ã‹ã‚‰1.0ï¼‰\nSimplified DistributedDataParallel for RusTorch â€¦\nGet device IDs ãƒ‡ãƒã‚¤ã‚¹IDã‚’å–å¾—\nForward pass with distributed synchronization â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nGet the wrapped module â€¦\nCreate a new simplified DDP wrapper â€¦\nEnable or disable automatic gradient synchronization â€¦\nSynchronize gradients across all processes â€¦\nConvenience function to wrap a module in simplified DDP â€¦\nDistribution trait for all statistical distributions â€¦\nValidation utilities for distributions â€¦\nBernoulli distribution implementation\nBeta distribution implementation\nCategorical distribution implementation\nCompute cumulative distribution function â€¦\nStatistical Distributions - torch.distributions.* â€¦\nGet distribution entropy â€¦\nExponential distribution implementation\nReturns the argument unchanged.\nGamma distribution implementation\nCompute inverse cumulative distribution function (quantile â€¦\nCalls <code>U::from(self)</code>.\nCompute log probability density/mass function â€¦\nGet distribution mean åˆ†å¸ƒã®å¹³å‡ã‚’å–å¾—\nNormal (Gaussian) distribution implementation\nCompute probability density/mass function â€¦\nGenerate random normal samples using Box-Muller transform â€¦\nGenerate single random normal scalar â€¦\nGenerate random uniform samples â€¦\nGenerate single random uniform scalar â€¦\nSample from the distribution â€¦\nSample with replacement å¾©å…ƒæŠ½å‡ºã§ã‚µãƒ³ãƒ—ãƒ«\nGet distribution standard deviation â€¦\nUniform distribution implementation\nValidate non-negative parameter (&gt;= 0) â€¦\nValidate positive parameter (&gt; 0) â€¦\nValidate probability parameter (0 &lt;= p &lt;= 1) â€¦\nGet distribution variance åˆ†å¸ƒã®åˆ†æ•£ã‚’å–å¾—\nBernoulli Distribution ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nBinary cross entropy for Bernoulli distributions â€¦\nCreate fair coin (p = 0.5) å…¬æ­£ãªã‚³ã‚¤ãƒ³ï¼ˆp = 0.5ï¼‰\nReturns the argument unchanged.\nCreate a new Bernoulli distribution from logits â€¦\nCreate a new Bernoulli distribution from probability â€¦\nCreate Bernoulli distribution with scalar probability â€¦\nGet logits (convert from probs if necessary) â€¦\nGet probabilities (convert from logits if necessary) â€¦\nCalls <code>U::from(self)</code>.\nLogits parameter (log-odds) - optional â€¦\nProbability parameter (p) - optional ç¢ºç‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â€¦\nBeta Distribution ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nSecond concentration parameter (Î²) â€¦\nFirst concentration parameter (Î±) â€¦\nReturns the argument unchanged.\nCreate Beta distribution with scalar parameters â€¦\nCalls <code>U::from(self)</code>.\nCreate a new Beta distribution â€¦\nSymmetric Beta distribution with equal parameters (Î±=Î²) â€¦\nUniform Beta distribution (Î±=1, Î²=1) - equivalent to â€¦\nCategorical Distribution ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nCompute cross entropy loss â€¦\nReturns the argument unchanged.\nCreate a new Categorical distribution from logits â€¦\nCreate a new Categorical distribution from probabilities â€¦\nGet logits (convert from probs if necessary) â€¦\nGet probabilities (convert from logits if necessary) â€¦\nCalls <code>U::from(self)</code>.\nLogits parameters (unnormalized log probabilities) - â€¦\nNumber of categories ã‚«ãƒ†ã‚´ãƒªæ•°\nProbability parameters - optional ç¢ºç‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ - â€¦\nCreate uniform categorical distribution â€¦\nBase Distribution struct for common functionality â€¦\nDistribution registry for factory pattern â€¦\nList all available distribution types â€¦\nBatch shape of the distribution åˆ†å¸ƒã®ãƒãƒƒãƒå½¢çŠ¶\nBroadcast two shapes together â€¦\nEvent shape of the distribution åˆ†å¸ƒã®ã‚¤ãƒ™ãƒ³ãƒˆå½¢çŠ¶\nExpand batch dimensions for sampling â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet distribution name for a given type â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new Distribution æ–°ã—ã„åˆ†å¸ƒã‚’ä½œæˆ\nGet the shape of a single sample â€¦\nWhether to validate parameters â€¦\nValidate tensor shape matches expected shape â€¦\nExponential Distribution æŒ‡æ•°åˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nReturns the argument unchanged.\nCreate Exponential distribution with scalar rate â€¦\nCreate exponential distribution from scale parameter â€¦\nCreate exponential distribution from scalar scale parameter\nCalls <code>U::from(self)</code>.\nCreate a new Exponential distribution â€¦\nRate parameter (Î») ãƒ¬ãƒ¼ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Î»)\nGet the scale parameter (1/rate) â€¦\nStandard exponential distribution (Î»=1) â€¦\nGamma Distribution ã‚¬ãƒ³ãƒåˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nShape parameter (Î±, concentration) â€¦\nCreate standard Gamma distribution (shape=1, scale=1) - â€¦\nReturns the argument unchanged.\nCreate a new Gamma distribution with concentration and rate\nCreate a new Gamma distribution with concentration and â€¦\nGet rate parameter (convert from scale if necessary) â€¦\nGet scale parameter (convert from rate if necessary) â€¦\nCalls <code>U::from(self)</code>.\nRate parameter (Î²) - optional if scale is provided â€¦\nScale parameter (Î¸ = 1/Î²) - optional if rate is provided â€¦\nNormal Distribution æ­£è¦åˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nReturns the argument unchanged.\nCreate Normal distribution with scalar parameters â€¦\nCalls <code>U::from(self)</code>.\nMean parameter (Î¼) å¹³å‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Î¼)\nCreate a new Normal distribution â€¦\nStandard deviation parameter (Ïƒ) â€¦\nStandard normal distribution (Î¼=0, Ïƒ=1) â€¦\nUniform Distribution ä¸€æ§˜åˆ†å¸ƒ\nBase distribution properties åŸºæœ¬åˆ†å¸ƒç‰¹æ€§\nReturns the argument unchanged.\nCreate Uniform distribution with scalar parameters â€¦\nUpper bound parameter (b) ä¸Šé™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (b)\nCalls <code>U::from(self)</code>.\nLower bound parameter (a) ä¸‹é™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (a)\nCreate a new Uniform distribution â€¦\nStandard uniform distribution on [0, 1) [0, â€¦\nSymmetric uniform distribution around zero [-a, a) â€¦\n16-bit brain floating point (bfloat16)\nBoolean\nComplex 128-bit (64-bit real + 64-bit imaginary)\nComplex 64-bit (32-bit real + 32-bit imaginary)\nData type enumeration\n16-bit floating point (half precision)\n32-bit floating point (single precision)\n64-bit floating point (double precision)\n16-bit signed integer\n32-bit signed integer\n64-bit signed integer\n8-bit signed integer\n16-bit unsigned integer\n32-bit unsigned integer\n64-bit unsigned integer\n8-bit unsigned integer\nGet the common dtype for two dtypes (promotion)\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if this is a boolean type\nCheck if two dtypes are compatible for operations\nCheck if this is a complex type\nCheck if this is a floating point type\nCheck if this is an integer type\nCheck if this is a signed integer type\nCheck if this is an unsigned integer type\nGet the maximum value for this data type (if applicable)\nGet the minimum value for this data type (if applicable)\nParse dtype from string\nGet the size in bytes of this data type\nGet the corresponding floating point type for this dtype\nGet the corresponding integer type for this dtype\nAutomatic differentiation error è‡ªå‹•å¾®åˆ†ã‚¨ãƒ©ãƒ¼\nBackend not available ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒåˆ©ç”¨ä¸å¯\nCreate cluster error\nCreate communication error\nCreate computation error\nCreate config error\nCreate configuration error\nCreate convergence error\nData loading error ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼\nDataLoader system errors (Phase 5) â€¦\nCreate dataset error\nDebug system and logging errors â€¦\nCreate deserialization error\nDevice operation failed ãƒ‡ãƒã‚¤ã‚¹æ“ä½œå¤±æ•—\nCreate device not available error\nDistributed computing errors â€¦\nCreate domain error\nCreate download error\nContains the error value\nCreate file not found error\nGPU-specific errors GPUå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nInput/Output and serialization errors â€¦\nImport-specific error ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›ºæœ‰ã‚¨ãƒ©ãƒ¼\nCreate incompatible shapes error\nCreate invalid data format error\nCreate invalid dimension error\nCreate invalid dimensions error\nCreate invalid image shape error\nCreate invalid model error\nCreate invalid operation error\nInvalid operation ç„¡åŠ¹ãªæ“ä½œ\nInvalid operation parameters ç„¡åŠ¹ãªæ“ä½œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nCreate invalid rank error\nCreate invalid transform params error\nCreate IO error\nKernel compilation error for GPU operations â€¦\nCreate kernel error\nCreate kernel execution error\nMemory allocation failed ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦å¤±æ•—\nCreate memory error\nCreate mismatched dimensions error\nModel import/export error â€¦\nCreate model not found error\nNeural network layer error â€¦\nFeature not implemented yet æ©Ÿèƒ½æœªå®Ÿè£…\nCreate not singleton dimension error\nContains the success value\nOut of memory error ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼\nCreate overflow error\nCreate parse error\nCreate plotting error\nCreate process group error\nProfiling and performance analysis errors â€¦\nCreate reshape error\nMain error type for RusTorch operations â€¦\nUnified Result type for all RusTorch operations - the ONLY â€¦\nSerialization error ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼\nCreate serialization error\nSerialization and model I/O errors (Phase 9) â€¦\nShape mismatch between tensors â€¦\nTensor operation failed ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œå¤±æ•—\nCreate unsupported device error\nCreate unsupported operation error\nData validation and quality assurance errors â€¦\nCreate validation error\nCreate verification error\nVision processing errors è¦–è¦šå‡¦ç†ã‚¨ãƒ©ãƒ¼\nVisualization errors å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼\nCreate autograd error\nCreate backend unavailable error\nCreate backward pass error\nError context and diagnostic information â€¦\nCreate data loading error\nCreate dataset error\nCreate a device error\nCreate device not available error\nCreate distributed error\nCreate empty tensor error\nCreate forward pass error\nReturns the argument unchanged.\nCreate a GPU error\nCreate gradient computation error\nCreate import error\nCreate index out of bounds error\nCalls <code>U::from(self)</code>.\nCreate invalid configuration error\nCreate invalid dimension error\nCreate invalid image format error\nCreate invalid image shape error\nCreate invalid parameter error (simple)\nCreate invalid parameters error\nCreate kernel compilation error\nCreate memory error (simple)\nCreate memory allocation error\nCreate model I/O error\nCreate neural network error\nCreate not implemented error\nCreate numeric error\nCreate out of memory error\nCreate parallel error\nCreate plotting error\nCreate process group error\nCreate quantization calibration failed error\nCreate quantization hardware not supported error\nCreate incompatible quantization schemes error\nCreate invalid quantization parameters error\nCreate quantization range overflow error\nCreate serialization error\nCreate a shape mismatch error\nCreate a tensor operation error\nCreate type error\nCreate unsupported format error\nCreate vision processing error\nCreate visualization error\nActual tensor shape that was provided â€¦\nAvailable memory in bytes â€¦\nName of the unavailable backend â€¦\nDevice identifier where the error occurred â€¦\nDevice where allocation failed â€¦\nExpected tensor shape æœŸå¾…ã•ã‚Œã‚‹ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nFeature that is not yet implemented æœªå®Ÿè£…ã®æ©Ÿèƒ½\nName of the neural network layer â€¦\nError message describing the tensor operation failure â€¦\nError message describing the device issue â€¦\nDescription of the parameter issue â€¦\nDetailed error message è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\nError message from the layer â€¦\nAutograd error description è‡ªå‹•å¾®åˆ†ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nModel I/O error description ãƒ¢ãƒ‡ãƒ«I/Oã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nImport operation error message â€¦\nData loading error description â€¦\nGPU operation error description GPUæ“ä½œã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nVision processing error description â€¦\nDistributed computing error description â€¦\nVisualization error description å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nProfiling error description â€¦\nData validation error description â€¦\nDebug system error description â€¦\nKernel compilation error description â€¦\nDataLoader error description DataLoaderã‚¨ãƒ©ãƒ¼ã®èª¬æ˜\nSerialization error description â€¦\nSerialization error description â€¦\nName of the operation with invalid parameters â€¦\nName of the invalid operation ç„¡åŠ¹ãªæ“ä½œå\nOperation that caused the serialization error â€¦\nRequested memory in bytes â€¦\nSize of the failed allocation in bytes â€¦\nOptional underlying error cause â€¦\nOptional underlying I/O error â€¦\nOptional underlying import error â€¦\nOptional underlying data loading error â€¦\nOptional underlying GPU error â€¦\nOptional underlying vision error â€¦\nOptional underlying distributed error â€¦\nOptional underlying visualization error â€¦\nError context for providing additional diagnostic â€¦\nFile location information ãƒ•ã‚¡ã‚¤ãƒ«ä½ç½®æƒ…å ±\nTrait for adding context to errors â€¦\nColumn number åˆ—ç•ªå·\nSource file name ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å\nGet formatted context information â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLine number è¡Œç•ªå·\nFile location where error occurred â€¦\nAdditional metadata è¿½åŠ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nCreate a new error context â€¦\nOperation that was being performed â€¦\nAdd operation to stack trace â€¦\nStack trace of operations æ“ä½œã®ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹\nAdd context to the error â€¦\nAdd location information ä½ç½®æƒ…å ±ã‚’è¿½åŠ \nAdd metadata to the context â€¦\nAdd simple operation context â€¦\nDynamic computation graph execution engine â€¦\nRuntime execution engine with JIT compilation and â€¦\nElement-wise addition\nBatch normalization\nConvolution operation\nCustom operation with name\nDropout\nDynamic execution context for runtime graph management â€¦\nDynamic execution statistics å‹•çš„å®Ÿè¡Œçµ±è¨ˆ\nDynamic execution node containing operation and runtime â€¦\nDynamic operation types for runtime execution â€¦\nExecution plan for optimized graph execution â€¦\nFused operation that combines multiple operations â€¦\nJIT compilation context for dynamic operations â€¦\nJIT compilation statistics JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çµ±è¨ˆ\nLinear transformation\nMatrix multiplication\nMemory allocation entry ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚¨ãƒ³ãƒˆãƒª\nMemory allocation plan ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ãƒ—ãƒ©ãƒ³\nElement-wise multiplication\nPlanned operation with optimization metadata â€¦\nReLU activation\nReshape operation\nSigmoid activation\nAdd a leaf node (input/parameter)\nAdd a dynamic operation node\nAdd operation to plan\nMemory allocation schedule\nAverage execution speedup\nCache hit rate\nCache hits\nCached output tensor\nClear all cached outputs and force recomputation\nTotal compilation time\nNumber of compilations\nCompile a sequence of operations into optimized function\nCreate execution plan with memory optimization\nWhether this node needs recomputation\nGet estimated total execution time\nEstimated execution time\nExecute the graph and return output of specified node\nExecute a single node\nExecution time tracking\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cached output if available\nGet dynamic node by ID\nGet execution statistics\nGet compilation statistics\nNode ID for tracking\nInput node IDs\nInput node references\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if node is dirty\nJIT compilations performed\nLifetime (when this memory can be freed)\nMark this node as dirty (needs recomputation)\nMemory allocations\nMemory allocation plan\nMemory requirements\nMemory usage tracking\nCreate a new dynamic node\nCreate new dynamic execution context\nCreate new JIT compiler\nCreate new fused operation\nCreate new execution plan\nNode ID\nOperation type\nOperation type\nOperation that needs this memory\nOrdered operations\nOptimize execution order for parallelism\nOptimize memory usage by analyzing lifetimes\nParallel execution opportunities\nCan be executed in parallel with previous operations\nPeak memory usage\nGet peak memory usage\nCan reuse memory from previous allocation\nMemory reuse opportunities\nSet cached output\nSize in bytes\nTotal execution time\nTotal operations executed\nBottleneck information ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±\nCached execution for pattern reuse â€¦\nGraph builder for fluent API â€¦\nJIT compilation metrics JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹\nMemory usage metrics ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹\nParallel execution metrics ä¸¦åˆ—å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹\nProfiling result with performance analysis â€¦\nRuntime configuration å®Ÿè¡Œæ™‚è¨­å®š\nRuntime execution engine for dynamic graph execution â€¦\nRuntime performance metrics â€¦\nAdd element-wise addition\nAdd execution time measurement\nAdd input tensor\nAdd operation\nAdd parameter tensor\nAllocation count\nAnalyze performance and generate recommendations\nAverage compilation time\nAverage execution time\nAverage parallelism factor\nAverage speedup from JIT\nCache hit rate\nClean up old cache entries\nRuntime configuration\nDynamic execution context\nAdd conv2d layer\nCurrent memory usage\nDeallocation count\nEnable operation fusion\nEnable JIT compilation\nEnable memory optimization\nEnable parallel execution\nExecute a computation graph with runtime optimization\nExecution cache for common patterns\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet runtime metrics\nHit count\nExpected input shapes\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nJIT compilation statistics\nJIT compilation threshold (operations)\nLast used timestamp\nAdd linear layer\nAdd matrix multiplication\nMaximum cache size\nMemory efficiency (reuse rate)\nMemory statistics\nCreate new runtime engine\nCreate new graph builder\nCreate new profile result\nOperation type causing bottleneck\nOutput shape\nParallel efficiency\nParallel executions performed\nParallel execution opportunities\nParallel execution statistics\nPeak memory usage\nExecution plan\nProfile execution and suggest optimizations\nRecommended optimization\nAdd ReLU activation\nReset all metrics\nAdd reshape operation\nAdd sigmoid activation\nSuccessful compilations\nGet performance summary\nPercentage of total time\nTotal compilations\nTotal executions\nWarm up the engine with common operations\nPyTorch compatibility format for RusTorch â€¦\nPyTorch model wrapper for RusTorch â€¦\nPyTorch state dict format for RusTorch â€¦\nSerializable tensor data â€¦\nAdd metadata ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ \nAdd tensor to state dict â€¦\nOptional architecture description â€¦\nTensor data as f64 for maximum precision â€¦\nData type string identifier ãƒ‡ãƒ¼ã‚¿å‹æ–‡å­—åˆ—è­˜åˆ¥å­\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nLoad model from PyTorch state dict â€¦\nGet layer bias ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒã‚¤ã‚¢ã‚¹ã‚’å–å¾—\nGet layer weights ãƒ¬ã‚¤ãƒ¤ãƒ¼é‡ã¿ã‚’å–å¾—\nGet metadata ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\nGet tensor from state dict â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet layer names ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å–å¾—\nLoad model from file â€¦\nLoad state dict from file (JSON format) â€¦\nOptional metadata for the state dict â€¦\nCreate new PyTorch model æ–°ã—ã„PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\nCreate new empty state dict â€¦\nSave model to file ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\nSave state dict to file (JSON format) â€¦\nSet model architecture description â€¦\nSet layer bias ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒã‚¤ã‚¢ã‚¹ã‚’è¨­å®š\nSet layer weights ãƒ¬ã‚¤ãƒ¤ãƒ¼é‡ã¿ã‚’è¨­å®š\nShape of the tensor ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\nThe modelâ€™s state dictionary containing parameters â€¦\nGet all tensor names å…¨ãƒ†ãƒ³ã‚½ãƒ«åã‚’å–å¾—\nMap of tensor names to tensor data â€¦\nUtility functions for PyTorch compatibility â€¦\nConvert layer name from PyTorch to RusTorch convention â€¦\nExtract model statistics ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆã‚’æŠ½å‡º\nConvert layer name from RusTorch to PyTorch convention â€¦\nValidate model structure ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’æ¤œè¨¼\nActivation functions\nAuto-select best available device â€¦\nColumn-major (Fortran-style) layout â€¦\nComplex number operations (CoreML unsupported)\nConvolution operations\nCPU device CPUãƒ‡ãƒã‚¤ã‚¹\nCUDA GPU device CUDA GPUãƒ‡ãƒã‚¤ã‚¹\nCustom kernel operations (CoreML unsupported)\nDevice capability information ãƒ‡ãƒã‚¤ã‚¹èƒ½åŠ›æƒ…å ±\nGPU device manager GPUãƒ‡ãƒã‚¤ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\nGPU device types with CoreML and hybrid support GPU â€¦\nDistributed operations (CoreML unsupported)\nStatistical distributions (CoreML unsupported)\nGPU operation context GPUæ¼”ç®—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\nGPU device types for fallback â€¦\nLinear algebra operations (matmul, etc.)\nGPU memory layout GPUãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\nMetal GPU device (for Apple Silicon) Metal â€¦\nNormalization operations\nOperation types for device capability checking â€¦\nOpenCL GPU device OpenCL GPUãƒ‡ãƒã‚¤ã‚¹\nReduction operations (sum, mean, etc.)\nRow-major (C-style) layout â€¦\nGPU activation operations and optimization â€¦\nGet available devices åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\nPerformance benchmark suite for GPU operations â€¦\nGPU convolution operations and cuDNN/MPS integration â€¦\nEnhanced CUDA cuBLAS integration for high-performance â€¦\nCUDA kernel implementations CUDAã‚«ãƒ¼ãƒãƒ«å®Ÿè£… CUDA â€¦\nGet current context ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\nGet the current device ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\nGet current device ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\nCustom GPU kernels for specialized tensor operations â€¦\nDevice management module for GPU operations â€¦\nGet the device type ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’å–å¾—\nDevice caching module for optimized initialization â€¦\nDistributed training infrastructure for multi-GPU learning â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet the global device manager â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if the device is available â€¦\nCheck if CUDA is available â€¦\nCheck if any GPU is available â€¦\nCheck if GPU is available GPUãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯\nCheck if Metal is available â€¦\nGPU kernel execution and management â€¦\nGPU matrix operations and BLAS integration â€¦\nGPU memory management and allocation â€¦\nGPU memory operations (modular implementation) â€¦\nGet memory pool size ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—\nGPU memory transfer operations GPUãƒ¡ãƒ¢ãƒªè»¢é€æ“ä½œ â€¦\nMetal Performance Shaders kernel implementations for GPU â€¦\nMulti-GPU distributed processing and communication â€¦\nMulti-GPU performance profiling and benchmarking â€¦\nCreate a new GPU context â€¦\nCreate a new device manager â€¦\nOpenCL kernel implementations for GPU acceleration â€¦\nOptimized OpenCL implementation for cross-platform GPU â€¦\nPerformance Benchmark Suite for GPU Kernels â€¦\nGPU performance optimizer â€¦\nGPU reduction operations and optimizations â€¦\nSet the current device globally â€¦\nSet current device ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\nSimple Metal GPU testing and benchmarking â€¦\nSmart device selection module for optimized operation â€¦\nGet number of streams ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã‚’å–å¾—\nCheck if device supports specific operation â€¦\nGPU synchronization primitives for multi-GPU operations â€¦\nUnified kernel interface for cross-platform GPU â€¦\nGPU kernel validation and testing â€¦\nGPU vs CPU verification tests GPU vs CPUæ¤œè¨¼ãƒ†ã‚¹ãƒˆ â€¦\nGPU activation operations trait â€¦\nGPU ELU activation GPU ELUæ´»æ€§åŒ–\nGPU GELU activation GPU GELUæ´»æ€§åŒ–\nGPU Leaky ReLU activation GPU Leaky ReLUæ´»æ€§åŒ–\nGPU ReLU activation GPU ReLUæ´»æ€§åŒ–\nGPU Sigmoid activation GPU Sigmoidæ´»æ€§åŒ–\nGPU Softmax activation GPU Softmaxæ´»æ€§åŒ–\nGPU Swish activation GPU Swishæ´»æ€§åŒ–\nGPU Tanh activation GPU Tanhæ´»æ€§åŒ–\nBenchmark configuration for GPU performance testing â€¦\nBenchmark result data structures and utilities â€¦\nPerformance benchmark suite implementation â€¦\nBenchmark configuration ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nMaximum benchmark duration in milliseconds â€¦\nEnable FLOPS measurements FLOPSæ¸¬å®šã‚’æœ‰åŠ¹åŒ–\nEnable memory bandwidth measurements â€¦\nNumber of measurement iterations æ¸¬å®šåå¾©å›æ•°\nMinimum benchmark duration in milliseconds â€¦\nNumber of warmup iterations â€¦\nBenchmark result for a single operation â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate a new benchmark result â€¦\nAdd CPU timing information to the benchmark result â€¦\nAdd error message to the benchmark result â€¦\nAdd FLOPS count to the benchmark result â€¦\nAdd GPU timing information to the benchmark result â€¦\nAdd memory bytes to the benchmark result â€¦\nPerformance benchmark suite for GPU operations â€¦\nClear all results ã™ã¹ã¦ã®çµæœã‚’ã‚¯ãƒªã‚¢\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate a new performance benchmark â€¦\nGet benchmark results ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å–å¾—\nRun comprehensive benchmark suite â€¦\nGPU Convolution trait for convolution operations with â€¦\nGPU-accelerated 2D convolution with automatic device â€¦\nGPU-accelerated 3D convolution GPUåŠ é€Ÿ3Dç•³ã¿è¾¼ã¿\nGPU-accelerated transposed convolution (deconvolution) â€¦\nGPU-accelerated depthwise separable convolution â€¦\nGPU-accelerated grouped convolution â€¦\nNon-CUDA fallback implementation â€¦\nPerform batch matrix multiplication using CUDA cuBLAS CUDA â€¦\nExecute batch CUDA matrix multiplication â€¦\nPublic interface functions for CUDA operations â€¦\nReturns the argument unchanged.\nGet CUDA compute capability (major, minor) â€¦\nGet CUDA device memory information (free, total) â€¦\nCalls <code>U::from(self)</code>.\nPerform matrix multiplication using CUDA cuBLAS CUDA â€¦\nCreate a new CUDA matrix executor for the specified device â€¦\nBatch normalization ãƒãƒƒãƒæ­£è¦åŒ–\nConvolution ç•³ã¿è¾¼ã¿\nCUDA memory buffer CUDAãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡\nNon-CUDA fallback executor for compatibility â€¦\nCUDA kernel parameters CUDAã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿#[â€¦\nCUDA kernel types CUDAã‚«ãƒ¼ãƒãƒ«ã‚¿ã‚¤ãƒ—#[derive(Debug, â€¦\nCUDA device properties CUDAãƒ‡ãƒã‚¤ã‚¹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£#[â€¦\nElement-wise operations è¦ç´ ã”ã¨æ¼”ç®—\nMatrix multiplication è¡Œåˆ—ä¹—ç®—\nReduction operations ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³æ¼”ç®—\nBlock dimensions ãƒ–ãƒ­ãƒƒã‚¯æ¬¡å…ƒ\nCompute capability version (major, minor) â€¦\nPerform 2D convolution using CUDA â€¦\nExecute CUDA 2D convolution CUDA 2Dç•³ã¿è¾¼ã¿ã‚’å®Ÿè¡Œ\nExecute CUDA element-wise addition â€¦\nPublic interface functions for CUDA operations â€¦\nExecute CUDA reduction sum â€¦\nCUDA kernel optimization utilities â€¦\nDevice ID ãƒ‡ãƒã‚¤ã‚¹ID\nPerform element-wise addition using CUDA â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGrid dimensions ã‚°ãƒªãƒƒãƒ‰æ¬¡å…ƒ\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nPerform matrix multiplication using CUDA â€¦\nMaximum shared memory size æœ€å¤§å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º\nMaximum threads per block â€¦\nTotal memory size ç·ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º\nDevice name ãƒ‡ãƒã‚¤ã‚¹å\nCreate a new CUDA kernel executor for the specified device â€¦\nPerform reduction sum using CUDA â€¦\nShared memory size å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º\nSize in elements è¦ç´ æ•°\nStream ID ã‚¹ãƒˆãƒªãƒ¼ãƒ ID\nWarp size ãƒ¯ãƒ¼ãƒ—ã‚µã‚¤ã‚º\nCalculate optimal grid and block dimensions â€¦\nGet device properties ãƒ‡ãƒã‚¤ã‚¹ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å–å¾—\nAttention mechanism kernel â€¦\nBatch normalization kernel ãƒãƒƒãƒæ­£è¦åŒ–ã‚«ãƒ¼ãƒãƒ«\nBoolean parameter çœŸå½å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nCompiled kernel representation â€¦\nCustom activation functions ã‚«ã‚¹ã‚¿ãƒ æ´»æ€§åŒ–é–¢æ•°\nCustom kernel manager â€¦\nCustom kernel types for specialized operations â€¦\nFast Fourier Transform kernel â€¦\nFloat parameter æµ®å‹•å°æ•°ç‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nFloat array parameter æµ®å‹•å°æ•°ç‚¹é…åˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nInteger parameter æ•´æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nInteger array parameter æ•´æ•°é…åˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nCustom kernel configuration ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«è¨­å®š#â€¦\nKernel parameter types â€¦\nKernel performance statistics â€¦\nOptimized convolution kernel â€¦\nMemory-optimized reduction â€¦\nSparse matrix operations ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—æ¼”ç®—\nString parameter æ–‡å­—åˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nTensor fusion operations ãƒ†ãƒ³ã‚½ãƒ«èåˆæ¼”ç®—\nCompiled binary data â€¦\nSize of the compiled binary in bytes â€¦\nBlock size for kernel execution (x, y, z) â€¦\nTime taken to compile the kernel â€¦\nTime when the kernel was compiled â€¦\nCompile and cache custom kernel â€¦\nKernel entry point function name â€¦\nExecute custom kernel ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè¡Œ\nNumber of times the kernel has been executed â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet kernel performance statistics â€¦\nGrid size for kernel execution (x, y, z) â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nType of custom kernel to execute â€¦\nType of the kernel ã‚«ãƒ¼ãƒãƒ«ã®ã‚¿ã‚¤ãƒ—\nType of the compiled kernel â€¦\nCreate new custom kernel manager â€¦\nKernel parameters ã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nShared memory size in bytes â€¦\nSource code of the kernel ã‚«ãƒ¼ãƒãƒ«ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰\nTotal time spent executing the kernel â€¦\nCPU device implementation#[derive(Debug)]\nCPU stream implementation#[derive(Debug)]\nGPU device capabilities GPUãƒ‡ãƒã‚¤ã‚¹æ©Ÿèƒ½#[â€¦\nDevice information and management â€¦\nDevice registry for managing multiple devices â€¦\nGPU backend for managing multiple devices\nGPU device trait\nGPU stream trait for asynchronous operations\nGet allocated memory in bytes\nAvailable memory in bytes â€¦\nGet best device for operation â€¦\nGet device capabilities ãƒ‡ãƒã‚¤ã‚¹æ©Ÿèƒ½ã‚’å–å¾—\nGet compute capability (for CUDA)\nCompute capability major version â€¦\nCompute capability minor version â€¦\nCreate device info for CPU â€¦\nCreate a stream for asynchronous operations\nCreate device info for CUDA device â€¦\nGet device type\nGet device type ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’å–å¾—\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet device by ID\nGet device info ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—\nGet device ID\nGet stream ID\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if device is available\nCheck if device is available â€¦\nCheck if device is CPU\nList all available devices\nList all available devices â€¦\nMaximum block dimensions æœ€å¤§ãƒ–ãƒ­ãƒƒã‚¯æ¬¡å…ƒ\nMaximum grid dimensions æœ€å¤§ã‚°ãƒªãƒƒãƒ‰æ¬¡å…ƒ\nMaximum threads per block â€¦\nGet memory usage information â€¦\nCreate device info for Metal device â€¦\nGet device name\nDevice name ãƒ‡ãƒã‚¤ã‚¹å\nCreate new GPU backend\nCreate a new CPU device æ–°ã—ã„CPUãƒ‡ãƒã‚¤ã‚¹ã‚’ä½œæˆ\nCreate a new CPU stream â€¦\nCreate a new device registry â€¦\nGet optimal block size for given problem size â€¦\nGet optimal grid size for given problem size and block size\nShared memory per block â€¦\nSupports double precision å€ç²¾åº¦ã‚µãƒãƒ¼ãƒˆ\nSupports half precision åŠç²¾åº¦ã‚µãƒãƒ¼ãƒˆ\nCheck if device supports operation â€¦\nSupports tensor cores ãƒ†ãƒ³ã‚µãƒ¼ã‚³ã‚¢ã‚µãƒãƒ¼ãƒˆ\nSynchronize device\nSynchronize this stream\nGet total memory in bytes\nTotal memory in bytes ç·ãƒ¡ãƒ¢ãƒªï¼ˆãƒã‚¤ãƒˆï¼‰\nWarp size ãƒ¯ãƒ¼ãƒ—ã‚µã‚¤ã‚º\nCache performance statistics â€¦\nCached device information â€¦\nCoreML-specific initialization cache â€¦\nDevice availability cache with lazy initialization â€¦\nDevice initialization result ãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–çµæœ\nClear expired cache entries â€¦\nInitialize CoreML with caching â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet cached device status â€¦\nGet cache statistics ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—\nGet global device cache instance â€¦\nGet global CoreML cache instance â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if device is available (with caching) â€¦\nCheck if cache entry is still valid (within 30 seconds) â€¦\nCreate new device cache â€¦\nReset initialization state (for testing) â€¦\nUpdate device cache ãƒ‡ãƒã‚¤ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°\nWarmup device cache by checking all common devices â€¦\nConstant learning rate\nCosine annealing\nDistributed training coordinator\nServer encountered an error\nExponential decay\nFault tolerance configuration\nLearning rate scheduling strategies\nLinear decay\nParameter server for distributed training\nParameter update record\nServer is running normally\nParameter server status\nServer is shutting down\nTraining configuration\nTraining performance metrics\nServer is temporarily unavailable\nWarm-up followed by decay\nApply parameter updates from all GPUs\nGet average GPU utilization\nAverage step time\nSynchronize all GPUs before critical operations\nCheckpoint frequency (in steps)\nEnable checkpointing\nCommunication overhead\nCommunication timeout\nGradient compression method\nGet efficiency ratio (useful computation vs total time)\nEnable performance profiling\nFault tolerance settings\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet GPU count\nGet training performance metrics\nGet profiling report\nGPU utilization per device\nEnable gradient accumulation on failure\nGradient synchronization time\nHandle GPU failure and recovery\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLearning rate scheduling\nMaximum retry attempts\nMemory usage per GPU\nParameter name\nCreate new distributed trainer\nCreate new parameter server\nCreate new metrics instance\nRetry delay\nSource GPU\nGet server status\nTotal training steps completed\nGradient synchronization frequency\nThroughput (samples/second)\nUpdate timestamp\nTotal training time\nExecute training step across all GPUs\nUpdate tensor\nVersion number\nElement-wise addition kernel è¦ç´ ã”ã¨åŠ ç®—ã‚«ãƒ¼ãƒãƒ«\nConvolution kernel ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«\nGPU kernel trait for different operations â€¦\nKernel executor for managing and running kernels â€¦\nGPU kernel execution parameters â€¦\nMatrix multiplication kernel è¡Œåˆ—ä¹—ç®—ã‚«ãƒ¼ãƒãƒ«\nModern GPU kernel trait without generic type parameter\nBlock/workgroup size â€¦\nCompile kernel from source\nGet device ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\nExecute the kernel ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè¡Œ\nExecute a kernel with automatic parameter optimization â€¦\nExecute a kernel with custom parameters â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGrid size ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSize of the convolution kernel â€¦\nLaunch kernel with given parameters\nGet kernel name\nCreate a new kernel executor â€¦\nGet optimal parameters for given problem size â€¦\nPadding applied to the input â€¦\nSet kernel parameter\nShared memory size å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º\nStream/queue ID ã‚¹ãƒˆãƒªãƒ¼ãƒ /ã‚­ãƒ¥ãƒ¼ID\nStride of the convolution operation â€¦\nBatch matrix multiplication for multiple matrices with GPU â€¦\nGPU-accelerated Linear Algebra operations\nGPU matrix multiplication executor\nPerform batch matrix multiplication with GPU acceleration â€¦\nGet the device type being used\nReturns the argument unchanged.\nReturns the argument unchanged.\nGPU batch matrix multiplication\nGPU matrix multiplication\nGPU matrix-vector multiplication\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if GPU acceleration is available\nCreate new matrix executor with device validation\nCreate new batch matrix executor with device validation\nData transfer operations between devices â€¦\nGPU memory manager for multiple devices â€¦\nGPU memory pool for efficient allocation â€¦\nGPU memory allocation information â€¦\nMemory utilization statistics ãƒ¡ãƒ¢ãƒªä½¿ç”¨çµ±è¨ˆ#[â€¦\nAllocate memory from the pool with optimized alignment â€¦\nAllocate memory on specific device â€¦\nCurrently allocated size in bytes\nGet basic memory usage statistics (legacy interface) â€¦\nGet memory statistics for all devices (legacy format) â€¦\nClear all pools å…¨ãƒ—ãƒ¼ãƒ«ã‚’ã‚¯ãƒªã‚¢\nDeallocate memory back to the pool â€¦\nDeallocate memory ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\nGet device ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\nDevice where memory is allocated â€¦\nCopy data between devices â€¦\nCopy data from device to host â€¦\nFragmentation ratio (lower is better)\nFree memory size in bytes\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet or create memory pool for device â€¦\nCopy data from host to device â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet current memory utilization statistics â€¦\nGet detailed memory statistics for all devices â€¦\nCreate a new GPU memory pool â€¦\nCreate a new GPU memory manager â€¦\nNumber of active allocations\nNumber of free blocks\nMemory pointer (platform-specific) â€¦\nSize in bytes ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰\nAllocation timestamp å‰²ã‚Šå½“ã¦ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—\nTotal pool size in bytes\nUtilization ratio (0.0 to 1.0)\nGPU Memory Buffer Abstraction â€¦\nCPU Fallback Operations for GPU Memory â€¦\nCUDA Memory Operations CUDAãƒ¡ãƒ¢ãƒªæ“ä½œ\nGPU Memory Manager GPUãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\nMetal Memory Operations Metalãƒ¡ãƒ¢ãƒªæ“ä½œ\nOpenCL Memory Operations OpenCLãƒ¡ãƒ¢ãƒªæ“ä½œ\nGPU Memory Transfer Operations GPUãƒ¡ãƒ¢ãƒªè»¢é€æ“ä½œ\nCPU fallback\nGPU memory buffer abstraction â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck if buffer is on CPU\nCheck if buffer is on CUDA device\nCheck if buffer is empty\nCheck if buffer is on Metal device\nCheck if buffer is on OpenCL device\nGet buffer size (number of elements)\nCPU fallback operations for GPU memory operations â€¦\nExecute attention mechanism using CPU fallback â€¦\nExecute batch normalization using CPU fallback â€¦\nExecute element-wise operation using CPU fallback â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStub for CUDA operations when CUDA is not available â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nGPU memory manager for tensor operations â€¦\nExecute attention operation (CPU fallback)\nExecute batch normalization on GPU buffer (CPU fallback)\nExecute element-wise operation on GPU buffers (CPU â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreate new GPU memory manager (CPU fallback) â€¦\nTransfer data from GPU to CPU (fallback implementation)\nTransfer tensor from CPU to GPU (fallback implementation)\nStub for Metal operations when Metal is not available â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nStub for OpenCL operations when OpenCL is not available â€¦\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nBatch normalization operations ãƒãƒƒãƒæ­£è¦åŒ–æ¼”ç®—\nConvolution operations ç•³ã¿è¾¼ã¿æ¼”ç®—\nElement-wise operations (add, mul, etc.) â€¦\nMatrix multiplication operations è¡Œåˆ—ä¹—ç®—æ¼”ç®—\nMetal buffer wrapper Metalãƒãƒƒãƒ•ã‚¡ãƒ©ãƒƒãƒ‘ãƒ¼\nNon-Metal fallback executor for compatibility â€¦\nMetal kernel parameters Metalã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nMetal kernel types Metalã‚«ãƒ¼ãƒãƒ«ã‚¿ã‚¤ãƒ—\nReduction operations (sum, mean, etc.) â€¦\nCopy data from host to Metal buffer â€¦\nCopy data from Metal buffer to host â€¦\nPerform element-wise addition using Metal â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nPerform matrix multiplication using Metal â€¦\nExecute Metal 2D convolution Metal 2Dç•³ã¿è¾¼ã¿ã‚’å®Ÿè¡Œ\nExecute Metal element-wise addition â€¦\nPublic interface functions for Metal operations â€¦\nExecute Metal reduction sum â€¦\nCreate a new Metal buffer with specified size â€¦\nCreate a new Metal kernel executor (fallback â€¦\nPerform reduction sum using Metal â€¦\nThreadgroups per grid for Metal kernel execution â€¦\nThreads per threadgroup for Metal kernel execution â€¦\nGradient aggregation methods\nAll-reduce algorithms\nSimple average\nBackward pass for micro-batch\nLoad balancing strategies\nBatch splitting strategy\nButterfly all-reduce\nColumn-wise partitioning\nCommunication backend types\nCommunication optimization settings\nCommunication group for collective operations\nCommunication manager for GPU-to-GPU transfers\nData parallelism - replicate model, split data\nData parallel trainer\nDelayed aggregation\nDouble binary tree\nDynamic based on current load\nDynamic based on processing speed\nError feedback compression\nEven split across GPUs\nExpert parallelism - for mixture of experts\nForward pass for micro-batch\nGPipe schedule\nGPU topology information\nGradient aggregation handler\nGradient compression methods\nHierarchical aggregation\nHost-staged transfers\nHybrid parallelism - combination of strategies\nHybrid partitioning\nInterleaved 1F1B\nLayer-wise partitioning\nLoad balancer for work distribution\nModel parallelism - split model across GPUs\nModel parallel partitioner\nMulti-GPU context manager\nNVIDIA NCCL\n1F1B (one forward one backward)\nDirect peer-to-peer\nMulti-GPU parallelism strategies\nModel partitioning strategies\nPipeDream schedule\nPipeline operation type\nPipeline parallelism - pipeline stages across GPUs\nPipeline scheduling algorithms\nPipeline parallel scheduler\nPredictive based on history\nQuantization\nAMD RCCL\nRandom sparsification\nRing all-reduce\nRow-wise partitioning\nSplit strategies for data parallelism\nStatic even distribution\nTensor-wise partitioning\nTop-K sparsification\nTransfer statistics\nTree all-reduce\nWeighted by GPU capability\nWeighted average\nWork stealing\nIntel XeLink\nAggregate gradients across GPUs\nAll-reduce operation across GPUs\nAll-reduce operation\nAverage bandwidth achieved\nNVLink/Interconnect bandwidth matrix (GB/s)\nBroadcast tensor from root GPU\nBroadcast operation\nEnable compression\nGPU compute capabilities\nCreate communication group\nGPU device IDs\nExecute operation across multiple GPUs\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nEnable fusion of small transfers\nGather tensors from all GPUs\nGather operation\nGenerate pipeline schedule\nGet device IDs\nGet GPU assignment for layer\nGet GPU IDs (alias for device IDs)\nGet recommended GPU for next operation\nGet number of GPUs\nParticipating GPU IDs\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCheck if GPU is available\nAvailable memory per GPU\nGroup name\nCheck if rebalancing is needed\nSimple constructor for testing\nCreate new communication manager\nCreate new load balancer\nCreate new data parallel trainer\nCreate new model parallel partitioner\nCreate new pipeline scheduler\nConstructor with strategy (renamed from original new)\nNumber of GPUs\nEnable overlap of computation and communication\nPeer-to-peer connectivity matrix\nPartition model across GPUs\nPeak bandwidth achieved\nGroup rank mapping\nRing buffer size for async operations\nRoot rank for broadcast operations\nScatter tensors across GPUs\nScatter operation\nSplit batch across GPUs\nTest P2P communication between two GPUs\nTotal bytes transferred\nTotal transfers\nUpdate load for GPU\nBatch size optimization\nTypes of performance bottlenecks\nCommunication bound\nCommunication performance metrics\nImprove communication patterns\nGPU compute bound\nGPU-specific performance metrics\nHardware configuration\nLoad balancing adjustment\nLoad imbalance\nMemory bandwidth bound\nMemory fragmentation\nOptimize memory usage\nMulti-GPU performance profiler\nOptimization recommendations\nAdjust parallelism strategy\nPerformance bottleneck identification\nPerformance analysis report\nPerformance snapshot for trend analysis\nPerformance trends analysis\nTypes of optimization recommendations\nSynchronization bound\nTraining session performance metrics\nSpecific actions to take\nAffected components\nAll-reduce operation times\nAnalyze performance and generate report\nAverage step time\nAverage step time\nBackward pass times\nNetwork bandwidth utilization\nBarrier synchronization times\nBottleneck type\nBottleneck identification\nBroadcast operation times\nCommunication metrics snapshot\nCommunication efficiency\nCommunication overhead ratio\nCommunication overhead trend\nImplementation complexity (1-10)\nCompute utilization percentage\nDescription\nDescription\nGPU device ID\nDisable profiling\nGPU efficiency score\nEnable profiling\nExpected performance gain percentage\nExport profiling data for external analysis\nForward pass times\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate comprehensive performance report\nGPU efficiency scores\nGPU metrics snapshot\nGPU utilization trend\nGradient synchronization times\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nKernel execution times\nMemory bandwidth utilization\nMemory efficiency trend\nMemory utilization percentage\nCreate new multi-GPU profiler\nOverall performance score (0-100)\nCommunication overhead percentage\nP2P transfer times\nImpact on overall performance\nPower consumption in watts\nPriority level (1-10)\nConvenience function to profile multi-GPU operations\nRecommendation type\nOptimization recommendations\nRecord communication operation\nRecord GPU kernel execution\nRecord training step metrics\nSession duration\nSeverity (0-100)\nStart profiling a multi-GPU operation\nStream synchronization times\nTemperature in Celsius\nThroughput (samples per second)\nThroughput trend (samples/sec over time)\nTimestamp\nTotal training steps\nTotal training steps\nTraining metrics snapshot\nMemory transfer times\nPerformance trends\nParameter update times\nBatch normalization operations ãƒãƒƒãƒæ­£è¦åŒ–æ¼”ç®—\nConvolution operations ç•³ã¿è¾¼ã¿æ¼”ç®—\nElement-wise operations (add, mul, etc.) â€¦\nMatrix multiplication operations è¡Œåˆ—ä¹—ç®—æ¼”ç®—\nOpenCL buffer wrapper OpenCLãƒãƒƒãƒ•ã‚¡ãƒ©ãƒƒãƒ‘ãƒ¼\nNon-OpenCL fallback executor for compatibility â€¦\nOpenCL kernel parameters OpenCLã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿#â€¦\nOpenCL kernel types OpenCLã‚«ãƒ¼ãƒãƒ«ã‚¿ã‚¤ãƒ—\nReduction operations (sum, mean, etc.) â€¦\nCopy buffer data to host memory (fallback when OpenCL not â€¦\nPerform elementwise addition on f32 arrays (fallback) â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate buffer from host data (fallback when OpenCL not â€¦\nGlobal work size for OpenCL kernel execution â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLocal work size for OpenCL kernel execution â€¦\nPerform matrix multiplication on f32 arrays (fallback) â€¦\nCreate a new OpenCL buffer (fallback when OpenCL not â€¦\nCreate a new OpenCL kernel executor (fallback when OpenCL â€¦\nExecute OpenCL element-wise addition â€¦\nPublic interface functions for OpenCL operations â€¦\nExecute OpenCL reduction sum â€¦\nQueue index for OpenCL command queue â€¦\nPerform reduction sum on f32 array (fallback) â€¦\nGet buffer size ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’å–å¾—\nOpenCL device information for optimization selection â€¦\nNon-OpenCL fallback implementation â€¦\nCompute units (cores) è¨ˆç®—ãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆã‚³ã‚¢ï¼‰\nDevice type (CPU/GPU/Accelerator) â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nGlobal memory size in bytes â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLocal memory size in bytes â€¦\nPerform matrix multiplication using OpenCL with â€¦\nMaximum clock frequency in MHz â€¦\nMaximum work group size â€¦\nDevice name ãƒ‡ãƒã‚¤ã‚¹å\nCreate a new OpenCL matrix executor with automatic device â€¦\nPublic interface functions for OpenCL operations â€¦\nVendor (AMD, Intel, NVIDIA) ãƒ™ãƒ³ãƒ€ãƒ¼ (AMD, Intel, â€¦\nBenchmark configuration ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š#[â€¦\nBenchmark result for a single operation â€¦\nPerformance benchmark suite for GPU operations â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum benchmark duration in milliseconds â€¦\nEnable FLOPS measurements FLOPSæ¸¬å®šã‚’æœ‰åŠ¹åŒ–\nEnable memory bandwidth measurements â€¦\nNumber of measurement iterations æ¸¬å®šåå¾©å›æ•°\nMinimum benchmark duration in milliseconds â€¦\nCreate a new benchmark result â€¦\nCreate a new performance benchmark â€¦\nRun comprehensive benchmark suite â€¦\nNumber of warmup iterations â€¦\nAdd CPU timing information to the benchmark result â€¦\nAdd error information to the benchmark result â€¦\nAdd FLOPS count to the benchmark result â€¦\nAdd GPU timing information to the benchmark result â€¦\nAdd memory bytes to the benchmark result â€¦\nPerformance profiler for device capabilities â€¦\nOptimization strategies for different scenarios â€¦\nPerformance benchmarking utilities â€¦\nPerformance metrics for GPU operations â€¦\nPerformance optimizer for GPU operations â€¦\nPerformance statistics summary â€¦\nDevice thermal state monitoring ãƒ‡ãƒã‚¤ã‚¹ç†±çŠ¶æ…‹ç›£è¦–\nBenchmark operation on different devices â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance statistics â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new performance optimizer â€¦\nRecord performance metrics for an operation â€¦\nRun comprehensive performance suite â€¦\nSelect optimal device for a given operation â€¦\nSet optimization strategy æœ€é©åŒ–æˆ¦ç•¥ã‚’è¨­å®š\nUpdate device profile with new measurements â€¦\nGPU reduction operations trait\nGPU reduction executor\nMaximum value reduction\nMean (average) reduction\nMinimum value reduction\nProduct reduction\nReduction operation types\nStandard deviation reduction\nSum reduction\nVariance reduction\nReturns the argument unchanged.\nReturns the argument unchanged.\nGPU max reduction\nGPU mean reduction\nGPU min reduction\nGPU standard deviation\nGPU sum reduction\nGPU variance\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new GPU reduction executor\nPerform reduction operation on GPU\nBenchmark CPU vs Metal performance â€¦\nTest Metal GPU availability and basic functionality Metal â€¦\nTest basic tensor operations with Metal backend â€¦\nDevice selection thresholds ãƒ‡ãƒã‚¤ã‚¹é¸æŠé–¾å€¤\nOperation characteristics for smart selection â€¦\nOperation type for device selection â€¦\nSmart device selector with operation-specific logic â€¦\nMaximum tensor size for CoreML (elements) â€¦\nMinimum tensor size for CoreML (elements) â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet fallback chain for operation â€¦\nMinimum memory footprint for GPU operations (bytes) â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalculate memory footprint in bytes â€¦\nMinimum tensor size for Metal GPU (elements) Metal â€¦\nSelect optimal device for the given operation â€¦\nSynchronization barrier\nEvent recording\nWait for event\nGPU event for synchronization\nGPU stream for asynchronous operations\nCompute kernel execution\nMemory copy operation\nMulti-GPU barrier for synchronization\nStream manager for coordinating GPU operations\nStream operations\nStream priority levels\nCompletion status\nCompletion notifier\nCreate new event\nCreate new stream on device\nCreation timestamp\nGPU device ID\nDevice ID\nAssociated events\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nEvent ID\nStream ID\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate new multi-GPU barrier\nCreate new stream manager\nOperation queue\nPriority level\nQuery event status\nRecord event on stream\nReset barrier state\nSynchronize all streams on device\nWait for all GPUs to reach barrier\nWait for event completion\nElement-wise addition è¦ç´ ã”ã¨ã®åŠ ç®—\nBatch normalization ãƒãƒƒãƒæ­£è¦åŒ–\n2D convolution 2Dç•³ã¿è¾¼ã¿\nElement-wise division è¦ç´ ã”ã¨ã®é™¤ç®—\nKernel performance metrics â€¦\nUnified kernel operation types â€¦\nKernel execution parameters â€¦\nSimplified kernel selector ç°¡æ½”ãªã‚«ãƒ¼ãƒãƒ«é¸æŠå™¨\nMatrix multiplication è¡Œåˆ—ä¹—ç®—\nElement-wise multiplication è¦ç´ ã”ã¨ã®ä¹—ç®—\nReLU activation ReLUæ´»æ€§åŒ–\nReduction mean ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³å¹³å‡\nReduction sum ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³åˆè¨ˆ\nSoftmax activation Softmaxæ´»æ€§åŒ–\nElement-wise subtraction è¦ç´ ã”ã¨ã®æ¸›ç®—\nUnified kernel executor çµ±ä¸€ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œè€…\nAdd executor å®Ÿè¡Œè€…ã‚’è¿½åŠ \nGet available devices åˆ©ç”¨å¯èƒ½ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\nGet device type ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ã‚’å–å¾—\nExecute kernel operation on f32 tensors â€¦\nExecute operation with best executor â€¦\nExecute kernel operation on f64 tensors â€¦\nExecute operation with best executor â€¦\nKernel execution time ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œæ™‚é–“\nAdditional kernel parameters â€¦\nFloating point operations per second â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet performance metrics â€¦\nInput tensor shapes for the kernel â€¦\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMemory bandwidth utilization (GB/s) â€¦\nCreate new unified kernel executor â€¦\nCreate new kernel selector â€¦\nGPU occupancy percentage GPUå æœ‰ç‡ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ\nOutput tensor shape å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶\nSelect best executor for operation â€¦\nCheck if operation is supported â€¦\nGPU kernel validator GPUã‚«ãƒ¼ãƒãƒ«æ¤œè¨¼å™¨\nValidation results for GPU operations â€¦\nThe GPU device type used for validation\nError message if validation failed\nExecution time in milliseconds\nReturns the argument unchanged.\nReturns the argument unchanged.\nGenerate a validation report æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMaximum error between expected and actual results\nCreate a new GPU validator æ–°ã—ã„GPUæ¤œè¨¼å™¨ã‚’ä½œæˆ\nName of the operation being validated\nWhether the validation passed\nPrint GPU validation report GPUæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›\nRun comprehensive GPU validation â€¦\nValidate all available GPU devices â€¦\nValidate element-wise addition operation â€¦\nValidate matrix multiplication operation â€¦\nValidate memory operations (allocation, copy, deallocation)\nTest tolerance for double precision floating point â€¦\nTest tolerance for floating point comparisons â€¦\nTest result structure ãƒ†ã‚¹ãƒˆçµæœæ§‹é€ ä½“\nVerification test suite for GPU operations â€¦\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new verification test suite â€¦\nCreate a new verification result â€¦\nRun all verification tests â€¦\nAdd error metrics to the verification result â€¦\nMark the verification as failed with error message â€¦\nAdd timing information to the verification result â€¦\nHigh-Performance BLAS Integration for RusTorch â€¦\nBenchmark different matrix multiplication implementations â€¦\nMulti-threaded matrix multiplication using Rayon â€¦\nFallback implementation when BLAS is not available â€¦\nComprehensive Memory Management System â€¦")