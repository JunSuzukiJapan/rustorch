#!/usr/bin/env python3
"""
Phase 2 Demo: Neural Network Foundation - achieving the implementation goal
"""

import rustorch

# ç›®æ¨™: åŸºæœ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
print("=== Phase 2 Demo: Basic Neural Network Foundation ===")

# Variable (è‡ªå‹•å¾®åˆ†å¯¾å¿œ)
x = rustorch.Variable(rustorch.tensor([1.0, 2.0]), requires_grad=True)
print(f"âœ“ Input Variable: {x}")
print(f"  Data shape: {x.data.shape}")  # [2]

# Linear Layer
linear = rustorch.Linear(2, 1, True)  # input_size=2, output_size=1
print(f"âœ“ Linear Layer: {linear}")

# Forward pass
y = linear(x)  # forward pass
print(f"âœ“ Forward pass output: {y}")
print(f"  Output shape: {y.data.shape}")  # [1]

# åŸºæœ¬çš„ãªè‡ªå‹•å¾®åˆ†
loss = y.sum()
print(f"âœ“ Loss: {loss}")

loss.backward()
print("âœ“ Backward pass completed")

grad = x.grad  # å‹¾é…æƒ…å ±
if grad is not None:
    print(f"âœ“ Input gradient: {grad}")
    print(f"  Gradient shape: {grad.shape}")
else:
    print("âŒ No gradient found")

# Additional demonstration: Variable arithmetic with autograd
print("\n=== Variable Arithmetic with Autograd ===")

# Create two variables
a = rustorch.Variable(rustorch.tensor([2.0, 3.0]), requires_grad=True)
b = rustorch.Variable(rustorch.tensor([4.0, 5.0]), requires_grad=True)

# Arithmetic operations
c = a + b  # Addition
d = c * a  # Multiplication
result = d.sum()  # Reduction

print(f"âœ“ a = {a}")
print(f"âœ“ b = {b}")
print(f"âœ“ c = a + b = {c}")
print(f"âœ“ d = c * a = {d}")
print(f"âœ“ result = d.sum() = {result}")

# Backward pass
result.backward()
print("âœ“ Backward pass through arithmetic operations completed")

if a.grad is not None:
    print(f"âœ“ a.grad = {a.grad}")
if b.grad is not None:
    print(f"âœ“ b.grad = {b.grad}")

print("\nğŸ‰ Phase 2 Neural Network Foundation Complete!")
print("âœ… Variable with autograd support")
print("âœ… Linear layer with learnable parameters")
print("âœ… Forward and backward passes working")
print("âœ… Variable arithmetic operations")
print("âœ… Gradient computation and accumulation")