# ãƒ•ã‚§ãƒ¼ã‚º2: é«˜åº¦æœ€é©åŒ–å™¨ - è©³ç´°æŠ€è¡“ä»•æ§˜æ›¸

## æ¦‚è¦

ãƒ•ã‚§ãƒ¼ã‚º2ã§ã¯ã€ç¾ä»£çš„ãªæ·±å±¤å­¦ç¿’ã«ä¸å¯æ¬ ãªé«˜åº¦æœ€é©åŒ–å™¨ç¾¤ã‚’å®Ÿè£…ã—ã¾ã™ã€‚Adamç³»ã®æ”¹è‰¯ç‰ˆã€æº–ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³æ³•ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€ãã—ã¦ç¢ºç‡çš„é‡ã¿å¹³å‡åŒ–ï¼ˆSWAï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€RusTorchã®PyTorchäº’æ›æ€§ã‚’55%ã‹ã‚‰65%ã«å‘ä¸Šã•ã›ã¾ã™ã€‚

## ğŸš€ **å®Ÿè£…å¯¾è±¡APIä¸€è¦§**

### **é«˜åº¦Adamç³»æœ€é©åŒ–å™¨**
- âœ… `AdamW` - é‡ã¿æ¸›è¡°ä»˜ãAdamï¼ˆæ—¢å­˜æ‹¡å¼µï¼‰
- ğŸ†• `NAdam` - Nesterovãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ä»˜ãAdam
- ğŸ†• `RAdam` - é©å¿œå­¦ç¿’ç‡ä¿®æ­£Adam
- ğŸ†• `Adamax` - ç„¡é™ãƒãƒ«ãƒ ç‰ˆAdam
- ğŸ†• `ASGD` - å¹³å‡åŒ–ç¢ºç‡çš„å‹¾é…é™ä¸‹

### **æº–ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³æ³•ãƒ»é«˜åº¦æœ€é©åŒ–å™¨**
- âœ… `LBFGS` - é™å®šè¨˜æ†¶BFGSï¼ˆæ—¢å­˜æ‹¡å¼µï¼‰
- ğŸ†• `Rprop` - Resilient Backpropagation
- ğŸ†• `SparseAdam` - ã‚¹ãƒ‘ãƒ¼ã‚¹å¯¾å¿œAdam

### **å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©**
- ğŸ†• `StepLR` - ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹æ¸›è¡°
- ğŸ†• `MultiStepLR` - ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—æ¸›è¡°  
- ğŸ†• `ExponentialLR` - æŒ‡æ•°æ¸›è¡°
- ğŸ†• `CosineAnnealingLR` - ã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°
- ğŸ†• `ReduceLROnPlateau` - ãƒ—ãƒ©ãƒˆãƒ¼ãƒ™ãƒ¼ã‚¹æ¸›å°‘
- ğŸ†• `CyclicLR` - å¾ªç’°å­¦ç¿’ç‡
- ğŸ†• `OneCycleLR` - ãƒ¯ãƒ³ã‚µã‚¤ã‚¯ãƒ«ãƒãƒªã‚·ãƒ¼
- ğŸ†• `LambdaLR` - ãƒ©ãƒ ãƒ€ãƒ™ãƒ¼ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°

### **ç¢ºç‡çš„é‡ã¿å¹³å‡åŒ–ï¼ˆSWAï¼‰**
- ğŸ†• `AveragedModel` - SWAãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼
- ğŸ†• `SWALR` - SWAå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©

---

## ğŸ”§ **è©³ç´°å®Ÿè£…ä»•æ§˜**

### **1. NAdamï¼ˆNesterov Adamï¼‰**

```rust
/// Nesterov accelerated Adam optimizer
/// ãƒã‚¹ãƒ†ãƒ­ãƒ•åŠ é€Ÿåº¦ä»˜ãAdamæœ€é©åŒ–å™¨
pub struct NAdam<T: Float + Clone + Send + Sync> {
    params: Vec<Tensor<T>>,
    lr: T,
    beta1: T,
    beta2: T,
    eps: T,
    weight_decay: T,
    momentum_decay: T,
    
    // State variables
    step: usize,
    exp_avg: Vec<Tensor<T>>,      // First moment estimates
    exp_avg_sq: Vec<Tensor<T>>,   // Second moment estimates
}

impl<T: Float + Clone + Send + Sync + 'static> NAdam<T> {
    pub fn new(
        params: Vec<Tensor<T>>,
        lr: T,
        betas: (T, T),
        eps: T,
        weight_decay: T,
        momentum_decay: T,
    ) -> Self {
        let (beta1, beta2) = betas;
        let num_params = params.len();
        
        Self {
            params,
            lr,
            beta1,
            beta2,
            eps,
            weight_decay,
            momentum_decay,
            step: 0,
            exp_avg: vec![Tensor::zeros_like(&params[0]); num_params],
            exp_avg_sq: vec![Tensor::zeros_like(&params[0]); num_params],
        }
    }
    
    pub fn step(&mut self, gradients: &[Tensor<T>]) -> RusTorchResult<()> {
        self.step += 1;
        let step_f = T::from(self.step).unwrap();
        
        // Bias correction
        let bias_correction1 = T::one() - self.beta1.powf(step_f);
        let bias_correction2 = T::one() - self.beta2.powf(step_f);
        
        for (i, (param, grad)) in self.params.iter_mut().zip(gradients.iter()).enumerate() {
            let mut grad = grad.clone();
            
            // Apply weight decay
            if self.weight_decay != T::zero() {
                grad = &grad + &(param * self.weight_decay);
            }
            
            // Update biased first moment estimate
            self.exp_avg[i] = &self.exp_avg[i] * self.beta1 + &grad * (T::one() - self.beta1);
            
            // Update biased second raw moment estimate  
            self.exp_avg_sq[i] = &self.exp_avg_sq[i] * self.beta2 + 
                &grad.elementwise_mul(&grad) * (T::one() - self.beta2);
            
            // Compute bias-corrected first moment
            let corrected_exp_avg = &self.exp_avg[i] / bias_correction1;
            
            // Compute bias-corrected second moment
            let corrected_exp_avg_sq = &self.exp_avg_sq[i] / bias_correction2;
            
            // Nesterov correction
            let momentum_factor = self.beta1 * (T::one() - 
                (T::from(0.96_f64).unwrap().powf(step_f * self.momentum_decay)));
            let nesterov_momentum = momentum_factor * &corrected_exp_avg + 
                (T::one() - self.beta1) * &grad / bias_correction1;
            
            // Update parameters
            let denominator = corrected_exp_avg_sq.sqrt() + self.eps;
            let update = nesterov_momentum.elementwise_div(&denominator) * self.lr;
            
            *param = param - &update;
        }
        
        Ok(())
    }
}
```

### **2. RAdamï¼ˆRectified Adamï¼‰**

```rust
/// Rectified Adam optimizer with variance rectification
/// åˆ†æ•£ä¿®æ­£ä»˜ãAdamæœ€é©åŒ–å™¨
pub struct RAdam<T: Float + Clone + Send + Sync> {
    params: Vec<Tensor<T>>,
    lr: T,
    beta1: T,
    beta2: T,
    eps: T,
    weight_decay: T,
    
    // State variables
    step: usize,
    exp_avg: Vec<Tensor<T>>,
    exp_avg_sq: Vec<Tensor<T>>,
    rho_inf: T, // Maximum length of approximated SMA
}

impl<T: Float + Clone + Send + Sync + 'static> RAdam<T> {
    pub fn new(
        params: Vec<Tensor<T>>,
        lr: T,
        betas: (T, T),
        eps: T,
        weight_decay: T,
    ) -> Self {
        let (beta1, beta2) = betas;
        let num_params = params.len();
        
        // Calculate rho_infinity
        let rho_inf = T::from(2.0).unwrap() / (T::one() - beta2) - T::one();
        
        Self {
            params,
            lr,
            beta1,
            beta2,
            eps,
            weight_decay,
            step: 0,
            exp_avg: vec![Tensor::zeros_like(&params[0]); num_params],
            exp_avg_sq: vec![Tensor::zeros_like(&params[0]); num_params],
            rho_inf,
        }
    }
    
    pub fn step(&mut self, gradients: &[Tensor<T>]) -> RusTorchResult<()> {
        self.step += 1;
        let step_f = T::from(self.step).unwrap();
        
        // Calculate current rho_t
        let beta2_power = self.beta2.powf(step_f);
        let rho_t = self.rho_inf - T::from(2.0).unwrap() * step_f * beta2_power / 
                   (T::one() - beta2_power);
        
        for (i, (param, grad)) in self.params.iter_mut().zip(gradients.iter()).enumerate() {
            let mut grad = grad.clone();
            
            // Apply weight decay
            if self.weight_decay != T::zero() {
                grad = &grad + &(param * self.weight_decay);
            }
            
            // Update moments
            self.exp_avg[i] = &self.exp_avg[i] * self.beta1 + &grad * (T::one() - self.beta1);
            self.exp_avg_sq[i] = &self.exp_avg_sq[i] * self.beta2 + 
                &grad.elementwise_mul(&grad) * (T::one() - self.beta2);
            
            // Bias correction
            let bias_correction1 = T::one() - self.beta1.powf(step_f);
            let corrected_exp_avg = &self.exp_avg[i] / bias_correction1;
            
            let update = if rho_t >= T::from(5.0).unwrap() {
                // Variance rectification available
                let bias_correction2 = T::one() - self.beta2.powf(step_f);
                let corrected_exp_avg_sq = &self.exp_avg_sq[i] / bias_correction2;
                
                // Length of SMA
                let r = ((rho_t - T::from(4.0).unwrap()) * (rho_t - T::from(2.0).unwrap()) * 
                        self.rho_inf / ((self.rho_inf - T::from(4.0).unwrap()) * 
                        (self.rho_inf - T::from(2.0).unwrap()) * rho_t)).sqrt();
                
                corrected_exp_avg.elementwise_div(&corrected_exp_avg_sq.sqrt().add_scalar(self.eps)) * r
            } else {
                // Simple momentum update
                corrected_exp_avg
            };
            
            *param = param - &(update * self.lr);
        }
        
        Ok(())
    }
}
```

### **3. å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©åŸºç›¤**

```rust
/// Base trait for learning rate schedulers
/// å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®åŸºåº•ãƒˆãƒ¬ã‚¤ãƒˆ
pub trait LRScheduler<T: Float> {
    fn step(&mut self, epoch: Option<usize>) -> RusTorchResult<()>;
    fn get_lr(&self) -> Vec<T>;
    fn state_dict(&self) -> HashMap<String, T>;
    fn load_state_dict(&mut self, state_dict: HashMap<String, T>) -> RusTorchResult<()>;
}

/// Step-based learning rate scheduler
/// ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
pub struct StepLR<T: Float> {
    optimizer: Arc<Mutex<dyn Optimizer<T>>>,
    step_size: usize,
    gamma: T,
    last_epoch: usize,
    base_lrs: Vec<T>,
}

impl<T: Float + Clone + Send + Sync> StepLR<T> {
    pub fn new(
        optimizer: Arc<Mutex<dyn Optimizer<T>>>,
        step_size: usize,
        gamma: T,
        last_epoch: usize,
    ) -> Self {
        let base_lrs = {
            let opt = optimizer.lock().unwrap();
            opt.get_lr()
        };
        
        Self {
            optimizer,
            step_size,
            gamma,
            last_epoch,
            base_lrs,
        }
    }
}

impl<T: Float + Clone + Send + Sync + 'static> LRScheduler<T> for StepLR<T> {
    fn step(&mut self, epoch: Option<usize>) -> RusTorchResult<()> {
        self.last_epoch = epoch.unwrap_or(self.last_epoch + 1);
        
        let factor = self.gamma.powf(T::from(self.last_epoch / self.step_size).unwrap());
        let new_lrs: Vec<T> = self.base_lrs.iter().map(|&lr| lr * factor).collect();
        
        {
            let mut opt = self.optimizer.lock().unwrap();
            opt.set_lr(&new_lrs)?;
        }
        
        Ok(())
    }
    
    fn get_lr(&self) -> Vec<T> {
        let factor = self.gamma.powf(T::from(self.last_epoch / self.step_size).unwrap());
        self.base_lrs.iter().map(|&lr| lr * factor).collect()
    }
    
    // ... state_dict implementations
}

/// Cosine annealing learning rate scheduler
/// ã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
pub struct CosineAnnealingLR<T: Float> {
    optimizer: Arc<Mutex<dyn Optimizer<T>>>,
    t_max: usize,
    eta_min: T,
    last_epoch: usize,
    base_lrs: Vec<T>,
}

impl<T: Float + Clone + Send + Sync + 'static> LRScheduler<T> for CosineAnnealingLR<T> {
    fn step(&mut self, epoch: Option<usize>) -> RusTorchResult<()> {
        self.last_epoch = epoch.unwrap_or(self.last_epoch + 1);
        
        let pi = T::from(std::f64::consts::PI).unwrap();
        let t_max_f = T::from(self.t_max).unwrap();
        let last_epoch_f = T::from(self.last_epoch).unwrap();
        
        let new_lrs: Vec<T> = self.base_lrs.iter().map(|&base_lr| {
            self.eta_min + (base_lr - self.eta_min) * 
            (T::one() + (pi * last_epoch_f / t_max_f).cos()) / T::from(2.0).unwrap()
        }).collect();
        
        {
            let mut opt = self.optimizer.lock().unwrap();
            opt.set_lr(&new_lrs)?;
        }
        
        Ok(())
    }
    
    fn get_lr(&self) -> Vec<T> {
        let pi = T::from(std::f64::consts::PI).unwrap();
        let t_max_f = T::from(self.t_max).unwrap();
        let last_epoch_f = T::from(self.last_epoch).unwrap();
        
        self.base_lrs.iter().map(|&base_lr| {
            self.eta_min + (base_lr - self.eta_min) * 
            (T::one() + (pi * last_epoch_f / t_max_f).cos()) / T::from(2.0).unwrap()
        }).collect()
    }
}
```

### **4. ç¢ºç‡çš„é‡ã¿å¹³å‡åŒ–ï¼ˆSWAï¼‰**

```rust
/// Averaged model for Stochastic Weight Averaging
/// ç¢ºç‡çš„é‡ã¿å¹³å‡åŒ–ã®ãŸã‚ã®å¹³å‡åŒ–ãƒ¢ãƒ‡ãƒ«
pub struct AveragedModel<M: Module> {
    model: M,
    avg_model: M,
    n_averaged: usize,
    use_buffers: bool,
}

impl<M: Module + Clone> AveragedModel<M> {
    pub fn new(model: M, use_buffers: bool) -> Self {
        let avg_model = model.clone();
        
        Self {
            model,
            avg_model,
            n_averaged: 0,
            use_buffers,
        }
    }
    
    /// Update averaged model with current model parameters
    /// ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¹³å‡åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
    pub fn update_parameters(&mut self) -> RusTorchResult<()> {
        self.n_averaged += 1;
        let n = self.n_averaged as f32;
        
        // Update averaged parameters
        for (avg_param, param) in self.avg_model.parameters_mut()
            .zip(self.model.parameters()) {
            
            // Moving average: avg = (n-1)/n * avg + 1/n * param
            *avg_param = &*avg_param * ((n - 1.0) / n) + param * (1.0 / n);
        }
        
        // Update buffers if requested
        if self.use_buffers {
            for (avg_buffer, buffer) in self.avg_model.buffers_mut()
                .zip(self.model.buffers()) {
                
                *avg_buffer = buffer.clone();
            }
        }
        
        Ok(())
    }
    
    /// Get the averaged model
    /// å¹³å‡åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    pub fn averaged_model(&self) -> &M {
        &self.avg_model
    }
}

/// SWA learning rate scheduler  
/// SWAå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
pub struct SWALR<T: Float> {
    optimizer: Arc<Mutex<dyn Optimizer<T>>>,
    swa_lr: T,
    anneal_epochs: usize,
    anneal_strategy: AnnealStrategy,
    last_epoch: usize,
    base_lrs: Vec<T>,
}

#[derive(Debug, Clone)]
pub enum AnnealStrategy {
    Linear,
    Cosine,
}

impl<T: Float + Clone + Send + Sync + 'static> SWALR<T> {
    pub fn new(
        optimizer: Arc<Mutex<dyn Optimizer<T>>>,
        swa_lr: T,
        anneal_epochs: usize,
        anneal_strategy: AnnealStrategy,
    ) -> Self {
        let base_lrs = {
            let opt = optimizer.lock().unwrap();
            opt.get_lr()
        };
        
        Self {
            optimizer,
            swa_lr,
            anneal_epochs,
            anneal_strategy,
            last_epoch: 0,
            base_lrs,
        }
    }
}

impl<T: Float + Clone + Send + Sync + 'static> LRScheduler<T> for SWALR<T> {
    fn step(&mut self, epoch: Option<usize>) -> RusTorchResult<()> {
        self.last_epoch = epoch.unwrap_or(self.last_epoch + 1);
        
        let new_lrs = if self.last_epoch < self.anneal_epochs {
            // Annealing phase
            let progress = T::from(self.last_epoch).unwrap() / T::from(self.anneal_epochs).unwrap();
            
            match self.anneal_strategy {
                AnnealStrategy::Linear => {
                    self.base_lrs.iter().map(|&base_lr| {
                        base_lr + (self.swa_lr - base_lr) * progress
                    }).collect()
                }
                AnnealStrategy::Cosine => {
                    let pi = T::from(std::f64::consts::PI).unwrap();
                    self.base_lrs.iter().map(|&base_lr| {
                        self.swa_lr + (base_lr - self.swa_lr) * 
                        (T::one() + (pi * progress).cos()) / T::from(2.0).unwrap()
                    }).collect()
                }
            }
        } else {
            // SWA phase - use constant SWA learning rate
            vec![self.swa_lr; self.base_lrs.len()]
        };
        
        {
            let mut opt = self.optimizer.lock().unwrap();
            opt.set_lr(&new_lrs)?;
        }
        
        Ok(())
    }
    
    fn get_lr(&self) -> Vec<T> {
        if self.last_epoch < self.anneal_epochs {
            let progress = T::from(self.last_epoch).unwrap() / T::from(self.anneal_epochs).unwrap();
            
            match self.anneal_strategy {
                AnnealStrategy::Linear => {
                    self.base_lrs.iter().map(|&base_lr| {
                        base_lr + (self.swa_lr - base_lr) * progress
                    }).collect()
                }
                AnnealStrategy::Cosine => {
                    let pi = T::from(std::f64::consts::PI).unwrap();
                    self.base_lrs.iter().map(|&base_lr| {
                        self.swa_lr + (base_lr - self.swa_lr) * 
                        (T::one() + (pi * progress).cos()) / T::from(2.0).unwrap()
                    }).collect()
                }
            }
        } else {
            vec![self.swa_lr; self.base_lrs.len()]
        }
    }
}
```

---

## ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ **

```
src/optim/
â”œâ”€â”€ mod.rs                    # æœ€é©åŒ–å™¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆ
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ mod.rs               # æœ€é©åŒ–å™¨ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ nadam.rs             # NAdamå®Ÿè£…
â”‚   â”œâ”€â”€ radam.rs             # RAdamå®Ÿè£…
â”‚   â”œâ”€â”€ adamax.rs            # Adamaxå®Ÿè£…
â”‚   â”œâ”€â”€ asgd.rs              # ASGDå®Ÿè£…
â”‚   â”œâ”€â”€ rprop.rs             # Rpropå®Ÿè£…
â”‚   â””â”€â”€ sparse_adam.rs       # SparseAdamå®Ÿè£…
â”œâ”€â”€ schedulers/
â”‚   â”œâ”€â”€ mod.rs               # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©çµ±åˆ
â”‚   â”œâ”€â”€ base.rs              # LRSchedulerãƒˆãƒ¬ã‚¤ãƒˆ
â”‚   â”œâ”€â”€ step_lr.rs           # StepLRå®Ÿè£…
â”‚   â”œâ”€â”€ multistep_lr.rs      # MultiStepLRå®Ÿè£…
â”‚   â”œâ”€â”€ exponential_lr.rs    # ExponentialLRå®Ÿè£…
â”‚   â”œâ”€â”€ cosine_annealing.rs  # CosineAnnealingLRå®Ÿè£…
â”‚   â”œâ”€â”€ reduce_on_plateau.rs # ReduceLROnPlateauå®Ÿè£…
â”‚   â”œâ”€â”€ cyclic_lr.rs         # CyclicLRå®Ÿè£…
â”‚   â”œâ”€â”€ onecycle_lr.rs       # OneCycleLRå®Ÿè£…
â”‚   â””â”€â”€ lambda_lr.rs         # LambdaLRå®Ÿè£…
â”œâ”€â”€ swa/
â”‚   â”œâ”€â”€ mod.rs               # SWAçµ±åˆ
â”‚   â”œâ”€â”€ averaged_model.rs    # AveragedModelå®Ÿè£…
â”‚   â””â”€â”€ swa_lr.rs            # SWALRå®Ÿè£…
â””â”€â”€ utils/
    â”œâ”€â”€ line_search.rs       # ç·šæ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    â”œâ”€â”€ momentum.rs          # ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆè¨ˆç®—
    â””â”€â”€ weight_decay.rs      # é‡ã¿æ¸›è¡°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
```

---

## ğŸ§ª **ãƒ†ã‚¹ãƒˆæˆ¦ç•¥**

### **å˜ä½“ãƒ†ã‚¹ãƒˆ**
```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_nadam_convergence() {
        // NAdamåæŸæ€§ãƒ†ã‚¹ãƒˆ
        let params = vec![Tensor::from_vec(vec![1.0, 1.0], vec![2])];
        let mut optimizer = NAdam::new(params, 0.001, (0.9, 0.999), 1e-8, 0.0, 0.004);
        
        // Simple quadratic function: f(x) = x^2
        for _ in 0..1000 {
            let gradients = vec![&params[0] * 2.0]; // df/dx = 2x
            optimizer.step(&gradients).unwrap();
        }
        
        // Should converge to zero
        assert!(params[0].data.iter().all(|&x| x.abs() < 0.01));
    }
    
    #[test]
    fn test_cosine_annealing_lr() {
        // ã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        let optimizer = Arc::new(Mutex::new(MockOptimizer::new(0.1)));
        let mut scheduler = CosineAnnealingLR::new(optimizer, 100, 0.0);
        
        let initial_lr = scheduler.get_lr()[0];
        assert_eq!(initial_lr, 0.1);
        
        // Half way through should be minimum
        for _ in 0..50 {
            scheduler.step(None).unwrap();
        }
        let mid_lr = scheduler.get_lr()[0];
        assert!(mid_lr < 0.01); // Should be close to eta_min
        
        // End should return to original
        for _ in 0..50 {
            scheduler.step(None).unwrap();
        }
        let final_lr = scheduler.get_lr()[0];
        assert!((final_lr - 0.1).abs() < 0.01);
    }
    
    #[test]
    fn test_swa_averaging() {
        // SWAå¹³å‡åŒ–ãƒ†ã‚¹ãƒˆ
        let model = MockModel::new();
        let mut swa_model = AveragedModel::new(model.clone(), false);
        
        // Update with different parameter values
        for i in 0..10 {
            model.set_parameter_value(i as f32);
            swa_model.update_parameters().unwrap();
        }
        
        // Average should be approximately (0+1+2+...+9)/10 = 4.5
        let avg_param = swa_model.averaged_model().get_parameter_value();
        assert!((avg_param - 4.5).abs() < 0.1);
    }
}
```

### **çµ±åˆãƒ†ã‚¹ãƒˆ**
```rust
#[test]
fn test_full_training_loop_with_scheduler() {
    // å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—ãƒ†ã‚¹ãƒˆ
    let model = SimpleLinearModel::new(10, 1);
    let optimizer = Arc::new(Mutex::new(RAdam::new(
        model.parameters(), 0.001, (0.9, 0.999), 1e-8, 0.01
    )));
    let mut scheduler = StepLR::new(optimizer.clone(), 30, 0.1, 0);
    
    for epoch in 0..100 {
        // Training step
        let loss = train_step(&model, &optimizer);
        
        // Update learning rate
        scheduler.step(Some(epoch)).unwrap();
        
        // Validate learning rate schedule
        if epoch == 29 {
            let lr = scheduler.get_lr()[0];
            assert!((lr - 0.001).abs() < 1e-6);
        } else if epoch == 30 {
            let lr = scheduler.get_lr()[0];
            assert!((lr - 0.0001).abs() < 1e-6);
        }
    }
}
```

### **æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```rust
#[bench]
fn bench_nadam_step(b: &mut Bencher) {
    let params = vec![Tensor::randn(&[1000, 1000])];
    let mut optimizer = NAdam::new(params.clone(), 0.001, (0.9, 0.999), 1e-8, 0.0, 0.004);
    let gradients = vec![Tensor::randn(&[1000, 1000])];
    
    b.iter(|| {
        optimizer.step(&gradients).unwrap();
    });
}

#[bench]
fn bench_cosine_scheduler_step(b: &mut Bencher) {
    let optimizer = Arc::new(Mutex::new(MockOptimizer::new(0.1)));
    let mut scheduler = CosineAnnealingLR::new(optimizer, 1000, 0.0);
    
    b.iter(|| {
        scheduler.step(None).unwrap();
    });
}
```

---

## ğŸš€ **å®Ÿè£…ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**

### **Week 1-2: åŸºç›¤æ•´å‚™**
- [ ] æœ€é©åŒ–å™¨åŸºåº•ãƒˆãƒ¬ã‚¤ãƒˆæ‹¡å¼µ
- [ ] ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©åŸºåº•ãƒˆãƒ¬ã‚¤ãƒˆå®Ÿè£…
- [ ] ãƒ†ã‚¹ãƒˆãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åŸºç›¤æ§‹ç¯‰
- [ ] CI/CDçµ±åˆ

### **Week 3-4: Adamç³»æœ€é©åŒ–å™¨**
- [ ] NAdamå®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆ
- [ ] RAdamå®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆ  
- [ ] Adamaxå®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆ
- [ ] ASGDå®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆ

### **Week 5-6: å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼ˆåŸºæœ¬ï¼‰**
- [ ] StepLR, MultiStepLRå®Ÿè£…
- [ ] ExponentialLRå®Ÿè£…
- [ ] CosineAnnealingLRå®Ÿè£…
- [ ] ReduceLROnPlateauå®Ÿè£…

### **Week 7-8: é«˜åº¦æ©Ÿèƒ½ãƒ»æœ€çµ‚åŒ–**
- [ ] CyclicLR, OneCycleLRå®Ÿè£…
- [ ] SWAå®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆ
- [ ] æ€§èƒ½æœ€é©åŒ–ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»çµ±åˆãƒ†ã‚¹ãƒˆ

---

## ğŸ“Š **äºˆæƒ³ã•ã‚Œã‚‹æ€§èƒ½å‘ä¸Š**

### **è¨“ç·´é€Ÿåº¦å‘ä¸Š**
- **NAdam**: å¾“æ¥Adamã‚ˆã‚Š10-20%é«˜é€ŸåæŸ
- **RAdam**: åˆæœŸè¨“ç·´ã§ã®å®‰å®šæ€§å‘ä¸Šï¼ˆ50%ä»¥ä¸Šã®åˆ†æ•£æ¸›å°‘ï¼‰
- **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©**: é©åˆ‡ãªå­¦ç¿’ç‡èª¿æ•´ã«ã‚ˆã‚‹20-30%ã®è¨“ç·´æ™‚é–“çŸ­ç¸®

### **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–**
- **åŠ¹ç‡çš„çŠ¶æ…‹ç®¡ç†**: æœ€é©åŒ–å™¨çŠ¶æ…‹ã®30%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **é…å»¶åˆæœŸåŒ–**: æœªä½¿ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¡ãƒ¢ãƒªç¯€ç´„
- **ãƒãƒƒãƒå‡¦ç†**: å‹¾é…ç´¯ç©ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Š

### **æ•°å€¤å®‰å®šæ€§**
- **RAdam**: ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–å­¦ç¿’ç‡ã«ã‚ˆã‚‹å‹¾é…çˆ†ç™ºé˜²æ­¢
- **é‡ã¿æ¸›è¡°**: æ­£å‰‡åŒ–ã«ã‚ˆã‚‹éå­¦ç¿’æŠ‘åˆ¶
- **æ··åˆç²¾åº¦**: FP16å¯¾å¿œã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã¨ãƒ¡ãƒ¢ãƒªç¯€ç´„

---

## ğŸ”— **æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã¨ã®çµ±åˆ**

### **ãƒ•ã‚§ãƒ¼ã‚º3ï¼ˆNNå±¤ï¼‰ã¨ã®é€£æº**
- LayerNorm, GroupNormã§ã®æœ€é©åŒ–å™¨äº’æ›æ€§
- RNNã‚»ãƒ«ã§ã®å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°çµ±åˆ
- è»¢ç½®ç•³ã¿è¾¼ã¿ã§ã®é‡ã¿åˆæœŸåŒ–æœ€é©åŒ–

### **ãƒ•ã‚§ãƒ¼ã‚º4ï¼ˆå‹¾é…ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰ã¨ã®é€£æº**  
- é«˜æ¬¡å¾®åˆ†å¯¾å¿œã®æœ€é©åŒ–å™¨æ‹¡å¼µ
- å‹¾é…ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½çµ±åˆ
- è‡ªå‹•æ··åˆç²¾åº¦ã¨ã®é€£æº

### **åˆ†æ•£å­¦ç¿’ã¸ã®æº–å‚™**
- è¤‡æ•°GPUå¯¾å¿œã®åŸºç›¤æ•´å‚™
- å‹¾é…åŒæœŸãƒ»é€šä¿¡æœ€é©åŒ–
- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å¯¾å¿œã®ãƒ¡ãƒ¢ãƒªç®¡ç†

---

ã“ã®ãƒ•ã‚§ãƒ¼ã‚º2ã®å®Œäº†ã«ã‚ˆã‚Šã€RusTorchã¯ç¾ä»£çš„ãªæ·±å±¤å­¦ç¿’è¨“ç·´ã«å¿…è¦ãªé«˜åº¦æœ€é©åŒ–æ‰‹æ³•ã‚’å®Œå…¨ã‚µãƒãƒ¼ãƒˆã—ã€PyTorchã¨ã®å®Ÿç”¨çš„äº’æ›æ€§ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚