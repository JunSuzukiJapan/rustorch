# RoPE Implementation Verification

**æ—¥æ™‚**: 2025-10-08
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: RoPEå®Ÿè£…ã¯æ­£ã—ã„ - å•é¡Œã¯åˆ¥ã®ç®‡æ‰€ã«ã‚ã‚‹

## âœ… æ¤œè¨¼çµæœ

### RoPE Lookup TableåˆæœŸåŒ–

**Parameters**:
```
head_dim = 64
max_seq_len = 2048
theta = 10000
```

**Position 0 (i=0,1,2)**:
```
pos=0, i=0: freq=1.0, angle=0.0, cos=1.0, sin=0.0 âœ…
pos=0, i=1: freq=0.7498942, angle=0.0, cos=1.0, sin=0.0 âœ…
pos=0, i=2: freq=0.5623413, angle=0.0, cos=1.0, sin=0.0 âœ…
```

**Position 1 (i=0,1,2)**:
```
pos=1, i=0: freq=1.0, angle=1.0, cos=0.5403023, sin=0.84147096 âœ…
pos=1, i=1: freq=0.7498942, angle=0.749894, cos=0.731761, sin=0.6815613 âœ…
pos=1, i=2: freq=0.5623413, angle=0.5623413, cos=0.84600914, sin=0.53316844 âœ…
```

**Position 2 (i=0,1,2)**:
```
pos=2, i=0: freq=1.0, angle=2.0, cos=-0.41614684, sin=0.90929741 âœ…
pos=2, i=1: freq=0.7498942, angle=1.4997884, cos=0.07094827, sin=0.99747998 âœ…
pos=2, i=2: freq=0.5623413, angle=1.1246826, cos=0.43146282, sin=0.90213072 âœ…
```

**æ•°å­¦çš„æ¤œè¨¼**:
- cos(0) = 1.0 âœ…
- sin(0) = 0.0 âœ…
- cos(1) â‰ˆ 0.5403 âœ…
- sin(1) â‰ˆ 0.8415 âœ…
- cos(2) â‰ˆ -0.4161 âœ…
- sin(2) â‰ˆ 0.9093 âœ…

### RoPE Rotationè¨ˆç®—

**Position 0ã§ã®å›è»¢ (æ’ç­‰å¤‰æ›)**:
```
cos=1.0, sin=0.0

Example 1:
  before: x0=-0.004824, x1=-0.012687
  r0 = x0*cos - x1*sin = -0.004824*1.0 - (-0.012687)*0.0 = -0.004824 âœ…
  r1 = x0*sin + x1*cos = -0.004824*0.0 + (-0.012687)*1.0 = -0.012687 âœ…
  after:  r0=-0.004824, r1=-0.012687 âœ…

Example 2:
  before: x0=0.014200, x1=0.061956
  after:  r0=0.014200, r1=0.061956 âœ…

Example 3:
  before: x0=0.170378, x1=0.064556
  after:  r0=0.170378, r1=0.064556 âœ…
```

**Formulaæ¤œè¨¼**:
```rust
r0 = x0 * cos - x1 * sin  // âœ… æ­£ã—ã„
r1 = x0 * sin + x1 * cos  // âœ… æ­£ã—ã„
```

## ğŸ” å•é¡Œã®çœŸå› 

RoPEå®Ÿè£…ã¯**å®Œå…¨ã«æ­£ã—ã„**ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ä»¥ä¸‹ã®å•é¡ŒãŒç¶™ç¶šï¼š
- Token 15010 ("drew") ãŒç”Ÿæˆã•ã‚Œã‚‹
- llama.cppã¯æ­£ã—ã„å‡ºåŠ›ã‚’ç”Ÿæˆ

**çµè«–**: RoPEã¯å•é¡Œã§ã¯ãªã„ã€‚ä»–ã®ç®‡æ‰€ã«å•é¡ŒãŒã‚ã‚‹ã€‚

## ğŸ¯ æ¬¡ã®èª¿æŸ»å¯¾è±¡

### 1. Embedding Layer
- Token embeddingsãŒæ­£ã—ããƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹
- Llama.cppã¨ã®æ•°å€¤çš„ä¸€è‡´ã‚’ç¢ºèª

### 2. Attention Mechanism
- Q, K, Vã®è¨ˆç®—ãŒæ­£ã—ã„ã‹
- Attention scoresã¨softmaxãŒæ­£ã—ã„ã‹
- Grouped-Query Attentionã®å®Ÿè£…

### 3. Output Projection
- LM headã®é‡ã¿ãŒæ­£ã—ã„ã‹
- Logitsè¨ˆç®—ã®æ­£ç¢ºæ€§

## ğŸ“Š ç¾åœ¨ã®çŠ¶æ³

**æ¤œè¨¼æ¸ˆã¿** âœ…:
- Position calculation (Step 0: pos=0, Step 1: pos=generated_ids.len-1)
- RoPE frequency precomputation
- RoPE rotation formula
- Cos/sin lookup table values

**æœªæ¤œè¨¼** â“:
- Embedding layer weights
- Attention QKV projections
- Softmax implementation
- Output LM head weights
- FFN (Feed-Forward Network)

## ğŸ’¡ ä»®èª¬

æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„åŸå› ï¼š
1. **Embedding weights**: Token embeddingsã®å€¤ãŒllama.cppã¨ç•°ãªã‚‹
2. **Attention scores**: softmaxã¾ãŸã¯attentionè¨ˆç®—ã®æ•°å€¤èª¤å·®
3. **Weight loading**: Q6_K quantizationã‹ã‚‰f32ã¸ã®å¤‰æ›èª¤å·®

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:
Embedding layerã®å‡ºåŠ›ã‚’llama.cppã¨æ¯”è¼ƒã—ã€ã©ã®æ™‚ç‚¹ã§å€¤ãŒ divergeã™ã‚‹ã‹ç‰¹å®šã™ã‚‹
