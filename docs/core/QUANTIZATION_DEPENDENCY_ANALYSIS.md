# 量子化方式依存性分析

**日時**: 2025-10-08
**ステータス**: 量子化方式による出力変動を確認

## 🎯 実験目的

異なる量子化方式（Q4_K_M vs Q4_0）で同じ問題が発生するか検証し、問題が量子化に起因するか確認する。

## 📊 実験結果

### 生成トークンの比較

| 実装 | 量子化 | プロンプト | 生成トークン | トークンID | Logit順位 |
|------|--------|-----------|------------|-----------|----------|
| RusTorch | Q4_K_M | "Hello" | diplom | 13487 | 6位 |
| RusTorch | Q4_0   | "Hello" | ER     | 1001  | 1位 |
| llama.cpp | Q4_0 | "Hello" | ,      | ?     | ? |

### レイヤー出力RMS成長パターンの比較

#### Q4_K_M (RusTorch)
| Layer | Input RMS | Output RMS | Mean値 | RMS成長率 |
|-------|-----------|-----------|--------|----------|
| 0     | 0.009276  | 0.015003  | -0.000174 | 1.0x |
| 5     | 0.101459  | 0.154940  | -0.001946 | 10.3x |
| 10    | 0.240584  | 0.317698  | -0.005547 | 21.2x |
| 15    | 0.372846  | 0.497063  | 0.000272 | 33.1x |
| 21    | 0.745669  | 1.124350  | -0.036644 | 74.9x |

#### Q4_0 (RusTorch)
| Layer | Input RMS | Output RMS | Mean値 | RMS成長率 |
|-------|-----------|-----------|--------|----------|
| 0     | 0.009303  | 0.014637  | 0.000089 | 1.0x |
| 5     | 0.113024  | 0.145677  | -0.000051 | 10.0x |
| 10    | 0.259480  | 0.290877  | 0.007852 | 19.9x |
| 15    | 0.430935  | 0.473644  | 0.008226 | 32.4x |
| 21    | 0.890467  | 1.054700  | 0.023314 | 72.1x |

### SwiGLU出力の比較

#### Q4_K_M
| Layer | SwiGLU RMS | SwiGLU Mean |
|-------|-----------|-------------|
| 0     | 0.001932  | -0.000018 |
| 5     | 0.018409  | -0.000326 |
| 10    | 0.036568  | 0.000115 |
| 15    | 0.057476  | 0.000129 |
| 21    | 0.165125  | -0.001773 |

#### Q4_0
| Layer | SwiGLU RMS | SwiGLU Mean |
|-------|-----------|-------------|
| 0     | 0.001841  | -0.000003 |
| 5     | 0.015926  | -0.000216 |
| 10    | 0.030457  | -0.000059 |
| 15    | 0.048314  | -0.000174 |
| 21    | 0.181741  | **0.015373** ⚠️ |

## 🔍 重要な発見

### 1. 量子化方式により生成トークンが異なる

- **Q4_K_M**: "diplom" (トークン13487, 6位)
- **Q4_0**: "ER" (トークン1001, 1位)

→ **量子化の微小な違いが最終出力に大きく影響**

### 2. RMS成長パターンは類似

両方の量子化方式で：
- Layer 0→21で約70-100倍のRMS成長
- ほぼ線形の成長パターン
- 後半レイヤーでの加速

→ **アーキテクチャ的な問題（RMSNorm + Residual）の影響が支配的**

### 3. Mean値のドリフトパターンが異なる

- **Q4_K_M**: -0.000174 → -0.036644 (211倍, 負方向)
- **Q4_0**: 0.000089 → 0.023314 (262倍, 正方向)

→ **量子化誤差がバイアスの方向に影響**

### 4. Q4_0のLayer 21異常値

**SwiGLU mean = 0.015373** (Q4_K_Mの約8倍)

これは：
1. 量子化誤差の累積
2. Residual接続を通じた正のフィードバックループ
3. 小さなバイアスが22層を通過して増幅された結果

### 5. llama.cppとの比較

llama.cpp (Q4_0) も "," (カンマ) という意味のないトークンを生成

→ **RusTorch特有の問題ではない可能性**

## 🎯 結論

### 確認されたこと

1. **量子化方式の違いが最終出力に影響する**
   - Q4_K_M vs Q4_0で異なるトークンが生成される
   - 微小な量子化誤差が22層で累積・増幅される

2. **RMS成長は量子化に依存しない**
   - 両方式で同様の成長パターン
   - アーキテクチャ的な特性（RMSNorm + Residual）

3. **Mean値ドリフトは量子化に依存する**
   - 量子化誤差がバイアスの方向を決定
   - Q4_K_M: 負方向, Q4_0: 正方向

4. **問題はRusTorch特有ではない**
   - llama.cppでも意味のあるトークンを生成できない
   - TinyLlama 1.1B Q4量子化の限界の可能性

### 推論される問題

#### 主因: 量子化精度不足

- **Q4量子化は22層のTransformerに不十分**
- 微小な量子化誤差が累積的に増幅される
- 最終的なlogit分布が大きく歪む

#### 副因: アーキテクチャ的増幅

- RMSNormによる値の拡大
- Residual接続によるバイアスの累積
- 22層を通じた指数関数的成長

## 📋 次に検証すべきこと

### 高優先度

1. **より高精度な量子化でテスト**
   - Q6_K (6-bit)
   - Q5_K_M (5-bit)
   - 量子化精度と出力品質の関係を確認

2. **別のモデルアーキテクチャでテスト**
   - Llama-2-7B (より大きいモデル)
   - 層数が多いモデルで同じ問題が顕著化するか確認

3. **単一トークンでの完全比較**
   - RusTorchとllama.cppで完全に同じ入力
   - Layer 0の埋め込み出力から比較
   - どの層から値が乖離するか特定

### 中優先度

4. **RoPE実装の検証**
   - 位置エンコーディングの正確性確認
   - llama.cppとの実装比較

5. **Q8_0サポート追加**
   - より高精度での検証を可能にする

### 低優先度

6. **HuggingFace Transformersとの比較**
   - 非量子化モデルでの動作確認

## 💡 暫定的な推奨事項

### 短期的対応

1. **Q6_KまたはQ5_K_Mモデルを使用**
   - より高精度な量子化で動作確認

2. **より大きなモデルでテスト**
   - Llama-2-7Bなど、パラメータ数が多いモデル
   - 量子化の影響が相対的に小さい

### 長期的対応

1. **Q8_0サポート実装**
   - 8-bit量子化は実用的な最小精度

2. **混合精度推論の検討**
   - 重要な層（最初と最後）は高精度
   - 中間層は低精度で高速化

3. **量子化誤差補正の実装**
   - バイアス補正
   - スケール因子の再調整
