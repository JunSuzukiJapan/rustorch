# 量子化精度分析：Q4 vs Q6

**日時**: 2025-10-08
**目的**: 高精度量子化（Q6_K）で問題が改善するか検証

## 📊 実験結果サマリー

### 生成トークンの比較

| 量子化 | トークンID | トークン | Logit順位 | Top Logit |
|--------|-----------|---------|----------|-----------|
| Q4_K_M | 13487 | "diplom" | 6位 | 7.3169 |
| Q4_0   | 1001  | "ER"     | 1位 | 9.6904 |
| Q6_K   | 15010 | "drew"   | 1位 | 11.8283 |

**重要**: 高精度量子化（Q6_K）でも異なる間違ったトークンが生成される

### Layer出力RMS成長パターン

| Layer | Q4_K_M | Q4_0 | Q6_K | Q6_K成長率 |
|-------|--------|------|------|-----------|
| 0     | 0.015003 | 0.014637 | 0.011214 | 1.0x |
| 5     | 0.154940 | 0.145677 | 0.175816 | 15.7x |
| 10    | 0.317698 | 0.290877 | 0.349671 | 31.2x |
| 15    | 0.497063 | 0.473644 | 0.573687 | 51.2x |
| 21    | 1.124350 | 1.054700 | 1.259880 | 112.4x |

### Mean値ドリフト

| Layer | Q4_K_M | Q4_0 | Q6_K |
|-------|--------|------|------|
| 0     | -0.000174 | 0.000089 | 0.000046 |
| 5     | -0.001946 | -0.000051 | 0.002492 |
| 10    | -0.005547 | 0.007852 | -0.004714 |
| 15    | 0.000272 | 0.008226 | -0.010374 |
| 21    | -0.036644 | 0.023314 | -0.009707 |

### Layer 21 SwiGLU出力

| 量子化 | SwiGLU RMS | SwiGLU Mean |
|--------|-----------|-------------|
| Q4_K_M | 0.165125 | -0.001773 |
| Q4_0   | 0.181741 | **0.015373** ⚠️ |
| Q6_K   | 0.187016 | 0.001583 |

## 🔍 詳細分析

### 1. 高精度量子化でも問題は解決しない

**Q6_K (6-bit)** は **Q4系 (4-bit)** より50%多くの情報を保持：
- Q4: 16段階の離散値
- Q6: 64段階の離散値

しかし：
- ✅ RMS成長は若干**悪化**（1.260 vs 1.124/1.055）
- ❌ 異なる間違ったトークン "drew" を生成
- ✅ Mean値ドリフトは**改善**（-0.009707 vs -0.036644/0.023314）

### 2. RMS成長パターンの比較

#### Q4_K_M vs Q6_K
```
Layer 0:  0.015 vs 0.011 (Q6_Kの方が小さい) ✅
Layer 5:  0.155 vs 0.176 (Q6_Kの方が大きい) ❌
Layer 10: 0.318 vs 0.350 (Q6_Kの方が大きい) ❌
Layer 15: 0.497 vs 0.574 (Q6_Kの方が大きい) ❌
Layer 21: 1.124 vs 1.260 (Q6_Kの方が大きい) ❌
```

**驚くべき結果**: 高精度量子化でRMS成長が**悪化**

### 3. Mean値ドリフトの比較

#### Q4_K_M: 負方向ドリフト
```
-0.000174 → -0.036644 (211倍増加)
```

#### Q4_0: 正方向ドリフト
```
0.000089 → 0.023314 (262倍増加)
```

#### Q6_K: 軽度の負方向ドリフト
```
0.000046 → -0.009707 (211倍増加、しかし絶対値は小さい)
```

**結論**: Q6_Kは**Mean値ドリフトを73%削減** (0.009707 vs 0.036644)

### 4. Layer 0の初期値比較

| 量子化 | Layer 0 RMS | Layer 0 Mean |
|--------|------------|--------------|
| Q4_K_M | 0.015003 | -0.000174 |
| Q4_0   | 0.014637 | 0.000089 |
| Q6_K   | **0.011214** | 0.000046 |

**Q6_Kは最小のLayer 0 RMS** → 量子化デコードがより正確

しかし後続レイヤーでより大きく成長 → **なぜ？**

## 🧠 仮説：量子化精度と成長のトレードオフ

### 仮説1: 量子化ノイズの性質

- **Q4系**: 大きな量子化誤差 → ランダムノイズ → 平滑化効果
- **Q6_K**: 小さな量子化誤差 → 構造的バイアス → 増幅される

### 仮説2: 重みの対称性

Q6_Kはより正確な重みをデコード → 重みの微小な非対称性が保持される → RMSNormで増幅

Q4系は大きく丸め → 重みが偶然対称に近くなる → 成長が抑制される（偶然）

### 仮説3: RMSNormの感度

RMSNorm: `output = input / sqrt(mean(input²) + eps) * weight`

高精度 → より正確な `sqrt(mean(input²))` → 小さな値になる可能性 → 除算で増幅

### 仮説4: Residual接続の累積

高精度 → より一貫した誤差の方向 → Residual接続で累積しやすい

低精度 → ランダムな誤差 → Residual接続で部分的にキャンセル

## 🎯 結論

### 確認された事実

1. **量子化精度を上げても問題は解決しない**
   - Q6_Kでも間違ったトークンを生成
   - RMS成長はむしろ悪化

2. **Mean値ドリフトは改善**
   - Q6_K: -0.009707 (Q4_K_Mの26%)
   - バイアスの累積は抑制されている

3. **量子化方式により異なるトークンが生成される**
   - Q4_K_M: "diplom"
   - Q4_0: "ER"
   - Q6_K: "drew"

### 推論

**問題の本質は量子化精度ではない**

1. **アーキテクチャ的な問題**
   - RMSNorm + Residual接続の相互作用
   - 22層を通じた値の指数関数的成長

2. **モデルサイズの問題**
   - TinyLlama 1.1Bは小さすぎる
   - より大きなモデルで安定している可能性

3. **入力の問題**
   - "Hello"のみは不十分なコンテキスト
   - チャットテンプレートが適用されていない

## 📋 次の検証ステップ

### 最優先

1. **チャットテンプレートの適用**
   ```
   <|user|>
   Hello</s>
   <|assistant|>
   ```
   - 適切なコンテキストでテスト

2. **より長い入力でテスト**
   - "Hello, how are you?"
   - RMS成長が安定するか確認

3. **llama.cppとの詳細比較**
   - 同じQ6_Kモデル
   - 同じ入力
   - Layer-by-layer比較

### 中優先度

4. **Llama-2-7B でテスト**
   - より大きなモデルで同じ問題が発生するか

5. **RMSNorm実装の検証**
   - llama.cppのRMSNorm実装と比較
   - Epsilon値の確認

### 低優先度

6. **Q5_K_Mサポート追加**
   - 現在サポートされていない
   - Q4とQ6の中間で検証

## 💡 暫定的な推奨事項

### 短期的対応

1. **チャットテンプレートを使用**
   - 単一単語ではなく適切なコンテキスト

2. **より長い入力でテスト**
   - RMS成長の挙動を観察

3. **Llama-2-7Bに切り替え**
   - より安定した可能性

### 長期的対応

1. **RMSNorm実装の再検証**
   - llama.cppとの数値的な比較

2. **Residual接続の検証**
   - 値の範囲チェック
   - オーバーフロー対策

3. **Post-RMSNormの検討**
   - Residual前にRMSNorm
   - 成長を抑制できる可能性
