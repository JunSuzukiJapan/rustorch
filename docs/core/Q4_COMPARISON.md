# Q4_K_M vs Q4_0 量子化方式の比較

**日時**: 2025-10-08
**目的**: 異なる量子化方式で同じ問題が発生するか検証

## 実験結果

### 生成トークン

| モデル | 生成トークン | トークンID | Logit |
|--------|------------|-----------|-------|
| Q4_K_M | diplom | 13487 | 7.3169 (rank 6) |
| Q4_0   | ER     | 1001  | 9.6904 (rank 1) |

**重要**: 両モデルとも間違ったトークンを生成（正しい英語応答ではない）

### レイヤー出力RMS成長パターン

#### Q4_K_M
| Layer | Input RMS | Output RMS | Mean値 |
|-------|-----------|-----------|--------|
| 0     | 0.009276  | 0.015003  | -0.000174 |
| 5     | 0.101459  | 0.154940  | -0.001946 |
| 10    | 0.240584  | 0.317698  | -0.005547 |
| 15    | 0.372846  | 0.497063  | 0.000272 |
| 21    | 0.745669  | 1.124350  | -0.036644 |

#### Q4_0
| Layer | Input RMS | Output RMS | Mean値 |
|-------|-----------|-----------|--------|
| 0     | 0.009303  | 0.014637  | 0.000089 |
| 5     | 0.113024  | 0.145677  | -0.000051 |
| 10    | 0.259480  | 0.290877  | 0.007852 |
| 15    | 0.430935  | 0.473644  | 0.008226 |
| 21    | 0.890467  | 1.054700  | 0.023314 |

### SwiGLU出力の成長

#### Q4_K_M
| Layer | SwiGLU RMS | Mean値 |
|-------|-----------|--------|
| 0     | 0.001932  | -0.000018 |
| 5     | 0.018409  | -0.000326 |
| 10    | 0.036568  | 0.000115 |
| 15    | 0.057476  | 0.000129 |
| 21    | 0.165125  | -0.001773 |

#### Q4_0
| Layer | SwiGLU RMS | Mean値 |
|-------|-----------|--------|
| 0     | 0.001841  | -0.000003 |
| 5     | 0.015926  | -0.000216 |
| 10    | 0.030457  | -0.000059 |
| 15    | 0.048314  | -0.000174 |
| 21    | 0.181741  | 0.015373 ⚠️ |

## 📊 比較分析

### 類似点

1. **両モデルとも間違ったトークンを生成**
   - どちらも正しい英語応答ではない

2. **RMS値の線形成長パターン**
   - Layer 0 → 21で約70-100倍に成長

3. **Mean値のドリフト**
   - Layer 0: ≈0 → Layer 21: 0.02-0.04範囲に増加

4. **SwiGLU出力の成長**
   - Layer 21で両モデルとも大きな値（Q4_K_M: 0.165, Q4_0: 0.182）

### 相違点

1. **成長速度**
   - Q4_K_M: より急激（Layer 21 RMS = 1.124）
   - Q4_0: やや緩やか（Layer 21 RMS = 1.055）

2. **Layer 0の初期値**
   - ほぼ同じ（0.009276 vs 0.009303）
   - 量子化デコードは正しく動作している

3. **Mean値の符号**
   - Q4_K_M: Layer 15で正に転じる
   - Q4_0: Layer 10から正の値が安定

4. **Layer 21 SwiGLU mean**
   - Q4_K_M: -0.001773（ほぼゼロ）
   - Q4_0: 0.015373（大きな正のバイアス）⚠️

## 🎯 重要な結論

### 1. 量子化方式に依存しない問題

**Q4_K_MとQ4_0の両方で同じ問題が発生** → 量子化の詳細（block size, scale factorなど）の問題ではない

### 2. 量子化自体が問題の可能性

両モデルとも量子化されている → **F32/F16モデルでのテストが必須**

### 3. 成長パターンの一貫性

- RMSの線形成長
- Mean値のドリフト
- SwiGLU出力の増大

これらはアーキテクチャの性質（RMSNorm + Residual）に起因する可能性が高い

### 4. Q4_0のLayer 21異常値

Q4_0の**Layer 21 SwiGLU mean = 0.015373**は異常に大きい。これは：
- 量子化誤差の累積
- またはResidual接続を通じた正のフィードバック

## 🔧 技術的発見

### Q8_0サポート状況

**RusTorchは現在Q8_0量子化をサポートしていない**
- エラーメッセージ: `Tensor type Q8_0 not yet supported for loading`
- F32/F16モデルがHuggingFace Hubに存在しない
- 利用可能な最高精度モデル: Q6_K (6-bit)

### llama.cppとの出力比較

| 実装 | モデル | プロンプト | 生成トークン |
|------|--------|-----------|------------|
| RusTorch | Q4_K_M | "Hello" | 13487 "diplom" (rank 6) |
| RusTorch | Q4_0   | "Hello" | 1001 "ER" (rank 1) |
| llama.cpp | Q4_0 | "Hello" | "," (カンマ) |

**重要**:
- **異なる量子化方式で異なるトークン**が生成される
- llama.cppも英語として意味のあるトークンを生成していない（カンマのみ）
- 両実装とも「正しい」英語応答を生成できていない

## 📋 次のステップ

### 最優先

1. **Q6_Kモデルでテスト**
   - 利用可能な最高精度の量子化
   - Q4系との成長パターン比較

2. **llama.cppの詳細出力比較**
   - Q4_0モデルでBOSトークンのみ入力
   - 同一条件でのlogit分布比較

3. **RoPE実装の検証**
   - 位置エンコーディングの計算が正しいか
   - llama.cppのRoPE実装と比較

### 中優先度

4. **別のモデルでテスト**
   - Llama-2-7B Q4_K_M
   - 問題がTinyLlama特有か確認

5. **Q8_0サポート追加**
   - より高精度での検証を可能にする
   - 量子化誤差の影響を最小化

### 低優先度

6. **HuggingFace Transformersとの比較**
   - PyTorchベースのリファレンス実装で検証
   - GGUFではなくオリジナルモデルを使用
