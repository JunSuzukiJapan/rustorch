# Output Quality Comparison: RusTorch vs llama.cpp

ç”Ÿæˆæ—¥æ™‚: 2025-10-08
ãƒ¢ãƒ‡ãƒ«: TinyLlama-1.1B-Chat-v1.0 Q4_K_M
ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: Metal GPU

## ãƒ†ã‚¹ãƒˆæ¡ä»¶

**å…±é€šè¨­å®š:**
- ãƒ¢ãƒ‡ãƒ«: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (638MB)
- ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: 10
- Temperature: 0 (greedy sampling)
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: "Hello"

## å‡ºåŠ›æ¯”è¼ƒ

### llama.cpp (Reference Implementation)

```
Hello<|assistant|>
Hi there<|user|>
How are you?<|assistant|>
```

**åˆ†æ:**
- âœ… æ„å‘³ã®ã‚ã‚‹å¯¾è©±ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹
- âœ… é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³ (`<|assistant|>`, `<|user|>`)
- âœ… æ–‡è„ˆã«æ²¿ã£ãŸå¿œç­” ("Hi there", "How are you?")
- âœ… ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦: 244.64 tokens/sec (eval)

### RusTorch (Current Implementation)

```
Hello more wo ags ags O
```

**ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³è©³ç´°:**
```
Token 0: 901 -> 'more'
Token 1: 827 -> 'wo'
Token 2: 810 -> 'ags'
Token 3: 810 -> 'ags'
Token 4: 82  -> 'O'
```

**åˆ†æ:**
- âŒ Gibberishï¼ˆæ„å‘³ä¸æ˜ãªå‡ºåŠ›ï¼‰
- âŒ ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ ("ags ags")
- âŒ æ–‡è„ˆã«æ²¿ã£ã¦ã„ãªã„
- âœ… Segfaultãªã—ã€å®‰å®šå‹•ä½œ
- âœ… RoPE + Causal Maskingé©ç”¨æ¸ˆã¿

## å®Ÿè£…çŠ¶æ³

### âœ… å®Œäº†ã—ãŸå®Ÿè£…

1. **RoPE (Rotary Position Embedding)**
   - äº‹å‰è¨ˆç®—: rope_cos, rope_sin (10000.0 theta)
   - Q/KæŠ•å½±å¾Œã«é©ç”¨
   - å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‹•ä½œç¢ºèªæ¸ˆã¿

2. **Causal Masking**
   - Upper triangular mask (j > i â†’ -inf)
   - Softmaxå‰ã«é©ç”¨
   - å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‹•ä½œç¢ºèªæ¸ˆã¿

3. **GQA (Grouped Query Attention)**
   - K/V projection: kv_dim=256 (4 heads Ã— 64 dim)
   - Q projection: d_model=2048 (32 heads Ã— 64 dim)
   - KV head expansion: 4â†’32

4. **Auto d_ff Calculation**
   - TinyLlama: d_ff=5632 (éæ¨™æº–)
   - Weight sizeã‹ã‚‰è‡ªå‹•è¨ˆç®—

### ğŸ” ç–‘ã‚ã—ã„å®Ÿè£…ç®‡æ‰€

#### 1. ä½ç½®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° (æœ€æœ‰åŠ›å€™è£œ)

**ç¾åœ¨:**
```rust
let start_position = 0; // TODO: Track position for multi-token generation
let q_proj = self.apply_rope(&q_proj, seq_len, num_q_heads, head_dim, start_position);
let k_proj = self.apply_rope(&k_proj, seq_len, num_kv_heads, head_dim, start_position);
```

**å•é¡Œç‚¹:**
- å…¨ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã§`start_position=0`å›ºå®š
- ãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆæ™‚ã€ä½ç½®ãŒæ›´æ–°ã•ã‚Œãªã„
- llama.cppã¯KV Cacheã¨é€£å‹•ã—ã¦ä½ç½®ç®¡ç†

**å½±éŸ¿:**
- éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®é–¢ä¿‚ãŒæ­£ã—ããªã„
- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ãŒä¸æ­£ç¢º
- å‡ºåŠ›å“è³ªã«é‡å¤§ãªå½±éŸ¿

#### 2. Simplified GQA Implementation

**ç¾åœ¨:**
```rust
// Simplified GQA: Repeat KV heads to match Q heads
let k_expanded = Self::repeat_kv_heads(&k_proj, seq_len, num_kv_heads, num_q_heads, head_dim);
let v_expanded = Self::repeat_kv_heads(&v_proj, seq_len, num_kv_heads, num_q_heads, head_dim);
```

**å•é¡Œç‚¹:**
- å˜ç´”ãªKVãƒ˜ãƒƒãƒ‰ç¹°ã‚Šè¿”ã—ã®ã¿
- Full 32-head attentionã§ã¯ãªã„
- Head-wiseè¨ˆç®—ãªã—

**å½±éŸ¿:**
- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡¨ç¾åŠ›ãŒä½ä¸‹
- ãƒ¢ãƒ‡ãƒ«æœ¬æ¥ã®èƒ½åŠ›ã‚’ç™ºæ®ã§ããªã„

#### 3. Layer Normalization

**ç¾åœ¨:**
```rust
executor.layer_norm_f32(&x_f32, &ln1_weight_f32, &mut x_ln1, seq_len, d_model,
                        ln1_bias_f32.as_deref(), 1e-5)?;
```

**ç¢ºèªäº‹é …:**
- TinyLlamaã¯RMS Normä½¿ç”¨ã®å¯èƒ½æ€§
- ç¾åœ¨ã®Layer Normã¯é€šå¸¸ã®LayerNorm (mean + variance)
- llama.cppã®RMS Normã¨ç•°ãªã‚‹å¯èƒ½æ€§

#### 4. Softmax Numerical Stability

**ç¾åœ¨:**
```rust
// Find max for numerical stability
let max_val = row.iter().copied().fold(f32::NEG_INFINITY, f32::max);

// Subtract max and compute exp
for val in row.iter_mut() {
    *val = (*val - max_val).exp();
}

// Normalize
let sum: f32 = row.iter().sum();
for val in row.iter_mut() {
    *val /= sum;
}
```

**ç¢ºèªäº‹é …:**
- å®Ÿè£…è‡ªä½“ã¯å®‰å®š
- Causal maskå¾Œã®-infã®å‡¦ç†ã¯æ­£ã—ã„

## å‡ºåŠ›å“è³ªã‚®ãƒ£ãƒƒãƒ—ã®æ ¹æœ¬åŸå› å€™è£œ

### ğŸ”´ æœ€å„ªå…ˆ: ä½ç½®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

**ä»®èª¬:**
ãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆæ™‚ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®æƒ…å ±ãŒæ­£ã—ãæ›´æ–°ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€RoPEãŒèª¤ã£ãŸä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ã„ã‚‹ã€‚

**æ¤œè¨¼æ–¹æ³•:**
1. å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼ˆseq_len=1ï¼‰ã§å“è³ªç¢ºèª
2. ä½ç½®ã‚’æ‰‹å‹•è¿½è·¡ã—ã¦å†ãƒ†ã‚¹ãƒˆ
3. llama.cppã®KV Cacheå®Ÿè£…ã‚’å‚ç…§

### ğŸŸ¡ é‡è¦: RMS Norm vs Layer Norm

**ä»®èª¬:**
TinyLlamaã¯RMS Normã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€RusTorchã¯é€šå¸¸ã®Layer Normã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€æ´»æ€§åŒ–å€¤ã®åˆ†å¸ƒãŒç•°ãªã‚‹ã€‚

**æ¤œè¨¼æ–¹æ³•:**
1. GGUFãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§normalization typeã‚’ç¢ºèª
2. RMS Normå®Ÿè£…ã«åˆ‡ã‚Šæ›¿ãˆ
3. llama.cppã®normalizationå®Ÿè£…ã‚’å‚ç…§

### ğŸŸ¢ è¦æ¤œè¨: Full Multi-Head Attention

**ä»®èª¬:**
Simplified GQAï¼ˆKVãƒ˜ãƒƒãƒ‰ç¹°ã‚Šè¿”ã—ã®ã¿ï¼‰ã§ã¯ã€æœ¬æ¥ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã¨ç•°ãªã‚‹ã€‚

**æ¤œè¨¼æ–¹æ³•:**
1. Full 32-head attention loopã‚’å®Ÿè£…
2. Head-wiseè¨ˆç®—ã‚’æ­£ç¢ºã«å®Ÿè¡Œ
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã® tradeoffæ¤œè¨

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### Priority 1: ä½ç½®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ä¿®æ­£

```rust
// Generateé–¢æ•°å†…ã§ä½ç½®ã‚’è¿½è·¡
let mut position = 0;
for _ in 0..max_tokens {
    let output = self.forward_metal(&input, position, debug)?;
    // ...
    position += 1; // ä½ç½®ã‚’æ›´æ–°
}
```

### Priority 2: RMS Normå®Ÿè£…

```rust
fn rms_norm_f32(x: &[f32], weight: &[f32], output: &mut [f32], eps: f32) {
    let n = x.len();
    // Compute RMS (Root Mean Square)
    let rms: f32 = x.iter().map(|&v| v * v).sum::<f32>() / (n as f32);
    let rms = (rms + eps).sqrt();

    // Normalize and scale
    for i in 0..n {
        output[i] = (x[i] / rms) * weight[i];
    }
}
```

### Priority 3: KV Cacheå®Ÿè£…

- éå»ã®K/VæŠ•å½±ã‚’ä¿å­˜
- ä½ç½®ã¨é€£å‹•ã—ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨é€Ÿåº¦å‘ä¸Š

### Priority 4: Full Multi-Head Attention

- 32-head loopã§å„ãƒ˜ãƒƒãƒ‰ã‚’å€‹åˆ¥å‡¦ç†
- Head-wise reshape/attention/concat
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

**llama.cpp:**
- Prompt eval: 662.40 tokens/sec
- Generation: 244.64 tokens/sec
- Load time: 77.46 ms

**RusTorch:**
- TBD (é€Ÿåº¦æ¸¬å®šæœªå®Ÿæ–½)
- Metal forward pass: å‹•ä½œç¢ºèªæ¸ˆã¿

## çµè«–

**ç¾çŠ¶:**
- RoPE + Causal Maskingã‚’å®Ÿè£…ã—ãŸãŒã€å‡ºåŠ›å“è³ªã¯ä¾ç„¶ã¨ã—ã¦gibberish
- æ§‹é€ çš„ã«ã¯æ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ãŒã€ç´°éƒ¨ã®å®Ÿè£…å·®ç•°ãŒå¤§ããªå“è³ªã‚®ãƒ£ãƒƒãƒ—ã‚’ç”Ÿã‚“ã§ã„ã‚‹

**æ ¹æœ¬åŸå› ï¼ˆæ¨å®šï¼‰:**
1. **ä½ç½®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ä¸è¶³** (æœ€æœ‰åŠ›) - ãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆæ™‚ã®ä½ç½®æ›´æ–°ãªã—
2. **RMS Normæœªå®Ÿè£…** - Layer Normã¨RMS Normã®é•ã„
3. **Simplified GQA** - Full multi-head attentionã§ã¯ãªã„

**æ¬¡ã®å„ªå…ˆã‚¿ã‚¹ã‚¯:**
1. ä½ç½®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°å®Ÿè£… (å³åŠ¹æ€§å¤§)
2. RMS Normå®Ÿè£… (TinyLlamaä»•æ§˜ç¢ºèª)
3. KV Cacheå®Ÿè£… (é€Ÿåº¦+å“è³ªå‘ä¸Š)
4. Full multi-head attention (è¡¨ç¾åŠ›å‘ä¸Š)

---

**Status:** ğŸ” Output quality gap identified - Position tracking most likely cause
**Next Milestone:** Position tracking + RMS Norm implementation
