# Layer値成長問題の詳細分析

**日時**: 2025-10-07
**ステータス**: 根本原因特定完了、修正方法検討中

## 📊 問題の概要

TinyLlama-1.1B Q4_K_M推論で、22層を通過する間に隠れ状態の値が指数関数的に成長し、最終的に間違ったトークン (13487 "diplom") を生成する。

## 🔍 観測された値の成長 (最新データ: 2025-10-08)

### レイヤー出力RMS - 完全な成長パターン
| Layer | Output RMS | Max値 | 成長率 | Mean値 | レイヤーあたり増加 |
|-------|-----------|-------|--------|--------|------------------|
| 0     | 0.015003  | 0.095 | 1.0x (base) | -0.000174 | - |
| 5     | 0.154940  | 0.630 | 10.3x | -0.001946 | 0.028/層 |
| 10    | 0.317698  | 1.241 | 21.2x | -0.005547 | 0.033/層 |
| 15    | 0.497063  | 1.864 | 33.1x | 0.000272 | 0.036/層 |
| 21    | 1.124350  | 4.953 | 74.9x | -0.036644 | 0.105/層 ⚠️ |
| Final | 1.920277  | 7.224 | 128.0x | - | - |

### 重大な発見

1. **ほぼ線形の成長**: RMSは層数にほぼ比例して増加
2. **後半で加速**: Layer 15-21での成長率が急激に上昇 (0.036 → 0.105)
3. **Mean値のドリフト**: -0.000174 → -0.036644 (200倍以上の増加)
4. **符号の反転**: Layer 15でmeanが正に転じる (異常な挙動)

## 🔬 詳細調査結果

### 1. Layer 0の詳細統計

#### 入力 (トークン埋め込み)
```
RMS: 0.009276
Max: 0.087731
```

#### Attention RMSNorm前
```
attn_norm.weight: rms=0.046377, max=0.769531
```

#### Attention RMSNorm後
```
RMS: 0.107767
Max: 4.804578  ← 異常に大きい！
```

#### Attention処理
- GQA出力 (o_proj前): rms=0.032740, max=0.101824
- o_weight: rms=0.008275
- 最終Attention出力: rms=0.011593, max=0.052531

#### FFN処理
- Gate投影: rms=0.061013, max=0.305088
- Up投影: rms=0.062662, max=0.295239
- SwiGLU出力: rms=0.001932, max=0.020044
- FFN最終出力: rms=0.002681

#### Residual接続後
```
Layer 0出力: rms=0.015113
```

### 2. Layer 21の比較

#### FFN中間値の成長
| 段階 | Layer 0 | Layer 21 | 成長率 |
|------|---------|----------|--------|
| Gate RMS | 0.061 | 0.614 | 10倍 |
| Gate Max | 0.305 | 3.026 | 10倍 |
| SwiGLU RMS | 0.001932 | 0.165125 | **85倍** |
| SwiGLU Max | 0.020044 | 1.638480 | **82倍** |

## 🎯 根本原因の特定

### 主な発見

1. **RMSNorm実装は正しい**
   - 数式: `output = (input / rms) * weight`
   - epsilon = 1e-5 (標準値)

2. **RMSNorm後の値が大きすぎる**
   ```python
   Layer 0 RMSNorm計算例:
   input_value = 0.087731
   rms = 0.009276
   weight (一部次元) = 0.77
   output ≈ (0.087731 / 0.009276) * X = 9.46 * X

   もしX=0.5なら → 4.73 (観測値4.8に近い)
   ```

3. **Residual接続による累積**
   - 各層で `x_out = x_in + attention(x_in) + ffn(x_in)`
   - FFN出力は入力に比例して増大（Layer 0: 0.002, Layer 21: 0.22）
   - レイヤーあたり1.12〜1.34倍の成長

4. **初期層での急成長**
   - Layer 0→10: 1.34倍/層
   - Layer 10→21: 1.12倍/層
   - **初期層で加速度的に成長**

## ❓ 疑問点

### llama.cppとの比較必要項目

1. **Layer 0 Attention RMSNorm後の値**
   - llama.cppでも±5程度の値になるか？
   - それとも±2以下か？

2. **RMSNorm weight値**
   - attn_norm.weight の範囲が [0.05, 0.77] は正常か？
   - llama.cppも同じ値を使用しているか？

3. **Residual接続の成長率**
   - llama.cppでも1.3倍/層の成長が見られるか？
   - それとも安定しているか？

## 🔧 検証が必要な仮説

### 仮説A: RMSNormの解釈が間違っている
- 可能性: 低
- 理由: 実装は標準的で、weightのロードも正しい

### 仮説B: 埋め込みの初期値が大きすぎる
- 可能性: 中
- 理由: Layer 0入力のmax値 0.087は妥当に見えるが、一部次元で大きい可能性

### 仮説C: Attention実装に微妙な問題がある
- 可能性: 中
- 理由: GQA出力が入力の3倍 (0.032 vs 0.009) は大きい

### 仮説D: これは正常で、llama.cppも同じ動作をしている
- 可能性: 高
- 理由: すべての実装が数学的に正しく、問題は別の箇所（量子化など）かもしれない

## 📝 次のステップ

1. **llama.cppとの直接比較**
   - Layer 0の各段階での値を直接比較
   - 特にRMSNorm後の最大値を確認

2. **Q4_K量子化の再検証**
   - スケール係数の計算が正しいか再確認
   - llama.cppの実装と完全に一致するか検証

3. **代替アプローチ**
   - HuggingFace Transformersの実装と比較
   - F32精度のGGUFモデルで検証（量子化の影響を除外）

## 💡 暫定的結論

現時点での最も可能性の高い説明:

**RMSNorm実装とResidual接続は正しく動作しているが、入力値の一部が大きく、それがResidual接続を通じて累積的に増幅されている。この動作が正常かどうかは、llama.cppとの直接比較が必要。**

問題がQ4_K量子化のデコード精度にある可能性も高い。微小なスケール係数の違いが22層を通過する間に累積的に増幅される可能性がある。
