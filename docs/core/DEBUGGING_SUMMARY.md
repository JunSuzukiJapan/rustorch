# RusTorch Debugging Summary

**æ—¥æ™‚**: 2025-10-08
**å•é¡Œ**: Tokenç”ŸæˆãŒé–“é•ã£ã¦ã„ã‚‹ ("drew drew drew" instead of "Hello everyone")

## âœ… æ¤œè¨¼æ¸ˆã¿ - æ­£ã—ã„å®Ÿè£…

### 1. Position Calculation
- Step 0: `start_position = 0`, positions 0-19 âœ…
- Step 1+: `start_position = generated_ids.len() - 1` âœ…
- å…¨22å±¤ã§åŒã˜positionãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ âœ…

### 2. RoPE Implementation
**Lookup Table**:
- theta = 10000, head_dim = 64 âœ…
- cos(0) = 1.0, sin(0) = 0.0 âœ…
- cos(1) â‰ˆ 0.5403, sin(1) â‰ˆ 0.8415 âœ…
- cos(2) â‰ˆ -0.4161, sin(2) â‰ˆ 0.9093 âœ…

**Rotation Formula**:
```rust
r0 = x0 * cos - x1 * sin  âœ…
r1 = x0 * sin + x1 * cos  âœ…
```

**Numerical Verification**:
- Position 0: r0 = x0, r1 = x1 (identity) âœ…
- All rotation values mathematically correct âœ…

### 3. Grouped-Query Attention Structure
```rust
num_heads = 32
num_kv_heads = 4
num_groups = 32 / 4 = 8  âœ…
kv_head = h / num_groups  âœ…
```

## âŒ å•é¡Œç¶™ç¶š

### Output Comparison

**Input**: "Hello world\n"

**llama.cpp (Q6_K)**:
```
Output: "Hello everyone,"
Status: âœ… CORRECT
```

**RusTorch (Q6_K, hybrid-f32)**:
```
Output: "drew drew drew SuperhÃ©"
Token IDs: [15010, 15010, 15010, 5670, 19880]
Status: âŒ WRONG
```

### Logits Comparison

**RusTorch Step 0 Logits**:
```
#1: token=15010 ("drew")  logit=11.8314  â† é¸æŠã•ã‚Œã‚‹
#2: token=5670  ("Super") logit=9.4277
#3: token=19880 ("hÃ©")    logit=9.0241
...
Expected tokens:
  token 15043 logit=0.0468   â† ã“ã‚ŒãŒé¸æŠã•ã‚Œã‚‹ã¹ã
  token 6324 logit=-2.2811
```

Token 15010 ("drew")ãŒåœ§å€’çš„ã«é«˜ã„logitå€¤ã‚’æŒã£ã¦ã„ã‚‹ã€‚

## ğŸ” æœªæ¤œè¨¼é …ç›®

### 1. Weight Loading (Q6_K â†’ f32)
- Q6_K quantizationå½¢å¼ã‹ã‚‰f32ã¸ã®å¤‰æ›ãŒæ­£ã—ã„ã‹ï¼Ÿ
- llama.cppã¨åŒã˜dequantizationå®Ÿè£…ã‹ï¼Ÿ

### 2. Attention Mechanism Details
- Attention scoresã®è¨ˆç®—
- Causal maskingã®é©ç”¨
- Softmax numerically stable implementation
- Value aggregation

### 3. FFN (Feed-Forward Network)
- SwiGLU activation
- Gate/Up projections
- Down projection

### 4. Layer Normalization
- RMSNorm implementation
- Epsilon value (1e-5?)
- Numerical stability

### 5. Output LM Head
- Weight matrix shape and indexing
- Final logits calculation

## ğŸ’¡ ä»®èª¬

### æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„åŸå› 

**1. Q6_K Dequantization Bug** (ç¢ºç‡: 70%)
- RusTorchã®Q6_K â†’ f32å¤‰æ›ãŒllama.cppã¨ç•°ãªã‚‹
- å¾®å°ãªèª¤å·®ãŒ22å±¤ã§ç´¯ç©
- æœ€çµ‚logitsãŒå¤§ããä¹–é›¢

**æ¤œè¨¼æ–¹æ³•**:
- F32ãƒ¢ãƒ‡ãƒ«ï¼ˆæœªé‡å­åŒ–ï¼‰ã§ãƒ†ã‚¹ãƒˆ
- ã¾ãŸã¯ã€Q6_K dequantizationå®Ÿè£…ã‚’llama.cppã¨å®Œå…¨ã«ä¸€è‡´ã•ã›ã‚‹

**2. Attention Score Calculation** (ç¢ºç‡: 20%)
- Softmaxå‰ã®scaling factor
- Numerical stability issue
- KV cacheã¨ã®é€£çµå‡¦ç†

**3. Accumulated Numerical Errors** (ç¢ºç‡: 10%)
- f32ç²¾åº¦ã§ã®ç´¯ç©èª¤å·®
- 22å±¤ã‚’é€šéã™ã‚‹éç¨‹ã§å¢—å¹…
- Metal GPUã§ã®è¨ˆç®—é †åºã®é•ã„

## ğŸ¯ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 

### Priority 1: Quantization Verification
1. Q4_0ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å­åŒ–ï¼‰
2. Q6_K dequantizationå®Ÿè£…ã‚’llama.cpp/ggml-quants.cã¨æ¯”è¼ƒ
3. Dequantized weightså€¤ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒ

### Priority 2: Layer-by-Layer Comparison
1. Layer 0 output hidden stateã‚’llama.cppã¨æ¯”è¼ƒ
2. Divergence pointã‚’ç‰¹å®š
3. è©²å½“ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…ã‚’è©³ç´°æ¤œè¨¼

### Priority 3: Simple Test Case
1. å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›ã§ãƒ†ã‚¹ãƒˆ
2. Embedding â†’ Layer 0 â†’ Outputã®å„æ®µéšã‚’æ¤œè¨¼
3. æœ€å°é™ã®ã‚±ãƒ¼ã‚¹ã§å•é¡Œã‚’å†ç¾

## ğŸ“Š Debug Output Examples

### Embedding (First Token, Token ID=1)
```
First 10 values: [-0.0010786057, 0.0057525635, -0.00089883804, ...]
```

### Layer 0 Stats
```
Input: rms=0.010231, min=-0.060242, max=0.085144
After Attention: rms=0.003226, min=-0.030446, max=0.029820
After FFN: rms=0.002464, min=-0.009516, max=0.011336
Output: rms=0.010999, min=-0.060879, max=0.086502
```

### Final Logits (Step 0)
```
max=11.8314, min=-9.8738, mean=-0.0091
Top: 15010(11.83), 5670(9.43), 19880(9.02)
```

## ğŸ“ Notes

- RoPEå®Ÿè£…ã¯å®Œç’§ã«æ­£ã—ã„ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸ
- Position calculationã‚‚æ­£ã—ã„
- å•é¡Œã¯RoPEä»¥å¤–ã®ç®‡æ‰€ã«ã‚ã‚‹
- Quantization bugã¾ãŸã¯attentionå®Ÿè£…ã®å¯èƒ½æ€§ãŒæœ€ã‚‚é«˜ã„
