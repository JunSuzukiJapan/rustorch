# RusTorch üöÄ

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-968%20passing-brightgreen.svg)](#testing)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](#testing)

**Une biblioth√®que d'apprentissage profond pr√™te pour la production en Rust avec une API similaire √† PyTorch, l'acc√©l√©ration GPU et des performances de niveau entreprise**

RusTorch est une biblioth√®que d'apprentissage profond enti√®rement fonctionnelle qui exploite la s√©curit√© et les performances de Rust, fournissant des op√©rations tensorielles compl√®tes, la diff√©renciation automatique, des couches de r√©seau de neurones, des architectures de transformateur, l'acc√©l√©ration GPU multi-backend (CUDA/Metal/OpenCL), des optimisations SIMD avanc√©es, la gestion de m√©moire de niveau entreprise, la validation de donn√©es et l'assurance qualit√©, ainsi que des syst√®mes de d√©bogage et de journalisation complets.

## ‚ú® Fonctionnalit√©s

- üî• **Op√©rations Tensorielles Compl√®tes** : Op√©rations math√©matiques, diffusion, indexation et statistiques
- ü§ñ **Architecture Transformer** : Impl√©mentation compl√®te de transformer avec attention multi-t√™tes
- üßÆ **D√©composition Matricielle** : SVD, QR, d√©composition en valeurs propres avec compatibilit√© PyTorch
- üß† **Diff√©renciation Automatique** : Graphe computationnel bas√© sur bande pour le calcul de gradient
- üöÄ **Moteur d'Ex√©cution Dynamique** : Compilation JIT et optimisation √† l'ex√©cution
- üèóÔ∏è **Couches de R√©seaux de Neurones** : Linear, Conv1d/2d/3d, ConvTranspose, RNN/LSTM/GRU, BatchNorm, Dropout, et plus
- ‚ö° **Optimisations Multi-Plateformes** : SIMD (AVX2/SSE/NEON), sp√©cifiques √† la plateforme et optimisations conscientes du mat√©riel
- üéÆ **Int√©gration GPU** : Support CUDA/Metal/OpenCL avec s√©lection automatique de p√©riph√©rique
- üåê **Support WebAssembly** : ML complet dans le navigateur avec couches de r√©seau de neurones, vision par ordinateur et inf√©rence en temps r√©el
- üéÆ **Int√©gration WebGPU** : Acc√©l√©ration GPU optimis√©e Chrome avec repli CPU pour la compatibilit√© multi-navigateurs
- üìÅ **Support de Formats de Mod√®les** : Safetensors, inf√©rence ONNX, compatibilit√© state dict PyTorch
- ‚úÖ **Pr√™t pour la Production** : 968 tests r√©ussis, syst√®me de gestion d'erreurs unifi√©
- üìê **Fonctions Math√©matiques Am√©lior√©es** : Ensemble complet de fonctions math√©matiques (exp, ln, sin, cos, tan, sqrt, abs, pow)
- üîß **Surcharges d'Op√©rateurs Avanc√©es** : Support complet d'op√©rateurs pour tenseurs avec op√©rations scalaires et affectations en place
- üìà **Optimiseurs Avanc√©s** : SGD, Adam, AdamW, RMSprop, AdaGrad avec planificateurs de taux d'apprentissage
- üîç **Validation de Donn√©es et Assurance Qualit√©** : Analyse statistique, d√©tection d'anomalies, v√©rification de coh√©rence, surveillance en temps r√©el
- üêõ **D√©bogage et Journalisation Complets** : Journalisation structur√©e, profilage des performances, suivi de m√©moire, alertes automatis√©es

## üöÄ D√©marrage Rapide

**üìì Pour le guide complet de configuration Jupyter, voir [README_JUPYTER.md](../../README_JUPYTER.md)**

### D√©mo Python Jupyter Lab

#### D√©mo CPU Standard
Lancez RusTorch avec Jupyter Lab en une commande :

```bash
./start_jupyter.sh
```

#### D√©mo Acc√©l√©r√©e WebGPU
Lancez RusTorch avec support WebGPU pour l'acc√©l√©ration GPU bas√©e sur navigateur :

```bash
./start_jupyter_webgpu.sh
```

Les deux scripts vont :
- üì¶ Cr√©er l'environnement virtuel automatiquement
- üîß Construire les bindings Python RusTorch
- üöÄ Lancer Jupyter Lab avec le notebook de d√©monstration
- üìç Ouvrir le notebook de d√©monstration pr√™t √† ex√©cuter

**Fonctionnalit√©s WebGPU :**
- üåê Acc√©l√©ration GPU bas√©e sur navigateur
- ‚ö° Op√©rations matricielles haute performance dans le navigateur
- üîÑ Repli automatique vers CPU quand GPU indisponible
- üéØ Optimis√© Chrome/Edge (navigateurs recommand√©s)

### Installation

Ajoutez ceci √† votre `Cargo.toml` :

```toml
[dependencies]
rustorch = "0.5.10"

# Fonctionnalit√©s optionnelles
[features]
default = ["linalg"]
linalg = ["rustorch/linalg"]           # Op√©rations d'alg√®bre lin√©aire (SVD, QR, valeurs propres)
cuda = ["rustorch/cuda"]
metal = ["rustorch/metal"] 
opencl = ["rustorch/opencl"]
safetensors = ["rustorch/safetensors"]
onnx = ["rustorch/onnx"]
wasm = ["rustorch/wasm"]                # Support WebAssembly pour ML navigateur
webgpu = ["rustorch/webgpu"]            # Acc√©l√©ration WebGPU optimis√©e Chrome

# Pour d√©sactiver les fonctionnalit√©s linalg (√©viter les d√©pendances OpenBLAS/LAPACK) :
rustorch = { version = "0.5.10", default-features = false }
```

### Utilisation de Base

```rust
use rustorch::tensor::Tensor;
use rustorch::optim::{SGD, WarmupScheduler, OneCycleLR, AnnealStrategy};

fn main() {
    // Cr√©er des tenseurs
    let a = Tensor::from_vec(vec![1.0f32, 2.0, 3.0, 4.0], vec![2, 2]);
    let b = Tensor::from_vec(vec![5.0f32, 6.0, 7.0, 8.0], vec![2, 2]);
    
    // Op√©rations de base avec surcharges d'op√©rateurs
    let c = &a + &b;  // Addition √©l√©ment par √©l√©ment
    let d = &a - &b;  // Soustraction √©l√©ment par √©l√©ment
    let e = &a * &b;  // Multiplication √©l√©ment par √©l√©ment
    let f = &a / &b;  // Division √©l√©ment par √©l√©ment
    
    // Op√©rations scalaires
    let g = &a + 10.0;  // Ajouter scalaire √† tous les √©l√©ments
    let h = &a * 2.0;   // Multiplier par scalaire
    
    // Fonctions math√©matiques
    let exp_result = a.exp();   // Fonction exponentielle
    let ln_result = a.ln();     // Logarithme naturel
    let sin_result = a.sin();   // Fonction sinus
    let sqrt_result = a.sqrt(); // Racine carr√©e
    
    // Op√©rations matricielles
    let matmul_result = a.matmul(&b);  // Multiplication matricielle
    
    // Op√©rations d'alg√®bre lin√©aire (n√©cessite la fonctionnalit√© linalg)
    #[cfg(feature = "linalg")]
    {
        let svd_result = a.svd();       // D√©composition SVD
        let qr_result = a.qr();         // D√©composition QR
        let eig_result = a.eigh();      // D√©composition en valeurs propres
    }
    
    // Optimiseurs avanc√©s avec planification de taux d'apprentissage
    let optimizer = SGD::new(0.01);
    let mut scheduler = WarmupScheduler::new(optimizer, 0.1, 5); // √âchauffement √† 0.1 sur 5 √©poques
    
    println!("Forme : {:?}", c.shape());
    println!("R√©sultat : {:?}", c.as_slice());
}
```

### Utilisation WebAssembly

Pour les applications ML bas√©es sur navigateur :

```javascript
import init, * as rustorch from './pkg/rustorch.js';

async function browserML() {
    await init();
    
    // Couches de r√©seau de neurones
    const linear = new rustorch.WasmLinear(784, 10, true);
    const conv = new rustorch.WasmConv2d(3, 32, 3, 1, 1, true);
    
    // Fonctions math√©matiques am√©lior√©es
    const gamma_result = rustorch.WasmSpecial.gamma_batch([1.5, 2.0, 2.5]);
    const bessel_result = rustorch.WasmSpecial.bessel_i_batch(0, [0.5, 1.0, 1.5]);
    
    // Distributions statistiques
    const normal_dist = new rustorch.WasmDistributions();
    const samples = normal_dist.normal_sample_batch(100, 0.0, 1.0);
    
    // Optimiseurs pour l'entra√Ænement
    const sgd = new rustorch.WasmOptimizer();
    sgd.sgd_init(0.01, 0.9); // taux_apprentissage, momentum
    
    // Traitement d'image
    const resized = rustorch.WasmVision.resize(image, 256, 256, 224, 224, 3);
    const normalized = rustorch.WasmVision.normalize(resized, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 3);
    
    // Passage avant
    const predictions = conv.forward(normalized, 1, 224, 224);
    console.log('Pr√©dictions ML navigateur :', predictions);
}
```

## üìö Documentation

- **[Guide de D√©marrage](../getting-started.md)** - Utilisation de base et exemples
- **[Fonctionnalit√©s](../features.md)** - Liste compl√®te des fonctionnalit√©s et sp√©cifications
- **[Performance](../performance.md)** - Benchmarks et d√©tails d'optimisation
- **[Guide Jupyter WASM](jupyter-guide.md)** - Configuration pas √† pas de Jupyter Notebook

### WebAssembly et ML Navigateur
- **[Guide WebAssembly](../WASM_GUIDE.md)** - Int√©gration WASM compl√®te et r√©f√©rence API
- **[Int√©gration WebGPU](../WEBGPU_INTEGRATION.md)** - Acc√©l√©ration GPU optimis√©e Chrome

### Production et Op√©rations
- **[Guide d'Acc√©l√©ration GPU](../GPU_ACCELERATION_GUIDE.md)** - Configuration et utilisation GPU
- **[Guide de Production](../PRODUCTION_GUIDE.md)** - D√©ploiement et mise √† l'√©chelle

## üìä Performance

**R√©sultats de benchmarks r√©cents :**

| Op√©ration | Performance | D√©tails |
|-----------|-------------|---------|
| **D√©composition SVD** | ~1ms (matrice 8x8) | ‚úÖ Bas√© sur LAPACK |
| **D√©composition QR** | ~24Œºs (matrice 8x8) | ‚úÖ D√©composition rapide |
| **Valeurs Propres** | ~165Œºs (matrice 8x8) | ‚úÖ Matrices sym√©triques |
| **FFT Complexe** | 10-312Œºs (8-64 √©chantillons) | ‚úÖ Optimis√© Cooley-Tukey |
| **R√©seau de Neurones** | 1-7s entra√Ænement | ‚úÖ D√©mo Boston housing |
| **Fonctions d'Activation** | <1Œºs | ‚úÖ ReLU, Sigmoid, Tanh |

## üß™ Tests

**968 tests r√©ussis** - Assurance qualit√© pr√™te pour la production avec syst√®me de gestion d'erreurs unifi√©.

```bash
# Ex√©cuter tous les tests
cargo test --no-default-features

# Ex√©cuter tests avec fonctionnalit√©s d'alg√®bre lin√©aire
cargo test --features linalg
```

## ü§ù Contribution

Nous accueillons les contributions ! Voyez les domaines o√π l'aide est particuli√®rement n√©cessaire :

- **üéØ Pr√©cision des Fonctions Sp√©ciales** : Am√©liorer la pr√©cision num√©rique
- **‚ö° Optimisation des Performances** : Am√©liorations SIMD, optimisation GPU
- **üß™ Tests** : Cas de test plus complets
- **üìö Documentation** : Exemples, tutoriels, am√©liorations
- **üåê Support de Plateformes** : WebAssembly, plateformes mobiles

## Licence

Sous licence soit :

 * Licence Apache, Version 2.0 ([LICENSE-APACHE](../../LICENSE-APACHE) ou http://www.apache.org/licenses/LICENSE-2.0)
 * Licence MIT ([LICENSE-MIT](../../LICENSE-MIT) ou http://opensource.org/licenses/MIT)

√† votre choix.