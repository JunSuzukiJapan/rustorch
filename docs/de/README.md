# RusTorch ğŸš€

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-968%20passing-brightgreen.svg)](#testing)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](#testing)

**Eine produktionsbereite Deep Learning Bibliothek in Rust mit PyTorch-Ã¤hnlicher API, GPU-Beschleunigung und Unternehmens-Performance**

RusTorch ist eine vollstÃ¤ndig funktionale Deep Learning Bibliothek, die Rusts Sicherheit und Performance nutzt und umfassende Tensoroperationen, automatische Differentiation, neuronale Netzwerkschichten, Transformer-Architekturen, Multi-Backend GPU-Beschleunigung (CUDA/Metal/OpenCL), fortgeschrittene SIMD-Optimierungen, Unternehmens-Memory-Management, Datenvalidierung & QualitÃ¤tssicherung sowie umfassende Debug- und Logging-Systeme bietet.

## âœ¨ Funktionen

- ğŸ”¥ **Umfassende Tensoroperationen**: Mathematische Operationen, Broadcasting, Indizierung und Statistiken
- ğŸ¤– **Transformer-Architektur**: VollstÃ¤ndige Transformer-Implementierung mit Multi-Head-Attention
- ğŸ§® **Matrixzerlegung**: SVD, QR, Eigenwertzerlegung mit PyTorch-KompatibilitÃ¤t
- ğŸ§  **Automatische Differentiation**: Bandbasierter Berechnungsgraph fÃ¼r Gradientenberechnung
- ğŸš€ **Dynamische AusfÃ¼hrungs-Engine**: JIT-Kompilierung und Laufzeitoptimierung
- ğŸ—ï¸ **Neuronale Netzwerkschichten**: Linear, Conv1d/2d/3d, ConvTranspose, RNN/LSTM/GRU, BatchNorm, Dropout und mehr
- âš¡ **PlattformÃ¼bergreifende Optimierungen**: SIMD (AVX2/SSE/NEON), plattformspezifische und hardwarebewusste Optimierungen
- ğŸ® **GPU-Integration**: CUDA/Metal/OpenCL-UnterstÃ¼tzung mit automatischer GerÃ¤teauswahl
- ğŸŒ **WebAssembly-UnterstÃ¼tzung**: VollstÃ¤ndiges Browser-ML mit neuronalen Netzwerkschichten, Computer Vision und Echtzeit-Inferenz
- ğŸ® **WebGPU-Integration**: Chrome-optimierte GPU-Beschleunigung mit CPU-Fallback fÃ¼r browserÃ¼bergreifende KompatibilitÃ¤t
- ğŸ“ **Modellformat-UnterstÃ¼tzung**: Safetensors, ONNX-Inferenz, PyTorch-State-Dict-KompatibilitÃ¤t
- âœ… **Produktionsbereit**: 968 Tests bestanden, vereinheitlichtes Fehlerbehandlungssystem
- ğŸ“ **Erweiterte mathematische Funktionen**: VollstÃ¤ndiger Satz mathematischer Funktionen (exp, ln, sin, cos, tan, sqrt, abs, pow)
- ğŸ”§ **Fortgeschrittene OperatorÃ¼berladungen**: VollstÃ¤ndige OperatorunterstÃ¼tzung fÃ¼r Tensoren mit skalaren Operationen und In-Place-Zuweisungen
- ğŸ“ˆ **Fortgeschrittene Optimierer**: SGD, Adam, AdamW, RMSprop, AdaGrad mit Lernraten-Schedulern
- ğŸ” **Datenvalidierung & QualitÃ¤tssicherung**: Statistische Analyse, Anomalieerkennung, KonsistenzprÃ¼fung, Echtzeit-Ãœberwachung
- ğŸ› **Umfassendes Debug & Logging**: Strukturiertes Logging, Performance-Profiling, Memory-Tracking, automatisierte Alerts

## ğŸš€ Schnellstart

**ğŸ““ FÃ¼r die vollstÃ¤ndige Jupyter-Setup-Anleitung, siehe [README_JUPYTER.md](../../README_JUPYTER.md)**

### Python Jupyter Lab Demo

ğŸ““ **[VollstÃ¤ndige Jupyter-Setup-Anleitung](../../README_JUPYTER.md)** | **[Jupyter-Anleitung](jupyter-guide.md)**

#### Standard-CPU-Demo
RusTorch mit Jupyter Lab in einem Befehl starten:

```bash
./start_jupyter.sh
```

#### WebGPU-beschleunigte Demo
RusTorch mit WebGPU-UnterstÃ¼tzung fÃ¼r browserbasierte GPU-Beschleunigung starten:

```bash
./start_jupyter_webgpu.sh
```

### Grundlegende Installation

```bash
cargo add rustorch
```

### Einfaches Beispiel

```rust
use rustorch::{
    tensor::Tensor,
    nn::{Linear, Module},
    optim::SGD,
};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Erstelle Tensor
    let x = Tensor::<f32>::new(vec![1.0, 2.0, 3.0, 4.0], vec![2, 2])?;
    let y = Tensor::<f32>::new(vec![0.0, 1.0], vec![2, 1])?;
    
    // Erstelle Modell
    let mut model = Linear::<f32>::new(2, 1)?;
    let mut optimizer = SGD::new(model.parameters(), 0.01)?;
    
    // Training
    for epoch in 0..100 {
        // Forward-Pass
        let pred = model.forward(&x)?;
        let loss = (pred - &y)?.pow(2.0)?.mean()?;
        
        // Backward-Pass
        loss.backward()?;
        optimizer.step()?;
        optimizer.zero_grad()?;
        
        if epoch % 10 == 0 {
            println!("Epoch {}: Loss = {:.4}", epoch, loss.data()[0]);
        }
    }
    
    Ok(())
}
```

## ğŸ“‹ Anforderungen

- **Rust**: Version 1.75.0 oder hÃ¶her
- **Plattformen**: Windows, macOS, Linux
- **Optional**: CUDA 12.0+, OpenCL 2.0+, Metal (fÃ¼r GPU-Beschleunigung)

## ğŸ¯ AnwendungsfÃ¤lle

### ğŸ­ Unternehmensbereit
- âœ… **968 Tests bestanden**: Umfassende Test-Suite gewÃ¤hrleistet ZuverlÃ¤ssigkeit
- ğŸ”’ **Memory-sicher**: Rust verhindert Segfaults und Memory-Leaks
- ğŸ“Š **Performance-profiled**: Optimiert fÃ¼r Produktionslasten
- ğŸŒ **PlattformÃ¼bergreifend**: LÃ¤uft auf Windows, macOS, Linux

### ğŸ§  Machine Learning
- ğŸ”¥ **Transformer-Modelle**: State-of-the-art NLP mit Multi-Head-Attention
- ğŸ–¼ï¸ **Computer Vision**: CNN, ResNet, Bildklassifizierung
- ğŸ“Š **Datenanalyse**: Statistik, Anomalieerkennung, Zeitreihen
- ğŸ¯ **Reinforcement Learning**: Richtlinien-Gradienten, Q-Learning

### âš¡ High Performance Computing
- ğŸš€ **SIMD-Optimierungen**: AVX2/SSE/NEON-Vektorisierung
- ğŸ® **Multi-GPU**: CUDA/Metal/OpenCL-Parallelisierung
- ğŸ§µ **Multithreading**: Rayon-basierte Parallelverarbeitung
- ğŸ’¾ **Memory-optimiert**: Efficient Memory-Pooling und Caching

### ğŸŒ Web & Edge Deployment
- ğŸ“± **WebAssembly**: VollstÃ¤ndige ML-Pipeline im Browser
- âš¡ **WebGPU**: Chrome-optimierte GPU-Beschleunigung
- ğŸ”§ **Edge-Computing**: Eingebettete Systeme und IoT
- ğŸŒ **PlattformunabhÃ¤ngig**: Ein Code-base, Ã¼berall lauffÃ¤hig

## ğŸ—ï¸ Architektur

RusTorch folgt einer modularen Architektur mit klaren Verantwortlichkeiten:

```
rustorch/
â”œâ”€â”€ tensor/          # Kern-Tensoroperationen
â”œâ”€â”€ autograd/        # Automatische Differentiation
â”œâ”€â”€ nn/              # Neuronale Netzwerkschichten
â”œâ”€â”€ optim/           # Optimierungsalgorithmen
â”œâ”€â”€ data/            # Datenladen und -vorverarbeitung  
â”œâ”€â”€ gpu/             # GPU-Backend (CUDA/Metal/OpenCL)
â”œâ”€â”€ wasm/            # WebAssembly-Bindings
â””â”€â”€ examples/        # Beispiele und Demos
```

## ğŸ”§ Features

### Tensor-Operationen
```rust
let a = Tensor::<f32>::ones(vec![3, 3])?;
let b = Tensor::<f32>::eye(3)?;
let c = a.matmul(&b)?;  // Matrix-Multiplikation
let d = c.transpose(0, 1)?;  // Transponierung
```

### GPU-Beschleunigung
```rust
#[cfg(feature = "cuda")]
{
    let device = Device::cuda(0)?;
    let tensor = Tensor::<f32>::ones(vec![1000, 1000])?.to_device(&device)?;
    let result = tensor.matmul(&tensor)?;  // GPU-beschleunigt
}
```

### Transformer-Architektur
```rust
use rustorch::nn::transformer::TransformerEncoder;

let encoder = TransformerEncoder::<f32>::new(
    512,    // d_model
    8,      // num_heads
    2048,   // d_ff
    6,      // num_layers
    0.1,    // dropout
)?;
```

## ğŸ“š Dokumentation

- ğŸ“– **[VollstÃ¤ndige API-Dokumentation](https://docs.rs/rustorch)**
- ğŸ““ **[Jupyter-Setup-Anleitung](../../README_JUPYTER.md)**
- ğŸ¯ **[Beispiele](../../examples/)**
- ğŸ§ª **[Tests](../../tests/)**

## ğŸ¤ Beitragen

BeitrÃ¤ge sind willkommen! Siehe [CONTRIBUTING.md](../../CONTRIBUTING.md) fÃ¼r Details.

## ğŸ“„ Lizenz

Dieses Projekt steht unter MIT ODER Apache-2.0 Lizenz - siehe die [LICENSE](../../LICENSE) Dateien fÃ¼r Details.

## ğŸ™ Danksagungen

- Inspiriert von PyTorch fÃ¼r API-Design
- Rust-Community fÃ¼r auÃŸergewÃ¶hnliche Werkzeuge
- Candle fÃ¼r Rust-ML-Inspiration
- Alle Mitwirkenden und Tester

---

**ğŸš€ Bereit loszulegen? Schauen Sie sich unsere [Jupyter-Demo](../../README_JUPYTER.md) an!**