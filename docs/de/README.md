# RusTorch ğŸš€

[![Crates.io](https://img.shields.io/crates/v/rustorch)](https://crates.io/crates/rustorch)
[![Documentation](https://docs.rs/rustorch/badge.svg)](https://docs.rs/rustorch)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/JunSuzukiJapan/rustorch)
[![Tests](https://img.shields.io/badge/tests-1128%20passing-brightgreen.svg)](#testing)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](#testing)

**Produktionsbereite Deep Learning Bibliothek in Rust mit PyTorch-Ã¤hnlicher API, GPU-Beschleunigung und Enterprise-Performance**

RusTorch ist eine voll funktionsfÃ¤hige Deep Learning Bibliothek, die Rusts Sicherheit und Performance nutzt und umfassende Tensor-Operationen, automatische Differenzierung, neurale Netzwerk-Layer, Transformer-Architekturen, Multi-Backend GPU-Beschleunigung (CUDA/Metal/OpenCL), erweiterte SIMD-Optimierungen, Enterprise-Level Speicherverwaltung, Datenvalidierung und QualitÃ¤tssicherung sowie umfassende Debug- und Logging-Systeme bietet.

## ğŸ“š Dokumentation

- **[VollstÃ¤ndige API-Referenz](API_DOCUMENTATION.md)** - Umfassende API-Dokumentation fÃ¼r alle Module
- **[WASM API-Referenz](WASM_API_DOCUMENTATION.md)** - WebAssembly-spezifische API-Dokumentation
- **[Jupyter Leitfaden](jupyter-guide.md)** - Anleitung zur Verwendung von Jupyter Notebooks

## âœ¨ Features

- ğŸ”¥ **Umfassende Tensor-Operationen**: Mathematische Operationen, Broadcasting, Indizierung und Statistiken, Phase 8 erweiterte Utilities
- ğŸ¤– **Transformer-Architektur**: VollstÃ¤ndige Transformer-Implementierung mit Multi-Head-Attention
- ğŸ§® **Matrix-Zerlegung**: SVD, QR, Eigenwert-Zerlegung mit PyTorch-KompatibilitÃ¤t
- ğŸ§  **Automatische Differenzierung**: Tape-basierter Berechnungsgraph fÃ¼r Gradientenberechnung
- ğŸš€ **Dynamische AusfÃ¼hrungs-Engine**: JIT-Kompilation und Laufzeit-Optimierung
- ğŸ—ï¸ **Neurale Netzwerk-Layer**: Linear, Conv1d/2d/3d, ConvTranspose, RNN/LSTM/GRU, BatchNorm, Dropout und mehr
- âš¡ **Cross-Platform-Optimierungen**: SIMD (AVX2/SSE/NEON), plattformspezifische und hardware-bewusste Optimierungen
- ğŸ® **GPU-Integration**: CUDA/Metal/OpenCL-Support mit automatischer GerÃ¤tewahl
- ğŸŒ **WebAssembly-Support**: VollstÃ¤ndiges Browser-ML mit neuronalen Netzwerk-Layern, Computer Vision und Echtzeit-Inferenz
- ğŸ® **WebGPU-Integration**: Chrome-optimierte GPU-Beschleunigung mit CPU-Fallback fÃ¼r Cross-Browser-KompatibilitÃ¤t
- ğŸ“ **Modellformat-Support**: Safetensors, ONNX-Inferenz, PyTorch state dict KompatibilitÃ¤t
- âœ… **Produktionsbereit**: 1128 Tests bestanden, einheitliches Fehlerbehandlungssystem
- ğŸ¯ **Phase 8 Tensor-Utilities**: Bedingte Operationen (where, masked_select, masked_fill), Indizierungs-Operationen (gather, scatter, index_select), statistische Operationen (topk, kthvalue) und erweiterte Utilities (unique, histogram)

## ğŸš€ Schnellstart

**ğŸ““ FÃ¼r die vollstÃ¤ndige Jupyter-Setup-Anleitung siehe [README_JUPYTER.md](../../README_JUPYTER.md)**

### Python Jupyter Lab Demo

ğŸ““ **[VollstÃ¤ndige Jupyter-Setup-Anleitung](../../README_JUPYTER.md)** | **[Jupyter Leitfaden](jupyter-guide.md)**

#### Standard CPU Demo
RusTorch mit Jupyter Lab in einem Befehl starten:

```bash
./start_jupyter.sh
```

#### WebGPU-beschleunigte Demo
RusTorch mit WebGPU-Support fÃ¼r browser-basierte GPU-Beschleunigung starten:

```bash
./start_jupyter_webgpu.sh
```

### Rust-Verwendung

```rust
use rustorch::tensor::Tensor;
use rustorch::nn::{Linear, ReLU};
use rustorch::optim::Adam;

// Tensor-Erstellung
let x = Tensor::randn(vec![32, 784]); // Batch-GrÃ¶ÃŸe 32, Features 784
let y = Tensor::randn(vec![32, 10]);  // 10 Klassen

// Neurales Netzwerk definieren
let linear1 = Linear::new(784, 256)?;
let relu = ReLU::new();
let linear2 = Linear::new(256, 10)?;

// VorwÃ¤rtsdurchgang
let z1 = linear1.forward(&x)?;
let a1 = relu.forward(&z1)?;
let output = linear2.forward(&a1)?;

// Optimierer
let mut optimizer = Adam::new(
    vec![linear1.weight(), linear2.weight()], 
    0.001, 0.9, 0.999, 1e-8
)?;
```

## ğŸ§ª Tests

### Alle Tests ausfÃ¼hren
```bash
cargo test --lib
```

### Feature-spezifische Tests
```bash
cargo test tensor::     # Tensor-Operationen Tests
cargo test nn::         # Neurale Netzwerk Tests
cargo test autograd::   # Automatische Differenzierung Tests
cargo test optim::      # Optimierer Tests
cargo test gpu::        # GPU-Operationen Tests
```

## ğŸ”§ Installation

### Cargo.toml
```toml
[dependencies]
rustorch = "0.5.15"

# GPU Features
rustorch = { version = "0.5.15", features = ["cuda"] }      # CUDA
rustorch = { version = "0.5.15", features = ["metal"] }     # Metal (macOS)
rustorch = { version = "0.5.15", features = ["opencl"] }    # OpenCL

# WebAssembly
rustorch = { version = "0.5.15", features = ["wasm"] }      # WASM Basic
rustorch = { version = "0.5.15", features = ["webgpu"] }    # WebGPU
```

## âš ï¸ Bekannte EinschrÃ¤nkungen

1. **GPU-Speicher-BeschrÃ¤nkung**: Explizite Speicherverwaltung erforderlich fÃ¼r groÃŸe Tensoren (>8GB)
2. **WebAssembly-BeschrÃ¤nkung**: Einige BLAS-Operationen nicht verfÃ¼gbar in WASM-Umgebung
3. **Verteiltes Lernen**: NCCL-Backend nur unter Linux unterstÃ¼tzt
4. **Metal-BeschrÃ¤nkung**: Einige erweiterte Operationen nur mit CUDA-Backend verfÃ¼gbar

## ğŸ¤ Beitrag

Pull Requests und Issues sind willkommen! Siehe [CONTRIBUTING.md](../../CONTRIBUTING.md) fÃ¼r Details.

## ğŸ“„ Lizenz

MIT Lizenz - siehe [LICENSE](../../LICENSE) fÃ¼r Details.

---

**Entwickelt von**: Jun Suzuki | **Version**: v0.5.15 | **Letzte Aktualisierung**: 2025