# F32GPTModel Metal GPUå®Ÿè£… - ç¾çŠ¶å ±å‘Š

## å®Œäº†ã—ãŸä½œæ¥­

### 1. F32GPTModelå®Ÿè£… âœ…
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/hybrid_f32/models/gpt.rs`
- **æ©Ÿèƒ½**:
  - ãƒã‚¤ãƒ†ã‚£ãƒ–f32ç²¾åº¦GPTãƒ¢ãƒ‡ãƒ«
  - DeviceTypeå¯¾å¿œ: Metal, CoreML, CPU, Hybrid
  - GGUFãƒ¢ãƒ‡ãƒ«ã‚’f64ã‹ã‚‰f32ã«å¤‰æ›ã—ã¦ãƒ­ãƒ¼ãƒ‰
  - Metal/CoreMLãƒãƒƒãƒ•ã‚¡çµ±åˆ

### 2. Forward Passå®Ÿè£… âœ…
- **æ©Ÿèƒ½**:
  - `get_embeddings()`: ãƒˆãƒ¼ã‚¯ãƒ³IDã‹ã‚‰åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
  - `apply_layer_norm()`: Metal GPU LayerNormï¼ˆMetal/CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
  - `project_to_vocab()`: éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰ãƒ­ã‚¸ãƒƒãƒˆç”Ÿæˆ
  - Metal LayerNormã‚«ãƒ¼ãƒãƒ«çµ±åˆå®Œäº†

### 3. example-cliçµ±åˆ âœ…
- **ãƒ•ã‚¡ã‚¤ãƒ«**:
  - `example-cli/src/model/inference.rs`
  - `example-cli/src/main.rs`
- **æ©Ÿèƒ½**:
  - InferenceEngineã«F32GPTModelå¯¾å¿œ
  - `--hybrid-f32`ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§Metal GPUè‡ªå‹•é¸æŠ
  - `generate_with_f32_gpt()`: æ¨è«–ãƒ«ãƒ¼ãƒ—å®Ÿè£…
  - æ—¢å­˜ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆf64 GPTModelï¼‰ã¨ã®äº’æ›æ€§ç¶­æŒ

## ç™ºè¦‹ã—ãŸåˆ¶é™äº‹é …

### GGUFé‡å­åŒ–å½¢å¼æœªå¯¾å¿œ âš ï¸

**å•é¡Œ**: RusTorchã®GGUFLoaderã¯ç¾åœ¨ã€Q4_K/Q6_Ké‡å­åŒ–å½¢å¼ã«æœªå¯¾å¿œ

#### TinyLlama Q4_K_M ãƒ¢ãƒ‡ãƒ«åˆ†æ
```
ç·ãƒ†ãƒ³ã‚µãƒ¼æ•°: 201
âœ… ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: 45 (22%)  - F32 LayerNorm weights
âŒ ãƒ­ãƒ¼ãƒ‰å¤±æ•—: 156 (78%) - Q4_K/Q6_K quantized weights
```

#### ãƒ­ãƒ¼ãƒ‰æˆåŠŸã—ãŸãƒ†ãƒ³ã‚µãƒ¼
```
blk.0.attn_norm.weight    (F32)
blk.0.ffn_norm.weight     (F32)
blk.1.attn_norm.weight    (F32)
...
output_norm.weight        (F32)
```

#### ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã—ãŸãƒ†ãƒ³ã‚µãƒ¼
```
token_embd.weight                (Q4_K) - Token embeddings
blk.0.attn_q.weight             (Q4_K) - Query projection
blk.0.attn_k.weight             (Q4_K) - Key projection
blk.0.attn_v.weight             (Q6_K) - Value projection
blk.0.attn_output.weight        (Q4_K) - Output projection
blk.0.ffn_gate.weight           (Q4_K) - FFN gate
blk.0.ffn_up.weight             (Q4_K) - FFN up
blk.0.ffn_down.weight           (Q6_K) - FFN down
output.weight                    (Q6_K) - Output projection
```

#### ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
```
Parse error: Tensor type Q4_K not yet supported for loading
Parse error: Tensor type Q6_K not yet supported for loading
```

## ãƒ†ã‚¹ãƒˆçµæœ

### ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
```bash
$ cargo build --package rustorch-cli --features hybrid-f32 --release
$ echo "Hello" | ./target/release/rustorch-cli \
    --model ~/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
    --backend hybrid-f32 \
    --max-tokens 5
```

**çµæœ**:
```
âœ… F32 GPT model loaded successfully on Metal backend (Metal GPU)
ğŸ“Š Loading GPT model weights as f32
âœ… Loaded 45 weights as f32
âŒ Embedding weight not found
```

### å®Ÿè¡Œãƒ­ã‚°
```
ğŸš€ Creating F32GPTModel with Metal device
   Precision: native f32 (optimized for GPU)
ğŸ“Š Loading GPT model weights as f32
   Device: Metal
   Vocab size: 32000
   Layers: 22
   d_model: 2048
âš ï¸  Failed to load tensor 'token_embd.weight': Parse error: Tensor type Q4_K not yet supported
âš ï¸  Failed to load tensor 'blk.0.attn_q.weight': Parse error: Tensor type Q4_K not yet supported
âš ï¸  Failed to load tensor 'blk.0.attn_v.weight': Parse error: Tensor type Q6_K not yet supported
...
âœ… Loaded 45 weights as f32

ğŸ”„ F32GPTModel forward pass
   Device: Metal
   Input length: 2
âŒ Error: Embedding weight not found
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### Option A: GGUFé‡å­åŒ–ãƒ‡ã‚³ãƒ¼ãƒ€å®Ÿè£… (æ¨å¥¨)
1. **Q4_Kãƒ‡ã‚³ãƒ¼ãƒ€å®Ÿè£…**
   - ãƒ•ã‚¡ã‚¤ãƒ«: `src/formats/gguf.rs`
   - å®Ÿè£…: `decode_q4_k()` é–¢æ•°
   - å‚è€ƒ: llama.cpp ã®å®Ÿè£…

2. **Q6_Kãƒ‡ã‚³ãƒ¼ãƒ€å®Ÿè£…**
   - å®Ÿè£…: `decode_q6_k()` é–¢æ•°
   - å‚è€ƒ: llama.cpp ã®å®Ÿè£…

3. **ãƒ†ã‚¹ãƒˆ**
   - å®Œå…¨ãª201ãƒ†ãƒ³ã‚µãƒ¼ãƒ­ãƒ¼ãƒ‰ç¢ºèª
   - Metal GPUæ¨è«–å®Ÿè¡Œ
   - æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### Option B: F32 GGUFãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
1. **éé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«å–å¾—**
   - TinyLlama F32/F16ç‰ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
   - ã¾ãŸã¯ã€è‡ªåˆ†ã§é‡å­åŒ–è§£é™¤

2. **å³åº§ã«ãƒ†ã‚¹ãƒˆå¯èƒ½**
   - å…¨ãƒ†ãƒ³ã‚µãƒ¼ãŒãƒ­ãƒ¼ãƒ‰å¯èƒ½
   - Metal GPUæ¨è«–ã‚’å³åº§ã«æ¤œè¨¼

### Option C: PyTorch/Safetensorså½¢å¼å¯¾å¿œ
1. **Safetensorsãƒ­ãƒ¼ãƒ€ãƒ¼æ‹¡å¼µ**
   - F32GPTModelç”¨ã®Safetensorsãƒ­ãƒ¼ãƒ‰å®Ÿè£…
   - HuggingFace transformersãƒ¢ãƒ‡ãƒ«å¯¾å¿œ

## æŠ€è¡“çš„è©³ç´°

### Metal GPU LayerNorm
- **å®Ÿè£…**: `src/gpu/metal_kernels.rs`
- **ã‚«ãƒ¼ãƒãƒ«**: `metal_layer_norm_f32()`
- **æ©Ÿèƒ½**: GPUåŠ é€Ÿæ­£è¦åŒ–å‡¦ç†
- **çŠ¶æ…‹**: âœ… å®Ÿè£…å®Œäº†ã€ãƒ†ã‚¹ãƒˆæ¸ˆã¿

### F32Tensoræ§‹é€ 
```rust
pub struct F32Tensor {
    pub data: Array<f32, IxDyn>,
    pub metal_buffer: Option<Arc<MetalBuffer>>,
    pub coreml_buffer: Option<Arc<CoreMLBuffer>>,
    pub device_state: DeviceState,
    pub requires_grad: bool,
}
```

### Forward Pass ãƒ•ãƒ­ãƒ¼
```
1. get_embeddings()     â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿å–å¾—
2. apply_layer_norm()   â†’ Metal GPU LayerNorm
3. [TODO] attention()   â†’ Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
4. [TODO] ffn()         â†’ Feed-Forward Network
5. project_to_vocab()   â†’ ãƒ­ã‚¸ãƒƒãƒˆç”Ÿæˆ
```

## é–¢é€£ã‚³ãƒŸãƒƒãƒˆ

1. `feat: ãƒã‚¤ãƒ†ã‚£ãƒ–f32ç²¾åº¦GPTãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆMetal GPUå¯¾å¿œï¼‰` - 9a549af49
2. `feat: Metal GPU LayerNormçµ±åˆã®forward passå®Ÿè£…` - 5c770fd6d
3. `feat: F32GPTModelçµ±åˆ - Metal GPUæ¨è«–ã‚’example-cliã«å®Ÿè£…` - 3e58b601e
4. `fix: GGUFé‡å­åŒ–ãƒ†ãƒ³ã‚µãƒ¼æœªå¯¾å¿œå•é¡Œã‚’ç™ºè¦‹ãƒ»è¨ºæ–­` - 4aded4dbe

## ã¾ã¨ã‚

### é”æˆäº‹é …
- âœ… F32GPTModelå®Œå…¨å®Ÿè£…
- âœ… Metal LayerNormã‚«ãƒ¼ãƒãƒ«çµ±åˆ
- âœ… example-cliçµ±åˆ
- âœ… éƒ¨åˆ†çš„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸï¼ˆLayerNorm weightsï¼‰

### æœªé”æˆ
- âŒ é‡å­åŒ–GGUFãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãƒ­ãƒ¼ãƒ‰
- âŒ ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¨è«–å®Ÿè¡Œ
- âŒ Metal GPUæ€§èƒ½æ¸¬å®š

### æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
**Option A (GGUFé‡å­åŒ–å¯¾å¿œ)** ã‚’æ¨å¥¨ï¼š
- æœ€ã‚‚ä¸€èˆ¬çš„ãªãƒ¢ãƒ‡ãƒ«å½¢å¼
- HuggingFaceãƒ¢ãƒ‡ãƒ«ã®å¤§åŠãŒGGUFå½¢å¼
- å®Ÿè£…ã™ã‚Œã°å¹…åºƒã„ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨å¯èƒ½

## å‚è€ƒè³‡æ–™

- llama.cpp Q4_K/Q6_Kå®Ÿè£…: https://github.com/ggerganov/llama.cpp
- GGUFä»•æ§˜: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- Metal Performance Shaders: https://developer.apple.com/metal/
