//! Heavy Metal vs CoreML Performance Benchmark
//! é‡ã„Metal vs CoreMLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
//!
//! This benchmark is designed to run for approximately 1 hour and showcase
//! the true performance differences between Metal GPU acceleration and
//! CoreML Neural Engine optimization on Apple Silicon.
//!
//! ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ç´„1æ™‚é–“ã®å®Ÿè¡Œã‚’æƒ³å®šã—ã€Apple Siliconä¸Šã§ã®
//! Metal GPUåŠ é€Ÿã¨CoreML Neural Engineæœ€é©åŒ–ã®çœŸã®æ€§èƒ½å·®ã‚’ç¤ºã™ã‚ˆã†è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
//!
//! **WARNING: This benchmark is computationally intensive and will:**
//! **è­¦å‘Š: ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯è¨ˆç®—é›†ç´„çš„ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:**
//! - Run for approximately 1 hour
//! - Use 4-8GB+ of memory
//! - Generate significant heat (thermal throttling may occur)
//! - Consume significant battery power
//!
//! Run with: cd benchmarks && cargo run --bin metal_coreml_heavy_benchmark --features "metal coreml" --release
//! Or for intelligent backend selection: cd benchmarks && cargo run --bin metal_coreml_heavy_benchmark --features "mac-hybrid" --release
//! å®Ÿè¡Œæ–¹æ³•: cd benchmarks && cargo run --bin metal_coreml_heavy_benchmark --features "metal coreml" --release
//! ã¾ãŸã¯ã€ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠç”¨: cd benchmarks && cargo run --bin metal_coreml_heavy_benchmark --features "mac-hybrid" --release
//!
//! To skip this benchmark in CI, the following environment variables disable it:
//! CIã§ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã§ç„¡åŠ¹åŒ–ã§ãã¾ã™:
//! - RUSTORCH_SKIP_HEAVY_BENCHMARK=1
//! - CI=true (without RUSTORCH_HEAVY_BENCHMARK=1)

use rustorch::error::RusTorchResult;
use rustorch::tensor::Tensor;
use std::collections::HashMap;
use std::env;
use std::time::{Duration, Instant};

// Conditional imports for available features

// Removed DeviceType import - using fully qualified name instead

/// Heavy benchmark configuration for 1-hour runtime
/// 1æ™‚é–“å®Ÿè¡Œç”¨ã®é‡ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
#[derive(Debug, Clone)]
struct HeavyBenchmarkConfig {
    // Phase 1: Large-scale matrix operations (20 minutes)
    // ãƒ•ã‚§ãƒ¼ã‚º1: å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—ï¼ˆ20åˆ†ï¼‰
    matrix_size: usize,           // 2048x2048 matrices
    matrix_batch_size: usize,     // Multiple matrices in parallel
    matrix_iterations: usize,     // Number of operations per timing
    matrix_duration_minutes: f64, // Target duration for this phase

    // Phase 2: Deep neural network inference (20 minutes)
    // ãƒ•ã‚§ãƒ¼ã‚º2: æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¨è«–ï¼ˆ20åˆ†ï¼‰
    image_size: usize,          // 1024x1024 input images
    image_batch_size: usize,    // Batch processing
    conv_layers: usize,         // Number of convolution layers (20+)
    conv_channels: Vec<usize>,  // Channel progression
    conv_duration_minutes: f64, // Target duration for this phase

    // Phase 3: Transformer-style attention (20 minutes)
    // ãƒ•ã‚§ãƒ¼ã‚º3: Transformeré¢¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆ20åˆ†ï¼‰
    sequence_length: usize,          // Input sequence length
    embedding_dim: usize,            // Embedding dimensions
    num_heads: usize,                // Multi-head attention heads
    attention_layers: usize,         // Number of attention layers
    attention_duration_minutes: f64, // Target duration for this phase

    // Global settings
    // å…¨ä½“è¨­å®š
    warmup_iterations: usize,       // Warmup before timing
    measurement_interval: Duration, // How often to record metrics
}

impl Default for HeavyBenchmarkConfig {
    fn default() -> Self {
        Self {
            // Phase 1: Large matrix operations
            matrix_size: 2048,
            matrix_batch_size: 4,
            matrix_iterations: 10,
            matrix_duration_minutes: 20.0,

            // Phase 2: Deep convolution network
            image_size: 1024,
            image_batch_size: 8,
            conv_layers: 24,
            conv_channels: vec![3, 64, 128, 256, 512, 1024], // Progressive channel increase
            conv_duration_minutes: 20.0,

            // Phase 3: Transformer attention
            sequence_length: 1024,
            embedding_dim: 512,
            num_heads: 16,
            attention_layers: 12,
            attention_duration_minutes: 20.0,

            // Global settings
            warmup_iterations: 3,
            measurement_interval: Duration::from_secs(30), // Record every 30 seconds
        }
    }
}

/// Performance metrics collected during benchmark
/// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¸­ã«åé›†ã•ã‚Œã‚‹æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹
#[derive(Debug, Clone)]
struct PerformanceMetrics {
    #[cfg(feature = "metal")]
    timestamp: Duration,
    operation_time_ms: f64,
    memory_usage_mb: f64,
    #[cfg(feature = "metal")]
    temperature_celsius: Option<f32>,
    #[cfg(feature = "coreml")]
    cpu_usage_percent: f32,
    #[cfg(feature = "metal")]
    gpu_usage_percent: f32,
    #[cfg(feature = "metal")]
    power_usage_watts: Option<f32>,
}

/// Comprehensive benchmark results
/// åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
#[derive(Debug, Clone)]
struct HeavyBenchmarkResult {
    device_name: String,
    phase_results: HashMap<String, PhaseResult>,
    total_duration: Duration,
    total_operations: usize,
    thermal_throttling_detected: bool,
    average_memory_usage_mb: f64,
    peak_memory_usage_mb: f64,
    power_efficiency_score: Option<f64>, // Operations per watt
    metrics_timeline: Vec<PerformanceMetrics>,
}

#[derive(Debug, Clone)]
struct PhaseResult {
    phase_name: String,
    duration: Duration,
    operations_completed: usize,
    average_op_time_ms: f64,
    throughput_ops_per_sec: f64,
    memory_peak_mb: f64,
    success_rate: f64,
    thermal_events: usize,
}

/// Heavy performance benchmark executor
/// é‡ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå™¨
struct HeavyPerformanceBenchmark {
    config: HeavyBenchmarkConfig,
    results: HashMap<String, HeavyBenchmarkResult>,
    start_time: Instant,
}

impl HeavyPerformanceBenchmark {
    fn new(config: HeavyBenchmarkConfig) -> Self {
        Self {
            config,
            results: HashMap::new(),
            start_time: Instant::now(),
        }
    }

    /// Check if heavy benchmark should be skipped
    /// é‡ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯
    fn should_skip_benchmark() -> bool {
        // Skip in CI unless explicitly enabled
        if env::var("CI").is_ok() && env::var("RUSTORCH_HEAVY_BENCHMARK").is_err() {
            return true;
        }

        // Skip if explicitly disabled
        if env::var("RUSTORCH_SKIP_HEAVY_BENCHMARK").is_ok() {
            return true;
        }

        false
    }

    /// Phase 1: Heavy matrix multiplication benchmark
    /// ãƒ•ã‚§ãƒ¼ã‚º1: é‡ã„è¡Œåˆ—ä¹—ç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    #[cfg(all(feature = "metal", not(feature = "mac-hybrid")))]
    fn benchmark_heavy_metal_matrices(&mut self) -> RusTorchResult<PhaseResult> {
        println!("ğŸ”¥ Phase 1: Heavy Metal Matrix Operations (20 minutes)");
        println!(
            "    Matrix size: {}x{}",
            self.config.matrix_size, self.config.matrix_size
        );
        println!("    Batch size: {}", self.config.matrix_batch_size);

        let mut metrics_timeline = Vec::new();
        let phase_start = Instant::now();
        let target_duration =
            Duration::from_secs((self.config.matrix_duration_minutes * 60.0) as u64);

        let mut operations_completed = 0;
        let mut total_operation_time = Duration::ZERO;

        // Initialize Metal executor for GPU matrix operations
        use rustorch::gpu::matrix_ops::GpuMatrixExecutor;
        let executor = GpuMatrixExecutor::new(rustorch::gpu::DeviceType::Metal(0))?;

        // Warmup
        println!("    ğŸ”„ Warming up Metal GPU...");
        for _ in 0..self.config.warmup_iterations {
            let a = Tensor::<f32>::ones(&[self.config.matrix_size, self.config.matrix_size]);
            let b = Tensor::<f32>::ones(&[self.config.matrix_size, self.config.matrix_size]);
            let _result = executor.metal_matmul(&a, &b)?;
        }

        println!("    âš¡ Starting heavy matrix computations...");
        let mut last_metric_time = Instant::now();

        while phase_start.elapsed() < target_duration {
            // Create large batch of matrices
            let mut matrices_a = Vec::new();
            let mut matrices_b = Vec::new();

            for _ in 0..self.config.matrix_batch_size {
                matrices_a.push(Tensor::<f32>::randn(&[
                    self.config.matrix_size,
                    self.config.matrix_size,
                ]));
                matrices_b.push(Tensor::<f32>::randn(&[
                    self.config.matrix_size,
                    self.config.matrix_size,
                ]));
            }

            // Measure batch operation time
            let op_start = Instant::now();
            for i in 0..self.config.matrix_batch_size {
                for _ in 0..self.config.matrix_iterations {
                    let _result = executor.metal_matmul(&matrices_a[i], &matrices_b[i])?;
                }
            }
            let op_duration = op_start.elapsed();

            operations_completed += self.config.matrix_batch_size * self.config.matrix_iterations;
            total_operation_time += op_duration;

            // Record metrics periodically
            if last_metric_time.elapsed() >= self.config.measurement_interval {
                let metrics = PerformanceMetrics {
                    #[cfg(feature = "metal")]
                    timestamp: phase_start.elapsed(),
                    operation_time_ms: op_duration.as_secs_f64() * 1000.0,
                    memory_usage_mb: self.estimate_memory_usage(),
                    #[cfg(feature = "metal")]
                    temperature_celsius: None, // TODO: Implement if possible
                    #[cfg(feature = "coreml")]
                    cpu_usage_percent: 0.0, // TODO: Implement if possible
                    #[cfg(feature = "metal")]
                    gpu_usage_percent: 0.0, // TODO: Implement if possible
                    #[cfg(feature = "metal")]
                    power_usage_watts: None, // TODO: Implement if possible
                };

                // Use the metrics in debug output (Metal-specific)
                #[cfg(feature = "metal")]
                {
                    println!(
                        "      ğŸ“Š Metal Metrics - Time: {:.2}ms, Memory: {:.1}MB, Temp: {:?}Â°C, GPU: {:.1}%",
                        metrics.operation_time_ms,
                        metrics.memory_usage_mb,
                        metrics.temperature_celsius,
                        metrics.gpu_usage_percent
                    );
                    println!(
                        "      â±ï¸  Timestamp: {:.1}s, Power: {:?}W",
                        metrics.timestamp.as_secs_f64(),
                        metrics.power_usage_watts
                    );
                }
                #[cfg(not(feature = "metal"))]
                {
                    println!(
                        "      ğŸ“Š Metrics - Time: {:.2}ms, Memory: {:.1}MB",
                        metrics.operation_time_ms, metrics.memory_usage_mb
                    );
                }

                metrics_timeline.push(metrics);
                last_metric_time = Instant::now();

                println!(
                    "      ğŸ“Š Progress: {:.1}min, {} ops completed",
                    phase_start.elapsed().as_secs_f64() / 60.0,
                    operations_completed
                );
            }
        }

        let phase_duration = phase_start.elapsed();
        let average_op_time = if operations_completed > 0 {
            total_operation_time.as_secs_f64() * 1000.0 / operations_completed as f64
        } else {
            0.0
        };

        let phase_result = PhaseResult {
            phase_name: "Heavy Metal Matrices".to_string(),
            duration: phase_duration,
            operations_completed,
            average_op_time_ms: average_op_time,
            throughput_ops_per_sec: operations_completed as f64 / phase_duration.as_secs_f64(),
            memory_peak_mb: self.estimate_memory_usage(),
            success_rate: 100.0,
            thermal_events: 0, // TODO: Detect thermal throttling
        };

        // Display detailed phase results
        println!(
            "    âœ… {} completed: {} operations in {:.1}min",
            phase_result.phase_name,
            phase_result.operations_completed,
            phase_result.duration.as_secs_f64() / 60.0
        );
        println!(
            "    ğŸ“Š Performance: {:.2}ms avg, {:.1} ops/sec, {:.1}MB peak, {:.1}% success",
            phase_result.average_op_time_ms,
            phase_result.throughput_ops_per_sec,
            phase_result.memory_peak_mb,
            phase_result.success_rate
        );
        println!("    ğŸŒ¡ï¸  Thermal events: {}", phase_result.thermal_events);

        Ok(phase_result)
    }

    /// Phase 1: Heavy Mac-Hybrid matrix operations
    /// ãƒ•ã‚§ãƒ¼ã‚º1: é‡ã„Mac-Hybridè¡Œåˆ—æ¼”ç®—
    #[cfg(feature = "mac-hybrid")]
    fn benchmark_heavy_mac_hybrid_matrices(&mut self) -> RusTorchResult<PhaseResult> {
        println!("ğŸ”€ Phase 1: Heavy Mac-Hybrid Matrix Operations (20 minutes)");
        println!(
            "    Matrix size: {}x{}",
            self.config.matrix_size, self.config.matrix_size
        );
        println!("    Utilizing intelligent Metal/CoreML selection...");

        let mut metrics_timeline = Vec::new();
        let phase_start = Instant::now();
        let target_duration =
            Duration::from_secs((self.config.matrix_duration_minutes * 60.0) as u64);

        let mut operations_completed = 0;
        let mut total_operation_time = Duration::ZERO;

        // Warmup hybrid system
        println!("    ğŸ”„ Warming up Mac-Hybrid system...");
        for _ in 0..self.config.warmup_iterations {
            let a = Tensor::<f32>::ones(&[self.config.matrix_size, self.config.matrix_size]);
            let b = Tensor::<f32>::ones(&[self.config.matrix_size, self.config.matrix_size]);
            // Use hybrid method which automatically selects best backend
            let _result = a.matmul_hybrid(&b)?;
        }

        println!("    âš¡ Starting heavy hybrid computations...");
        let mut last_metric_time = Instant::now();

        while phase_start.elapsed() < target_duration {
            // Create large batch of matrices
            let mut matrices_a = Vec::new();
            let mut matrices_b = Vec::new();

            for _ in 0..self.config.matrix_batch_size {
                matrices_a.push(Tensor::<f32>::randn(&[
                    self.config.matrix_size,
                    self.config.matrix_size,
                ]));
                matrices_b.push(Tensor::<f32>::randn(&[
                    self.config.matrix_size,
                    self.config.matrix_size,
                ]));
            }

            // Measure batch operation time with hybrid selection
            let op_start = Instant::now();
            for i in 0..self.config.matrix_batch_size {
                for _ in 0..self.config.matrix_iterations {
                    // Let mac-hybrid automatically choose Metal or CoreML
                    let _result = matrices_a[i].matmul_hybrid(&matrices_b[i])?;
                }
            }
            let op_duration = op_start.elapsed();

            operations_completed += self.config.matrix_batch_size * self.config.matrix_iterations;
            total_operation_time += op_duration;

            // Record metrics periodically
            if last_metric_time.elapsed() >= self.config.measurement_interval {
                let metrics = PerformanceMetrics {
                    #[cfg(feature = "metal")]
                    timestamp: phase_start.elapsed(),
                    operation_time_ms: op_duration.as_secs_f64() * 1000.0,
                    memory_usage_mb: self.estimate_memory_usage(),
                    #[cfg(feature = "metal")]
                    temperature_celsius: None,
                    #[cfg(feature = "coreml")]
                    cpu_usage_percent: 0.0,
                    #[cfg(feature = "metal")]
                    gpu_usage_percent: 0.0,
                    #[cfg(feature = "metal")]
                    power_usage_watts: None,
                };

                // Use the metrics in debug output (Mac-Hybrid-specific)
                #[cfg(feature = "mac-hybrid")]
                {
                    println!(
                        "      ğŸ“Š Mac-Hybrid Metrics - Time: {:.2}ms, Memory: {:.1}MB",
                        metrics.operation_time_ms, metrics.memory_usage_mb
                    );
                    #[cfg(feature = "metal")]
                    {
                        println!(
                            "      â±ï¸  Timestamp: {:.1}s, Temp: {:?}Â°C, GPU: {:.1}%, Power: {:?}W",
                            metrics.timestamp.as_secs_f64(),
                            metrics.temperature_celsius,
                            metrics.gpu_usage_percent,
                            metrics.power_usage_watts
                        );
                    }
                    #[cfg(feature = "coreml")]
                    {
                        println!("      ğŸ’» CPU: {:.1}%", metrics.cpu_usage_percent);
                    }
                }

                metrics_timeline.push(metrics);
                last_metric_time = Instant::now();

                println!(
                    "      ğŸ“Š Progress: {:.1}min, {} ops completed (Hybrid selection)",
                    phase_start.elapsed().as_secs_f64() / 60.0,
                    operations_completed
                );
            }
        }

        let phase_duration = phase_start.elapsed();
        let average_op_time = if operations_completed > 0 {
            total_operation_time.as_secs_f64() * 1000.0 / operations_completed as f64
        } else {
            0.0
        };

        let phase_result = PhaseResult {
            phase_name: "Heavy Mac-Hybrid Matrices".to_string(),
            duration: phase_duration,
            operations_completed,
            average_op_time_ms: average_op_time,
            throughput_ops_per_sec: operations_completed as f64 / phase_duration.as_secs_f64(),
            memory_peak_mb: self.estimate_memory_usage(),
            success_rate: 100.0,
            thermal_events: 0,
        };

        // Display detailed phase results
        println!(
            "    âœ… {} completed: {} operations in {:.1}min",
            phase_result.phase_name,
            phase_result.operations_completed,
            phase_result.duration.as_secs_f64() / 60.0
        );
        println!(
            "    ğŸ“Š Performance: {:.2}ms avg, {:.1} ops/sec, {:.1}MB peak, {:.1}% success",
            phase_result.average_op_time_ms,
            phase_result.throughput_ops_per_sec,
            phase_result.memory_peak_mb,
            phase_result.success_rate
        );
        println!("    ğŸŒ¡ï¸  Thermal events: {}", phase_result.thermal_events);

        Ok(phase_result)
    }

    /// Phase 1: Heavy CoreML matrix operations
    /// ãƒ•ã‚§ãƒ¼ã‚º1: é‡ã„CoreMLè¡Œåˆ—æ¼”ç®—
    #[cfg(all(feature = "coreml", not(feature = "mac-hybrid")))]
    fn benchmark_heavy_coreml_matrices(&mut self) -> RusTorchResult<PhaseResult> {
        println!("ğŸ§  Phase 1: Heavy CoreML Matrix Operations (20 minutes)");
        println!(
            "    Matrix size: {}x{}",
            self.config.matrix_size, self.config.matrix_size
        );
        println!("    Utilizing Apple Neural Engine...");

        let mut metrics_timeline = Vec::new();
        let phase_start = Instant::now();
        let target_duration =
            Duration::from_secs((self.config.matrix_duration_minutes * 60.0) as u64);

        let mut operations_completed = 0;
        let mut total_operation_time = Duration::ZERO;

        // Initialize CoreML backend
        use rustorch::gpu::matrix_ops::GpuMatrixExecutor;
        let executor = GpuMatrixExecutor::new(rustorch::gpu::DeviceType::CoreML(0))?;

        // Warmup Neural Engine
        println!("    ğŸ”„ Warming up Apple Neural Engine...");
        for _ in 0..self.config.warmup_iterations {
            let a = Tensor::<f32>::ones(&[self.config.matrix_size, self.config.matrix_size]);
            let b = Tensor::<f32>::ones(&[self.config.matrix_size, self.config.matrix_size]);
            let _result = executor.coreml_matmul(&a, &b)?;
        }

        println!("    âš¡ Starting heavy Neural Engine computations...");
        let mut last_metric_time = Instant::now();

        while phase_start.elapsed() < target_duration {
            // Create large batch of matrices optimized for Neural Engine
            let mut matrices_a = Vec::new();
            let mut matrices_b = Vec::new();

            for _ in 0..self.config.matrix_batch_size {
                matrices_a.push(Tensor::<f32>::randn(&[
                    self.config.matrix_size,
                    self.config.matrix_size,
                ]));
                matrices_b.push(Tensor::<f32>::randn(&[
                    self.config.matrix_size,
                    self.config.matrix_size,
                ]));
            }

            // Measure batch operation time with Neural Engine
            let op_start = Instant::now();
            for i in 0..self.config.matrix_batch_size {
                for _ in 0..self.config.matrix_iterations {
                    let _result = executor.coreml_matmul(&matrices_a[i], &matrices_b[i])?;
                }
            }
            let op_duration = op_start.elapsed();

            operations_completed += self.config.matrix_batch_size * self.config.matrix_iterations;
            total_operation_time += op_duration;

            // Record metrics periodically
            if last_metric_time.elapsed() >= self.config.measurement_interval {
                let metrics = PerformanceMetrics {
                    #[cfg(feature = "metal")]
                    timestamp: Duration::ZERO,
                    operation_time_ms: op_duration.as_secs_f64() * 1000.0,
                    memory_usage_mb: self.estimate_memory_usage(),
                    #[cfg(feature = "metal")]
                    temperature_celsius: None,
                    #[cfg(feature = "coreml")]
                    cpu_usage_percent: 0.0,
                    #[cfg(feature = "metal")]
                    gpu_usage_percent: 0.0,
                    #[cfg(feature = "metal")]
                    power_usage_watts: None,
                };

                // Use the metrics in debug output (CoreML-specific)
                #[cfg(feature = "coreml")]
                {
                    println!(
                        "      ğŸ“Š CoreML Metrics - Time: {:.2}ms, Memory: {:.1}MB, CPU: {:.1}%",
                        metrics.operation_time_ms,
                        metrics.memory_usage_mb,
                        metrics.cpu_usage_percent
                    );
                }
                #[cfg(not(feature = "coreml"))]
                {
                    println!(
                        "      ğŸ“Š Metrics - Time: {:.2}ms, Memory: {:.1}MB",
                        metrics.operation_time_ms, metrics.memory_usage_mb
                    );
                }

                metrics_timeline.push(metrics);
                last_metric_time = Instant::now();

                println!(
                    "      ğŸ“Š Progress: {:.1}min, {} ops completed (Neural Engine)",
                    phase_start.elapsed().as_secs_f64() / 60.0,
                    operations_completed
                );
            }
        }

        let phase_duration = phase_start.elapsed();
        let average_op_time = if operations_completed > 0 {
            total_operation_time.as_secs_f64() * 1000.0 / operations_completed as f64
        } else {
            0.0
        };

        let phase_result = PhaseResult {
            phase_name: "Heavy CoreML Matrices".to_string(),
            duration: phase_duration,
            operations_completed,
            average_op_time_ms: average_op_time,
            throughput_ops_per_sec: operations_completed as f64 / phase_duration.as_secs_f64(),
            memory_peak_mb: self.estimate_memory_usage(),
            success_rate: 100.0,
            thermal_events: 0,
        };

        // Display detailed phase results
        println!(
            "    âœ… {} completed: {} operations in {:.1}min",
            phase_result.phase_name,
            phase_result.operations_completed,
            phase_result.duration.as_secs_f64() / 60.0
        );
        println!(
            "    ğŸ“Š Performance: {:.2}ms avg, {:.1} ops/sec, {:.1}MB peak, {:.1}% success",
            phase_result.average_op_time_ms,
            phase_result.throughput_ops_per_sec,
            phase_result.memory_peak_mb,
            phase_result.success_rate
        );
        println!("    ğŸŒ¡ï¸  Thermal events: {}", phase_result.thermal_events);

        Ok(phase_result)
    }

    /// Phase 2: Heavy deep convolution network benchmark
    /// ãƒ•ã‚§ãƒ¼ã‚º2: é‡ã„æ·±å±¤ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_heavy_convolution_network(
        &mut self,
        device_name: &str,
    ) -> RusTorchResult<PhaseResult> {
        println!(
            "ğŸŒŠ Phase 2: Heavy Deep Convolution Network (20 minutes) - {}",
            device_name
        );
        println!(
            "    Input size: {}x{}, batch: {}",
            self.config.image_size, self.config.image_size, self.config.image_batch_size
        );
        println!("    Network depth: {} layers", self.config.conv_layers);

        let phase_start = Instant::now();
        let target_duration =
            Duration::from_secs((self.config.conv_duration_minutes * 60.0) as u64);

        let mut operations_completed = 0;
        let mut total_operation_time = Duration::ZERO;

        // Create large input batch
        let input_shape = [
            self.config.image_batch_size,
            self.config.conv_channels[0],
            self.config.image_size,
            self.config.image_size,
        ];

        println!("    ğŸ”„ Preparing deep convolution network...");

        let mut last_metric_time = Instant::now();

        while phase_start.elapsed() < target_duration {
            let input = Tensor::<f32>::randn(&input_shape);
            let mut current_tensor = input;

            // Deep convolution network simulation
            let op_start = Instant::now();

            for layer in 0..self.config.conv_layers {
                let in_channels = if layer == 0 {
                    self.config.conv_channels[0]
                } else {
                    self.config.conv_channels[(layer % (self.config.conv_channels.len() - 1)) + 1]
                };
                let out_channels =
                    self.config.conv_channels[(layer % (self.config.conv_channels.len() - 1)) + 1];

                // Create convolution kernel
                let kernel = Tensor::<f32>::randn(&[out_channels, in_channels, 3, 3]);

                // Perform convolution (CPU implementation for compatibility)
                // In a real implementation, this would use GPU/Neural Engine
                let conv_result = self.simulate_convolution(&current_tensor, &kernel)?;

                // Apply activation (ReLU)
                current_tensor = Tensor::from_ndarray(conv_result.data.mapv(|x| x.max(0.0)));

                // Simulate batch normalization
                let mean = current_tensor.data.mean().unwrap_or(0.0);
                let std = 1.0; // Simplified
                current_tensor =
                    Tensor::from_ndarray(current_tensor.data.mapv(|x| (x - mean) / std));
            }

            let op_duration = op_start.elapsed();
            operations_completed += 1;
            total_operation_time += op_duration;

            // Record metrics
            if last_metric_time.elapsed() >= self.config.measurement_interval {
                println!(
                    "      ğŸ“Š Progress: {:.1}min, {} deep networks processed",
                    phase_start.elapsed().as_secs_f64() / 60.0,
                    operations_completed
                );
                last_metric_time = Instant::now();
            }
        }

        let phase_duration = phase_start.elapsed();
        let average_op_time = if operations_completed > 0 {
            total_operation_time.as_secs_f64() * 1000.0 / operations_completed as f64
        } else {
            0.0
        };

        println!(
            "    âœ… Deep convolution phase completed: {} networks in {:.1}min",
            operations_completed,
            phase_duration.as_secs_f64() / 60.0
        );

        Ok(PhaseResult {
            phase_name: format!("Heavy Deep Convolution ({})", device_name),
            duration: phase_duration,
            operations_completed,
            average_op_time_ms: average_op_time,
            throughput_ops_per_sec: operations_completed as f64 / phase_duration.as_secs_f64(),
            memory_peak_mb: self.estimate_memory_usage(),
            success_rate: 100.0,
            thermal_events: 0,
        })
    }

    /// Phase 3: Heavy transformer-style attention benchmark
    /// ãƒ•ã‚§ãƒ¼ã‚º3: é‡ã„Transformeré¢¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_heavy_attention_network(
        &mut self,
        device_name: &str,
    ) -> RusTorchResult<PhaseResult> {
        println!(
            "ğŸ¤– Phase 3: Heavy Transformer Attention (20 minutes) - {}",
            device_name
        );
        println!(
            "    Sequence length: {}, embedding dim: {}",
            self.config.sequence_length, self.config.embedding_dim
        );
        println!(
            "    Attention heads: {}, layers: {}",
            self.config.num_heads, self.config.attention_layers
        );

        let phase_start = Instant::now();
        let target_duration =
            Duration::from_secs((self.config.attention_duration_minutes * 60.0) as u64);

        let mut operations_completed = 0;
        let mut total_operation_time = Duration::ZERO;

        let batch_size = 4;
        // Use 2D tensor for matrix multiplication compatibility
        let flattened_seq_dim = batch_size * self.config.sequence_length;
        let input_shape = [flattened_seq_dim, self.config.embedding_dim];

        println!("    ğŸ”„ Preparing transformer attention network...");

        let mut last_metric_time = Instant::now();

        while phase_start.elapsed() < target_duration {
            let input = Tensor::<f32>::randn(&input_shape);
            let mut current_tensor = input;

            // Multi-layer transformer simulation
            let op_start = Instant::now();

            for _layer in 0..self.config.attention_layers {
                // Multi-head attention simulation
                let _head_dim = self.config.embedding_dim / self.config.num_heads;

                // Create Q, K, V matrices for all heads
                let q_weight =
                    Tensor::<f32>::randn(&[self.config.embedding_dim, self.config.embedding_dim]);
                let k_weight =
                    Tensor::<f32>::randn(&[self.config.embedding_dim, self.config.embedding_dim]);
                let v_weight =
                    Tensor::<f32>::randn(&[self.config.embedding_dim, self.config.embedding_dim]);

                // Compute Q, K, V (simplified matrix multiplication)
                let _q = current_tensor.matmul(&q_weight)?;
                let _k = current_tensor.matmul(&k_weight)?;
                let v = current_tensor.matmul(&v_weight)?;

                // Attention computation (simplified)
                // For 2D tensors: [flattened_seq, embed_dim] operations
                let attention_weight =
                    Tensor::<f32>::randn(&[flattened_seq_dim, flattened_seq_dim]);
                let attention_output = attention_weight.matmul(&v)?;

                // Feed-forward network
                let ff_weight1 = Tensor::<f32>::randn(&[
                    self.config.embedding_dim,
                    self.config.embedding_dim * 4,
                ]);
                let ff_weight2 = Tensor::<f32>::randn(&[
                    self.config.embedding_dim * 4,
                    self.config.embedding_dim,
                ]);

                let ff_intermediate = attention_output.matmul(&ff_weight1)?;
                let ff_activated = Tensor::from_ndarray(ff_intermediate.data.mapv(|x| x.max(0.0))); // ReLU
                current_tensor = ff_activated.matmul(&ff_weight2)?;
            }

            let op_duration = op_start.elapsed();
            operations_completed += 1;
            total_operation_time += op_duration;

            // Record metrics
            if last_metric_time.elapsed() >= self.config.measurement_interval {
                println!(
                    "      ğŸ“Š Progress: {:.1}min, {} transformer networks processed",
                    phase_start.elapsed().as_secs_f64() / 60.0,
                    operations_completed
                );
                last_metric_time = Instant::now();
            }
        }

        let phase_duration = phase_start.elapsed();
        let average_op_time = if operations_completed > 0 {
            total_operation_time.as_secs_f64() * 1000.0 / operations_completed as f64
        } else {
            0.0
        };

        println!(
            "    âœ… Transformer attention phase completed: {} networks in {:.1}min",
            operations_completed,
            phase_duration.as_secs_f64() / 60.0
        );

        Ok(PhaseResult {
            phase_name: format!("Heavy Transformer Attention ({})", device_name),
            duration: phase_duration,
            operations_completed,
            average_op_time_ms: average_op_time,
            throughput_ops_per_sec: operations_completed as f64 / phase_duration.as_secs_f64(),
            memory_peak_mb: self.estimate_memory_usage(),
            success_rate: 100.0,
            thermal_events: 0,
        })
    }

    /// Run complete heavy benchmark for Metal
    /// Metalç”¨ã®å®Œå…¨é‡ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
    #[cfg(all(feature = "metal", not(feature = "mac-hybrid")))]
    fn run_heavy_metal_benchmark(&mut self) -> RusTorchResult<()> {
        println!("ğŸš€ Starting Heavy Metal Benchmark (â‰ˆ60 minutes total)");
        println!("=================================================");

        let mut phase_results = HashMap::new();

        // Phase 1: Heavy matrix operations
        let matrix_result = self.benchmark_heavy_metal_matrices()?;
        phase_results.insert("Phase1_Matrices".to_string(), matrix_result);

        // Phase 2: Deep convolution network
        let conv_result = self.benchmark_heavy_convolution_network("Metal")?;
        phase_results.insert("Phase2_Convolution".to_string(), conv_result);

        // Phase 3: Transformer attention
        let attention_result = self.benchmark_heavy_attention_network("Metal")?;
        phase_results.insert("Phase3_Attention".to_string(), attention_result);

        let total_duration = self.start_time.elapsed();
        let total_operations: usize = phase_results.values().map(|r| r.operations_completed).sum();

        let result = HeavyBenchmarkResult {
            device_name: "Metal GPU".to_string(),
            phase_results,
            total_duration,
            total_operations,
            thermal_throttling_detected: false, // TODO: Implement detection
            average_memory_usage_mb: self.estimate_memory_usage(),
            peak_memory_usage_mb: self.estimate_memory_usage(),
            power_efficiency_score: None, // TODO: Calculate if power data available
            metrics_timeline: Vec::new(), // TODO: Aggregate from phases
        };

        // Display comprehensive benchmark results
        println!("\nğŸ¯ {} Benchmark Summary:", result.device_name);
        println!(
            "   ğŸ“Š Total: {} operations in {:.1} minutes",
            result.total_operations,
            result.total_duration.as_secs_f64() / 60.0
        );
        println!(
            "   ğŸ’¾ Memory: Avg {:.1}MB, Peak {:.1}MB",
            result.average_memory_usage_mb, result.peak_memory_usage_mb
        );
        println!(
            "   ğŸŒ¡ï¸  Thermal throttling: {}, Power efficiency: {:?}",
            result.thermal_throttling_detected, result.power_efficiency_score
        );
        println!(
            "   ğŸ“ˆ Metrics collected: {} data points",
            result.metrics_timeline.len()
        );

        for (phase_name, phase_result) in &result.phase_results {
            println!(
                "   â€¢ {}: {:.1} ops/sec, {:.1}% success",
                phase_name, phase_result.throughput_ops_per_sec, phase_result.success_rate
            );
        }

        self.results.insert("Metal".to_string(), result);
        Ok(())
    }

    /// Run complete heavy benchmark for Mac-Hybrid
    /// Mac-Hybridç”¨ã®å®Œå…¨é‡ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
    #[cfg(feature = "mac-hybrid")]
    fn run_heavy_mac_hybrid_benchmark(&mut self) -> RusTorchResult<()> {
        println!("ğŸ”€ Starting Heavy Mac-Hybrid Benchmark (â‰ˆ60 minutes total)");
        println!("======================================================");

        let mut phase_results = HashMap::new();

        // Phase 1: Heavy matrix operations with intelligent selection
        let matrix_result = self.benchmark_heavy_mac_hybrid_matrices()?;
        phase_results.insert("Phase1_Matrices".to_string(), matrix_result);

        // Phase 2: Deep convolution network with hybrid optimization
        let conv_result = self.benchmark_heavy_convolution_network("Mac-Hybrid")?;
        phase_results.insert("Phase2_Convolution".to_string(), conv_result);

        // Phase 3: Transformer attention with hybrid optimization
        let attention_result = self.benchmark_heavy_attention_network("Mac-Hybrid")?;
        phase_results.insert("Phase3_Attention".to_string(), attention_result);

        let total_duration = self.start_time.elapsed();
        let total_operations: usize = phase_results.values().map(|r| r.operations_completed).sum();

        let result = HeavyBenchmarkResult {
            device_name: "Mac-Hybrid (Metal/CoreML Auto-Selection)".to_string(),
            phase_results,
            total_duration,
            total_operations,
            thermal_throttling_detected: false,
            average_memory_usage_mb: self.estimate_memory_usage(),
            peak_memory_usage_mb: self.estimate_memory_usage(),
            power_efficiency_score: None,
            metrics_timeline: Vec::new(),
        };

        // Display comprehensive benchmark results
        println!("\\nğŸ¯ {} Benchmark Summary:", result.device_name);
        println!(
            "   ğŸ“Š Total: {} operations in {:.1} minutes",
            result.total_operations,
            result.total_duration.as_secs_f64() / 60.0
        );
        println!(
            "   ğŸ’¾ Memory: Avg {:.1}MB, Peak {:.1}MB",
            result.average_memory_usage_mb, result.peak_memory_usage_mb
        );
        println!(
            "   ğŸŒ¡ï¸  Thermal throttling: {}, Power efficiency: {:?}",
            result.thermal_throttling_detected, result.power_efficiency_score
        );
        println!(
            "   ğŸ“ˆ Metrics collected: {} data points",
            result.metrics_timeline.len()
        );
        println!("   ğŸ”€ Hybrid Selection: Automatically chose optimal Metal/CoreML backends");

        for (phase_name, phase_result) in &result.phase_results {
            println!(
                "   â€¢ {}: {:.1} ops/sec, {:.1}% success",
                phase_name, phase_result.throughput_ops_per_sec, phase_result.success_rate
            );
        }

        self.results.insert("Mac-Hybrid".to_string(), result);
        Ok(())
    }

    /// Run complete heavy benchmark for CoreML
    /// CoreMLç”¨ã®å®Œå…¨é‡ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
    #[cfg(all(feature = "coreml", not(feature = "mac-hybrid")))]
    fn run_heavy_coreml_benchmark(&mut self) -> RusTorchResult<()> {
        println!("ğŸ§  Starting Heavy CoreML Benchmark (â‰ˆ60 minutes total)");
        println!("===================================================");

        let mut phase_results = HashMap::new();

        // Phase 1: Heavy matrix operations with Neural Engine
        let matrix_result = self.benchmark_heavy_coreml_matrices()?;
        phase_results.insert("Phase1_Matrices".to_string(), matrix_result);

        // Phase 2: Deep convolution network optimized for Neural Engine
        let conv_result = self.benchmark_heavy_convolution_network("CoreML")?;
        phase_results.insert("Phase2_Convolution".to_string(), conv_result);

        // Phase 3: Transformer attention optimized for Neural Engine
        let attention_result = self.benchmark_heavy_attention_network("CoreML")?;
        phase_results.insert("Phase3_Attention".to_string(), attention_result);

        let total_duration = self.start_time.elapsed();
        let total_operations: usize = phase_results.values().map(|r| r.operations_completed).sum();

        let result = HeavyBenchmarkResult {
            device_name: "CoreML Neural Engine".to_string(),
            phase_results,
            total_duration,
            total_operations,
            thermal_throttling_detected: false,
            average_memory_usage_mb: self.estimate_memory_usage(),
            peak_memory_usage_mb: self.estimate_memory_usage(),
            power_efficiency_score: None,
            metrics_timeline: Vec::new(),
        };

        // Display comprehensive benchmark results
        println!("\nğŸ¯ {} Benchmark Summary:", result.device_name);
        println!(
            "   ğŸ“Š Total: {} operations in {:.1} minutes",
            result.total_operations,
            result.total_duration.as_secs_f64() / 60.0
        );
        println!(
            "   ğŸ’¾ Memory: Avg {:.1}MB, Peak {:.1}MB",
            result.average_memory_usage_mb, result.peak_memory_usage_mb
        );
        println!(
            "   ğŸŒ¡ï¸  Thermal throttling: {}, Power efficiency: {:?}",
            result.thermal_throttling_detected, result.power_efficiency_score
        );
        println!(
            "   ğŸ“ˆ Metrics collected: {} data points",
            result.metrics_timeline.len()
        );

        for (phase_name, phase_result) in &result.phase_results {
            println!(
                "   â€¢ {}: {:.1} ops/sec, {:.1}% success",
                phase_name, phase_result.throughput_ops_per_sec, phase_result.success_rate
            );
        }

        self.results.insert("CoreML".to_string(), result);
        Ok(())
    }

    /// Display comprehensive benchmark results
    /// åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’è¡¨ç¤º
    fn display_heavy_results(&self) {
        println!("\nğŸ“Š Heavy Performance Benchmark Results");
        println!("======================================");
        println!(
            "Total benchmark duration: {:.1} minutes",
            self.start_time.elapsed().as_secs_f64() / 60.0
        );
        println!();

        for result in self.results.values() {
            println!("ğŸ† {} Results:", result.device_name);
            println!(
                "   Total Duration:    {:.1} minutes",
                result.total_duration.as_secs_f64() / 60.0
            );
            println!("   Total Operations:  {}", result.total_operations);
            println!(
                "   Peak Memory:       {:.1} MB",
                result.peak_memory_usage_mb
            );
            println!();

            // Phase-by-phase results
            for phase_result in result.phase_results.values() {
                println!("   ğŸ“ˆ {}", phase_result.phase_name);
                println!(
                    "      Duration:        {:.1} min",
                    phase_result.duration.as_secs_f64() / 60.0
                );
                println!(
                    "      Operations:      {}",
                    phase_result.operations_completed
                );
                println!(
                    "      Throughput:      {:.2} ops/sec",
                    phase_result.throughput_ops_per_sec
                );
                println!(
                    "      Avg Time:        {:.2} ms/op",
                    phase_result.average_op_time_ms
                );
                println!("      Success Rate:    {:.1}%", phase_result.success_rate);
                println!();
            }
        }

        // Performance comparison
        if self.results.len() > 1 {
            println!("ğŸ“Š Performance Comparison");
            println!("=========================");

            let mut sorted_results: Vec<_> = self.results.values().collect();
            sorted_results.sort_by(|a, b| {
                let a_throughput: f64 = a
                    .phase_results
                    .values()
                    .map(|r| r.throughput_ops_per_sec)
                    .sum();
                let b_throughput: f64 = b
                    .phase_results
                    .values()
                    .map(|r| r.throughput_ops_per_sec)
                    .sum();
                b_throughput
                    .partial_cmp(&a_throughput)
                    .unwrap_or(std::cmp::Ordering::Equal)
            });

            for (rank, result) in sorted_results.iter().enumerate() {
                let total_throughput: f64 = result
                    .phase_results
                    .values()
                    .map(|r| r.throughput_ops_per_sec)
                    .sum();
                let rank_emoji = if rank == 0 { "ğŸ¥‡" } else { "ğŸ¥ˆ" };

                println!(
                    "{} {} - {:.2} total ops/sec",
                    rank_emoji, result.device_name, total_throughput
                );

                if let Some(fastest) = sorted_results.first() {
                    if rank > 0 {
                        let fastest_throughput: f64 = fastest
                            .phase_results
                            .values()
                            .map(|r| r.throughput_ops_per_sec)
                            .sum();
                        let speedup = fastest_throughput / total_throughput;
                        println!("   Performance gap: {:.2}x slower than fastest", speedup);
                    }
                }
            }
        }

        println!("\nâœ… Heavy benchmark analysis completed!");
        println!(
            "ğŸ’¡ Consider thermal management and power efficiency in your deployment decisions."
        );
    }

    /// Helper: Simulate convolution operation
    /// ãƒ˜ãƒ«ãƒ‘ãƒ¼: ç•³ã¿è¾¼ã¿æ¼”ç®—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    fn simulate_convolution(
        &self,
        input: &Tensor<f32>,
        _kernel: &Tensor<f32>,
    ) -> RusTorchResult<Tensor<f32>> {
        // Simple element-wise operation as convolution placeholder
        // In real implementation, this would be proper convolution
        let result_data = input.data.mapv(|x| x * 0.5 + 0.1);
        Ok(Tensor::from_ndarray(result_data))
    }

    /// Helper: Estimate current memory usage
    /// ãƒ˜ãƒ«ãƒ‘ãƒ¼: ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š
    fn estimate_memory_usage(&self) -> f64 {
        // Simple estimation based on tensor sizes
        let matrix_memory = (self.config.matrix_size
            * self.config.matrix_size
            * 4
            * self.config.matrix_batch_size
            * 2) as f64
            / (1024.0 * 1024.0);
        let image_memory = (self.config.image_size
            * self.config.image_size
            * 4
            * self.config.image_batch_size
            * self.config.conv_channels.len()) as f64
            / (1024.0 * 1024.0);
        let attention_memory = (self.config.sequence_length * self.config.embedding_dim * 4 * 4)
            as f64
            / (1024.0 * 1024.0); // Q, K, V, O

        matrix_memory + image_memory + attention_memory + 100.0 // Base overhead
    }
}

fn main() -> RusTorchResult<()> {
    println!("ğŸš€ RusTorch Heavy Metal vs CoreML Benchmark");
    println!("============================================");
    println!("âš ï¸  WARNING: This is a computationally intensive benchmark!");
    println!("    Expected runtime: ~1 hour");
    println!("    Memory usage: 4-8GB+");
    println!("    May generate significant heat");
    println!();

    // Check if benchmark should be skipped
    if HeavyPerformanceBenchmark::should_skip_benchmark() {
        println!("â© Heavy benchmark skipped (CI environment or explicitly disabled)");
        println!("   To run this benchmark:");
        println!("   - Local: cd benchmarks && cargo run --bin metal_coreml_heavy_benchmark --features \"metal coreml\" --release");
        println!("   - CI: Set RUSTORCH_HEAVY_BENCHMARK=1 environment variable");
        return Ok(());
    }

    // Confirm user wants to proceed
    println!("ğŸ¤” Do you want to proceed with the heavy benchmark?");
    println!("   This will run for approximately 1 hour and use significant system resources.");
    println!("   Press Ctrl+C to cancel, or the benchmark will start in 10 seconds...");
    std::thread::sleep(Duration::from_secs(10));

    println!("ğŸš€ Starting heavy benchmark...");
    println!();

    // Display available features
    println!("ğŸ” Available Features:");
    #[cfg(feature = "metal")]
    println!("   âœ… Metal GPU acceleration");
    #[cfg(not(feature = "metal"))]
    println!("   âŒ Metal GPU acceleration");

    #[cfg(feature = "coreml")]
    println!("   âœ… CoreML Neural Engine");
    #[cfg(not(feature = "coreml"))]
    println!("   âŒ CoreML Neural Engine");

    #[cfg(feature = "mac-hybrid")]
    println!("   âœ… Mac-Hybrid (Intelligent Metal/CoreML selection)");
    #[cfg(not(feature = "mac-hybrid"))]
    println!("   âŒ Mac-Hybrid (Intelligent Metal/CoreML selection)");

    println!();

    let config = HeavyBenchmarkConfig::default();
    let mut benchmark = HeavyPerformanceBenchmark::new(config);

    // Priority-based execution: Mac-Hybrid takes precedence over individual backends
    #[cfg(feature = "mac-hybrid")]
    {
        // When mac-hybrid is enabled, run ONLY the hybrid benchmark
        match benchmark.run_heavy_mac_hybrid_benchmark() {
            Ok(_) => println!("âœ… Mac-Hybrid benchmark completed successfully"),
            Err(e) => println!("âŒ Mac-Hybrid benchmark failed: {}", e),
        }
        println!();
    }

    #[cfg(not(feature = "mac-hybrid"))]
    {
        // Run Metal benchmark if available and Mac-Hybrid is NOT enabled
        #[cfg(feature = "metal")]
        {
            match benchmark.run_heavy_metal_benchmark() {
                Ok(_) => println!("âœ… Metal benchmark completed successfully"),
                Err(e) => println!("âŒ Metal benchmark failed: {}", e),
            }
            println!();
        }

        // Run CoreML benchmark if available and Mac-Hybrid is NOT enabled
        #[cfg(feature = "coreml")]
        {
            match benchmark.run_heavy_coreml_benchmark() {
                Ok(_) => println!("âœ… CoreML benchmark completed successfully"),
                Err(e) => println!("âŒ CoreML benchmark failed: {}", e),
            }
            println!();
        }
    }

    // Display results
    benchmark.display_heavy_results();

    #[cfg(not(any(feature = "metal", feature = "coreml", feature = "mac-hybrid")))]
    {
        println!("âš ï¸  No GPU acceleration features enabled!");
        println!("   Recompile with --features \"metal coreml\" to enable hardware acceleration");
        println!("   Or use --features \"mac-hybrid\" for intelligent backend selection");
    }

    Ok(())
}
