{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# RusTorch 中文演示 🚀\n",
    "\n",
    "欢迎使用 RusTorch！此笔记本演示了我们生产就绪的 Rust 深度学习库的核心功能，具有类似 PyTorch 的 API。\n",
    "\n",
    "## 演示功能：\n",
    "- 🔥 **张量运算**：创建、操作和计算张量\n",
    "- 🧮 **矩阵运算**：优化性能的线性代数\n",
    "- 🧠 **神经网络层**：深度学习的构建块\n",
    "- ⚡ **性能**：Rust 驱动的速度和 GPU 加速\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 RusTorch 和其他必需的库\n",
    "import rustorch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"RusTorch 导入成功！\")\n",
    "print(f\"可用操作：{dir(rustorch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-tensors",
   "metadata": {},
   "source": [
    "## 1. 基本张量创建\n",
    "\n",
    "RusTorch 提供了多种创建张量的方法，类似于 PyTorch，但具有 Rust 的性能优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建不同类型的张量\n",
    "零张量 = rustorch.zeros([3, 4])\n",
    "一张量 = rustorch.ones([3, 4])\n",
    "随机张量 = rustorch.randn([3, 4])\n",
    "自定义张量 = rustorch.PyTensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])\n",
    "\n",
    "print(\"零张量：\")\n",
    "print(f\"  形状：{零张量.shape()}\")\n",
    "print(f\"  数据：{零张量.data()}\")\n",
    "\n",
    "print(\"\\n一张量：\")\n",
    "print(f\"  形状：{一张量.shape()}\")\n",
    "print(f\"  数据：{一张量.data()}\")\n",
    "\n",
    "print(\"\\n随机张量（正态分布）：\")\n",
    "print(f\"  形状：{随机张量.shape()}\")\n",
    "print(f\"  数据：{随机张量.data()}\")\n",
    "\n",
    "print(\"\\n自定义张量：\")\n",
    "print(f\"  形状：{自定义张量.shape()}\")\n",
    "print(f\"  数据：{自定义张量.data()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor-ops",
   "metadata": {},
   "source": [
    "## 2. 张量运算\n",
    "\n",
    "使用优化的 Rust 后端对张量执行数学运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本算术运算\n",
    "a = rustorch.PyTensor([1.0, 2.0, 3.0, 4.0], [2, 2])\n",
    "b = rustorch.PyTensor([5.0, 6.0, 7.0, 8.0], [2, 2])\n",
    "\n",
    "# 加法\n",
    "加法 = a.add(b)\n",
    "print(\"张量加法：\")\n",
    "print(f\"  A：{a.data()}\")\n",
    "print(f\"  B：{b.data()}\")\n",
    "print(f\"  A + B：{加法.data()}\")\n",
    "\n",
    "# 逐元素乘法\n",
    "乘法 = a.mul(b)\n",
    "print(\"\\n逐元素乘法：\")\n",
    "print(f\"  A * B：{乘法.data()}\")\n",
    "\n",
    "# 矩阵乘法\n",
    "矩阵乘法 = a.matmul(b)\n",
    "print(\"\\n矩阵乘法：\")\n",
    "print(f\"  A @ B：{矩阵乘法.data()}\")\n",
    "print(f\"  形状：{矩阵乘法.shape()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activations",
   "metadata": {},
   "source": [
    "## 3. 激活函数\n",
    "\n",
    "在 Rust 中高效实现的神经网络基本激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建具有各种值的输入张量\n",
    "输入值 = [-3.0, -1.5, 0.0, 1.5, 3.0]\n",
    "输入张量 = rustorch.PyTensor(输入值, [5])\n",
    "\n",
    "print(f\"输入值：{输入值}\")\n",
    "print()\n",
    "\n",
    "# 应用不同的激活函数\n",
    "relu输出 = 输入张量.relu()\n",
    "sigmoid输出 = 输入张量.sigmoid()\n",
    "tanh输出 = 输入张量.tanh()\n",
    "\n",
    "print(\"激活函数：\")\n",
    "print(f\"  ReLU：    {relu输出.data()}\")\n",
    "print(f\"  Sigmoid： {sigmoid输出.data()}\")\n",
    "print(f\"  Tanh：    {tanh输出.data()}\")\n",
    "\n",
    "# 演示数学特性\n",
    "print(\"\\n数学特性：\")\n",
    "print(f\"  ReLU 将负值截断为零\")\n",
    "print(f\"  Sigmoid 输出范围为 0 到 1\")\n",
    "print(f\"  Tanh 输出范围为 -1 到 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network",
   "metadata": {},
   "source": [
    "## 4. 简单神经网络示例\n",
    "\n",
    "使用 RusTorch 的张量运算构建基本神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-network-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的 2 层神经网络\n",
    "def 简单前向传播(输入数据, 权重1, 偏置1, 权重2, 偏置2):\n",
    "    \"\"\"\n",
    "    通过 2 层神经网络执行前向传播。\n",
    "    \"\"\"\n",
    "    # 第 1 层：线性变换 + ReLU 激活\n",
    "    第1层线性 = 输入数据.matmul(权重1).add(偏置1)\n",
    "    第1层输出 = 第1层线性.relu()\n",
    "    \n",
    "    # 第 2 层：线性变换 + Sigmoid 激活\n",
    "    第2层线性 = 第1层输出.matmul(权重2).add(偏置2)\n",
    "    输出 = 第2层线性.sigmoid()\n",
    "    \n",
    "    return 输出, 第1层输出\n",
    "\n",
    "# 初始化网络参数\n",
    "输入大小, 隐藏大小, 输出大小 = 3, 4, 2\n",
    "\n",
    "# 创建输入数据（批量大小=2，输入大小=3）\n",
    "输入数据 = rustorch.PyTensor([0.5, -0.2, 1.0, -1.0, 0.8, 0.3], [2, 3])\n",
    "\n",
    "# 用小随机值初始化权重和偏置\n",
    "权重1 = rustorch.randn([输入大小, 隐藏大小]).mul(rustorch.PyTensor([0.1], [1]))\n",
    "偏置1 = rustorch.zeros([1, 隐藏大小])\n",
    "权重2 = rustorch.randn([隐藏大小, 输出大小]).mul(rustorch.PyTensor([0.1], [1]))\n",
    "偏置2 = rustorch.zeros([1, 输出大小])\n",
    "\n",
    "# 前向传播\n",
    "输出, 隐藏 = 简单前向传播(输入数据, 权重1, 偏置1, 权重2, 偏置2)\n",
    "\n",
    "print(\"神经网络前向传播：\")\n",
    "print(f\"  输入形状：{输入数据.shape()}\")\n",
    "print(f\"  输入数据：{输入数据.data()}\")\n",
    "print(f\"  隐藏层形状：{隐藏.shape()}\")\n",
    "print(f\"  隐藏层输出：{隐藏.data()}\")\n",
    "print(f\"  最终输出形状：{输出.shape()}\")\n",
    "print(f\"  最终输出：{输出.data()}\")\n",
    "print(f\"  （由于 sigmoid 激活，输出值在 0-1 之间）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance",
   "metadata": {},
   "source": [
    "## 5. 性能比较\n",
    "\n",
    "比较 RusTorch 与 NumPy 在矩阵运算方面的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能基准测试：矩阵乘法\n",
    "大小 = [100, 500, 1000]\n",
    "\n",
    "print(\"性能比较：RusTorch vs NumPy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for 尺寸 in 大小:\n",
    "    print(f\"\\n矩阵大小：{尺寸}x{尺寸}\")\n",
    "    \n",
    "    # RusTorch 基准测试\n",
    "    开始时间 = time.time()\n",
    "    rust_a = rustorch.randn([尺寸, 尺寸])\n",
    "    rust_b = rustorch.randn([尺寸, 尺寸])\n",
    "    rust结果 = rust_a.matmul(rust_b)\n",
    "    rust时间 = time.time() - 开始时间\n",
    "    \n",
    "    # NumPy 基准测试\n",
    "    开始时间 = time.time()\n",
    "    numpy_a = np.random.randn(尺寸, 尺寸).astype(np.float32)\n",
    "    numpy_b = np.random.randn(尺寸, 尺寸).astype(np.float32)\n",
    "    numpy结果 = np.dot(numpy_a, numpy_b)\n",
    "    numpy时间 = time.time() - 开始时间\n",
    "    \n",
    "    # 计算加速比\n",
    "    加速比 = numpy时间 / rust时间 if rust时间 > 0 else float('inf')\n",
    "    \n",
    "    print(f\"  RusTorch：{rust时间:.4f}s\")\n",
    "    print(f\"  NumPy：   {numpy时间:.4f}s\")\n",
    "    print(f\"  加速比：   {加速比:.2f}x {'（RusTorch 更快）' if 加速比 > 1 else '（NumPy 更快）'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"注意：性能可能因系统配置和可用优化而异。\")\n",
    "print(\"启用 GPU 加速后，RusTorch 性能会显著提升。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 🎉 结论\n",
    "\n",
    "本演示展示了 RusTorch 的核心功能：\n",
    "\n",
    "✅ **张量创建和操作**：类似 PyTorch 的易用 API  \n",
    "✅ **数学运算**：优化的线性代数运算  \n",
    "✅ **神经网络构建块**：激活函数和层运算  \n",
    "✅ **性能**：Rust 驱动的速度和潜在的 GPU 加速  \n",
    "\n",
    "### 下一步：\n",
    "- 探索使用 CUDA/Metal/OpenCL 后端的 GPU 加速\n",
    "- 构建更复杂的神经网络架构\n",
    "- 尝试 transformer 模型和高级优化器\n",
    "- 了解基于浏览器机器学习的 WebGPU 支持\n",
    "\n",
    "### 资源：\n",
    "- 📚 [文档](https://docs.rs/rustorch)\n",
    "- 🚀 [GitHub 仓库](https://github.com/JunSuzukiJapan/rustorch)\n",
    "- 📓 [完整 Jupyter 设置指南](../../README_JUPYTER.md)\n",
    "\n",
    "使用 RusTorch 愉快编程！🦀⚡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}