{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¦€ DÃ©mo RusTorch avec Noyau Rust\n",
    "\n",
    "Ce notebook dÃ©montre comment utiliser RusTorch directement en Rust dans Jupyter !\n",
    "\n",
    "## FonctionnalitÃ©s :\n",
    "- ğŸ”¥ **Performance Rust Native** : Abstractions Ã  coÃ»t zÃ©ro\n",
    "- ğŸ§® **OpÃ©rations Tensorielles Directes** : Calculs matriciels type-safe\n",
    "- ğŸ§  **Construction de RÃ©seaux de Neurones** : Deep learning prÃªt pour la production\n",
    "- âš¡ **AccÃ©lÃ©ration GPU** : Support CUDA/Metal/OpenCL\n",
    "\n",
    "CommenÃ§ons !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Configuration des DÃ©pendances\n",
    "\n",
    "D'abord, ajoutons RusTorch et ndarray comme dÃ©pendances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ":dep rustorch = \"0.6.22\"\n:dep ndarray = \"0.16\"\n\n// Configuration pour evcxr\nextern crate rustorch;\nextern crate ndarray;"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Importation des BibliothÃ¨ques\n",
    "\n",
    "Importons RusTorch et ndarray avec la macro array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "use rustorch::prelude::*;\nuse std::time::Instant;\n\nprintln!(\"âœ… RusTorch importÃ© avec succÃ¨s !\");"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ OpÃ©rations Tensorielles de Base\n",
    "\n",
    "CrÃ©ons des tenseurs et effectuons des opÃ©rations de base :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// CrÃ©er des tenseurs Ã  partir de vecteurs\nlet a = Tensor::from_vec(vec![1.0, 2.0, 3.0, 4.0], vec![2, 2]);\nlet b = Tensor::from_vec(vec![5.0, 6.0, 7.0, 8.0], vec![2, 2]);\n\nprintln!(\"Tenseur a: {:?}\", a);\nprintln!(\"Tenseur b: {:?}\", b);\nprintln!(\"Forme de a: {:?}\", a.shape());\nprintln!(\"Forme de b: {:?}\", b.shape());"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// Multiplication matricielle\nlet matmul_result = a.matmul(&b).expect(\"Matrix multiplication failed\");\nprintln!(\"Multiplication matricielle a @ b : {:?}\", matmul_result);\n\n// OpÃ©rations Ã©lÃ©ment par Ã©lÃ©ment\nlet sum = &a + &b;\nprintln!(\"Somme Ã©lÃ©ment par Ã©lÃ©ment a + b : {:?}\", sum);\n\nlet product = &a * &b;\nprintln!(\"Produit Ã©lÃ©ment par Ã©lÃ©ment a * b : {:?}\", product);"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ§® OpÃ©rations AvancÃ©es"
  },
  {
   "cell_type": "code",
   "source": "// CrÃ©er des tenseurs spÃ©ciaux (avec annotations de type explicites)\nlet zeros: Tensor<f32> = Tensor::zeros(&[3, 3]);\nlet ones: Tensor<f32> = Tensor::ones(&[3, 3]);\nlet random: Tensor<f32> = Tensor::randn(&[3, 3]);\n\nprintln!(\"Tenseur zÃ©ros: {:?}\", zeros);\nprintln!(\"Tenseur uns: {:?}\", ones);\nprintln!(\"Tenseur alÃ©atoire: {:?}\", random);\n\n// Application des fonctions d'activation\n// Note : Utilisation des fonctions d'activation du module nn\nprintln!(\"OpÃ©rations tensorielles terminÃ©es avec succÃ¨s !\");",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "use rustorch::nn::*;\n\n// CrÃ©er des couches de rÃ©seau neuronal de base (avec annotations de type explicites)\nlet linear1: Linear<f32> = Linear::new(784, 128);\nlet linear2: Linear<f32> = Linear::new(128, 10);\n\nprintln!(\"Couches de rÃ©seau neuronal crÃ©Ã©es\");\nprintln!(\"Couche d'entrÃ©e: 784 â†’ Couche cachÃ©e: 128 â†’ Couche de sortie: 10\");\n\n// CrÃ©er une entrÃ©e d'exemple\nlet input: Tensor<f32> = Tensor::randn(&[1, 784]); // Taille de lot 1, 784 caractÃ©ristiques\n\n// DÃ©montrer la crÃ©ation de couches (la passe avant nÃ©cessite une configuration plus complexe)\nprintln!(\"Forme de l'entrÃ©e: {:?}\", input.shape());\nprintln!(\"Couche linÃ©aire 1: 784 â†’ 128 neurones\");\nprintln!(\"Couche linÃ©aire 2: 128 â†’ 10 classes de sortie\");\nprintln!(\"Configuration des couches de rÃ©seau neuronal terminÃ©e!\");",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "// Benchmark de multiplication matricielle\nlet size = 500;\nlet a: Tensor<f32> = Tensor::randn(&[size, size]);\nlet b: Tensor<f32> = Tensor::randn(&[size, size]);\n\nprintln!(\"ğŸ Benchmark multiplication matricielle {}x{}...\", size, size);\n\nlet start = Instant::now();\nlet result = a.matmul(&b).expect(\"Matrix multiplication failed\");\nlet duration = start.elapsed();\n\nprintln!(\"âœ… TerminÃ© en : {:?}\", duration);\nprintln!(\"ğŸ“Š Forme du rÃ©sultat : {:?}\", result.shape());\nprintln!(\"ğŸ“ˆ DÃ©bit : {:.2} GFLOPS\", \n    (2.0 * size as f64 * size as f64 * size as f64) / (duration.as_secs_f64() * 1e9));",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// CrÃ©er un tenseur avec des valeurs positives/nÃ©gatives mÃ©langÃ©es\nlet input = Tensor::from_vec(vec![-2.0, -1.0, 0.0, 1.0, 2.0], vec![5]);\nprintln!(\"EntrÃ©e : {:?}\", input);\n\n// Note : Les fonctions d'activation sont disponibles via le module nn::activation\nprintln!(\"Les opÃ©rations tensorielles RusTorch fonctionnent correctement !\");"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Benchmark de Performance\n",
    "\n",
    "Comparons les performances de diffÃ©rentes opÃ©rations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// Benchmark de multiplication matricielle\nlet size = 256;\nlet a: Tensor<f32> = Tensor::randn(&[size, size]);\nlet b: Tensor<f32> = Tensor::randn(&[size, size]);\n\nprintln!(\"ğŸ Benchmark multiplication matricielle {}x{}...\", size, size);\n\nlet start = Instant::now();\nlet result = a.matmul(&b);\nlet duration = start.elapsed();\n\nprintln!(\"âœ… TerminÃ© en : {:?}\", duration);\nprintln!(\"ğŸ“Š Forme du rÃ©sultat : {:?}\", result.shape());\nprintln!(\"ğŸ“ˆ DÃ©bit : {:.2} GFLOPS\", \n    (2.0 * size as f64 * size as f64 * size as f64) / (duration.as_secs_f64() * 1e9));"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion\n",
    "\n",
    "Vous pouvez maintenant Ã©crire et exÃ©cuter du code Rust directement dans Jupyter !\n",
    "\n",
    "**Avantages :**\n",
    "- ğŸš€ Performance Rust native\n",
    "- ğŸ”§ AccÃ¨s direct aux bibliothÃ¨ques\n",
    "- ğŸ¯ SÃ©curitÃ© de type\n",
    "- âš¡ Abstractions Ã  coÃ»t zÃ©ro\n",
    "- ğŸ–¥ï¸ Support d'accÃ©lÃ©ration GPU\n",
    "\n",
    "**Prochaines Ã‰tapes :**\n",
    "- Explorer l'accÃ©lÃ©ration GPU avec les backends CUDA/Metal/OpenCL\n",
    "- Construire des architectures de rÃ©seaux de neurones plus complexes\n",
    "- Essayer les modÃ¨les transformer et optimiseurs avancÃ©s\n",
    "\n",
    "Bon codage avec RusTorch ! ğŸ¦€âš¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}