{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶Ä D√©mo RusTorch avec Noyau Rust\n",
    "\n",
    "Ce notebook d√©montre comment utiliser RusTorch directement en Rust dans Jupyter !\n",
    "\n",
    "## Fonctionnalit√©s :\n",
    "- üî• **Performance Rust Native** : Abstractions √† co√ªt z√©ro\n",
    "- üßÆ **Op√©rations Tensorielles Directes** : Calculs matriciels type-safe\n",
    "- üß† **Construction de R√©seaux de Neurones** : Deep learning pr√™t pour la production\n",
    "- ‚ö° **Acc√©l√©ration GPU** : Support CUDA/Metal/OpenCL\n",
    "\n",
    "Commen√ßons !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Configuration des D√©pendances\n",
    "\n",
    "D'abord, ajoutons RusTorch et ndarray comme d√©pendances :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep rustorch = \"0.5.11\"\n",
    ":dep ndarray = \"0.16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Importation des Biblioth√®ques\n",
    "\n",
    "Importons RusTorch et ndarray avec la macro array :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use rustorch::*;\n",
    "use ndarray::prelude::*;\n",
    "use ndarray::array;\n",
    "use std::time::Instant;\n",
    "\n",
    "println!(\"‚úÖ RusTorch et ndarray import√©s avec succ√®s !\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Op√©rations Tensorielles de Base\n",
    "\n",
    "Cr√©ons des tenseurs et effectuons des op√©rations de base :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Cr√©er des tenseurs avec la macro array!\n",
    "let a = Tensor::from_array(array![[1.0, 2.0], [3.0, 4.0]]);\n",
    "let b = Tensor::from_array(array![[5.0, 6.0], [7.0, 8.0]]);\n",
    "\n",
    "println!(\"Tenseur a : {:?}\", a);\n",
    "println!(\"Tenseur b : {:?}\", b);\n",
    "println!(\"Forme de a : {:?}\", a.shape());\n",
    "println!(\"Forme de b : {:?}\", b.shape());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Multiplication matricielle\n",
    "let matmul_result = a.matmul(&b);\n",
    "println!(\"Multiplication matricielle a @ b : {:?}\", matmul_result);\n",
    "\n",
    "// Op√©rations √©l√©ment par √©l√©ment\n",
    "let sum = &a + &b;\n",
    "println!(\"Somme √©l√©ment par √©l√©ment a + b : {:?}\", sum);\n",
    "\n",
    "let product = &a * &b;\n",
    "println!(\"Produit √©l√©ment par √©l√©ment a * b : {:?}\", product);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "// Cr√©er des tenseurs sp√©ciaux (avec annotations de type explicites)\nlet zeros: Tensor<f32> = Tensor::zeros(&[3, 3]);\nlet ones: Tensor<f32> = Tensor::ones(&[3, 3]);\nlet random: Tensor<f32> = Tensor::randn(&[3, 3]);\n\nprintln!(\"Tenseur z√©ros: {:?}\", zeros);\nprintln!(\"Tenseur uns: {:?}\", ones);\nprintln!(\"Tenseur al√©atoire: {:?}\", random);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Cr√©er un tenseur avec des valeurs positives/n√©gatives m√©lang√©es\n",
    "let input = Tensor::from_array(array![[-2.0, -1.0, 0.0, 1.0, 2.0]]);\n",
    "println!(\"Entr√©e : {:?}\", input);\n",
    "\n",
    "// Appliquer les fonctions d'activation\n",
    "let relu_result = input.relu();\n",
    "let sigmoid_result = input.sigmoid();\n",
    "let tanh_result = input.tanh();\n",
    "\n",
    "println!(\"ReLU : {:?}\", relu_result);\n",
    "println!(\"Sigmoid : {:?}\", sigmoid_result);\n",
    "println!(\"Tanh : {:?}\", tanh_result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Benchmark de Performance\n",
    "\n",
    "Comparons les performances de diff√©rentes op√©rations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "// Benchmark de multiplication matricielle\nlet size = 256;\nlet a: Tensor<f32> = Tensor::randn(&[size, size]);\nlet b: Tensor<f32> = Tensor::randn(&[size, size]);\n\nprintln!(\"üèÅ Benchmark multiplication matricielle {}x{}...\", size, size);\n\nlet start = Instant::now();\nlet result = a.matmul(&b);\nlet duration = start.elapsed();\n\nprintln!(\"‚úÖ Termin√© en : {:?}\", duration);\nprintln!(\"üìä Forme du r√©sultat : {:?}\", result.shape());\nprintln!(\"üìà D√©bit : {:.2} GFLOPS\", \n    (2.0 * size as f64 * size as f64 * size as f64) / (duration.as_secs_f64() * 1e9));"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "Vous pouvez maintenant √©crire et ex√©cuter du code Rust directement dans Jupyter !\n",
    "\n",
    "**Avantages :**\n",
    "- üöÄ Performance Rust native\n",
    "- üîß Acc√®s direct aux biblioth√®ques\n",
    "- üéØ S√©curit√© de type\n",
    "- ‚ö° Abstractions √† co√ªt z√©ro\n",
    "- üñ•Ô∏è Support d'acc√©l√©ration GPU\n",
    "\n",
    "**Prochaines √âtapes :**\n",
    "- Explorer l'acc√©l√©ration GPU avec les backends CUDA/Metal/OpenCL\n",
    "- Construire des architectures de r√©seaux de neurones plus complexes\n",
    "- Essayer les mod√®les transformer et optimiseurs avanc√©s\n",
    "\n",
    "Bon codage avec RusTorch ! ü¶Ä‚ö°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}