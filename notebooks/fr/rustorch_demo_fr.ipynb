{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# D√©monstration RusTorch en Fran√ßais üöÄ\n",
    "\n",
    "Bienvenue dans RusTorch ! Ce notebook d√©montre les capacit√©s principales de notre biblioth√®que d'apprentissage profond pr√™te pour la production en Rust avec une API similaire √† PyTorch.\n",
    "\n",
    "## Fonctionnalit√©s D√©montr√©es :\n",
    "- üî• **Op√©rations Tensorielles**: Cr√©er, manipuler et calculer avec des tenseurs\n",
    "- üßÆ **Op√©rations Matricielles**: Alg√®bre lin√©aire avec performances optimis√©es\n",
    "- üß† **Couches de R√©seaux de Neurones**: √âl√©ments constitutifs pour l'apprentissage profond\n",
    "- ‚ö° **Performance**: Vitesse aliment√©e par Rust avec acc√©l√©ration GPU\n",
    "\n",
    "Commen√ßons !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer RusTorch et autres biblioth√®ques requises\n",
    "import rustorch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"RusTorch import√© avec succ√®s !\")\n",
    "print(f\"Op√©rations disponibles : {dir(rustorch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-tensors",
   "metadata": {},
   "source": [
    "## 1. Cr√©ation de Tenseurs de Base\n",
    "\n",
    "RusTorch fournit plusieurs moyens de cr√©er des tenseurs, similaire √† PyTorch mais avec les avantages de performance de Rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er diff√©rents types de tenseurs\n",
    "tenseur_zeros = rustorch.zeros([3, 4])\n",
    "tenseur_ones = rustorch.ones([3, 4])\n",
    "tenseur_aleatoire = rustorch.randn([3, 4])\n",
    "tenseur_personnalise = rustorch.PyTensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])\n",
    "\n",
    "print(\"Tenseur de z√©ros :\")\n",
    "print(f\"  Forme : {tenseur_zeros.shape()}\")\n",
    "print(f\"  Donn√©es : {tenseur_zeros.data()}\")\n",
    "\n",
    "print(\"\\nTenseur de uns :\")\n",
    "print(f\"  Forme : {tenseur_ones.shape()}\")\n",
    "print(f\"  Donn√©es : {tenseur_ones.data()}\")\n",
    "\n",
    "print(\"\\nTenseur al√©atoire (distribution normale) :\")\n",
    "print(f\"  Forme : {tenseur_aleatoire.shape()}\")\n",
    "print(f\"  Donn√©es : {tenseur_aleatoire.data()}\")\n",
    "\n",
    "print(\"\\nTenseur personnalis√© :\")\n",
    "print(f\"  Forme : {tenseur_personnalise.shape()}\")\n",
    "print(f\"  Donn√©es : {tenseur_personnalise.data()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor-ops",
   "metadata": {},
   "source": [
    "## 2. Op√©rations sur les Tenseurs\n",
    "\n",
    "Effectuer des op√©rations math√©matiques sur les tenseurs avec un backend Rust optimis√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op√©rations arithm√©tiques de base\n",
    "a = rustorch.PyTensor([1.0, 2.0, 3.0, 4.0], [2, 2])\n",
    "b = rustorch.PyTensor([5.0, 6.0, 7.0, 8.0], [2, 2])\n",
    "\n",
    "# Addition\n",
    "addition = a.add(b)\n",
    "print(\"Addition de Tenseurs :\")\n",
    "print(f\"  A : {a.data()}\")\n",
    "print(f\"  B : {b.data()}\")\n",
    "print(f\"  A + B : {addition.data()}\")\n",
    "\n",
    "# Multiplication √©l√©ment par √©l√©ment\n",
    "multiplication = a.mul(b)\n",
    "print(\"\\nMultiplication √âl√©ment par √âl√©ment :\")\n",
    "print(f\"  A * B : {multiplication.data()}\")\n",
    "\n",
    "# Multiplication matricielle\n",
    "matmul = a.matmul(b)\n",
    "print(\"\\nMultiplication Matricielle :\")\n",
    "print(f\"  A @ B : {matmul.data()}\")\n",
    "print(f\"  Forme : {matmul.shape()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activations",
   "metadata": {},
   "source": [
    "## 3. Fonctions d'Activation\n",
    "\n",
    "Fonctions d'activation essentielles pour r√©seaux de neurones impl√©ment√©es efficacement en Rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tenseur d'entr√©e avec diverses valeurs\n",
    "valeurs_entree = [-3.0, -1.5, 0.0, 1.5, 3.0]\n",
    "tenseur_entree = rustorch.PyTensor(valeurs_entree, [5])\n",
    "\n",
    "print(f\"Valeurs d'entr√©e : {valeurs_entree}\")\n",
    "print()\n",
    "\n",
    "# Appliquer diff√©rentes fonctions d'activation\n",
    "sortie_relu = tenseur_entree.relu()\n",
    "sortie_sigmoid = tenseur_entree.sigmoid()\n",
    "sortie_tanh = tenseur_entree.tanh()\n",
    "\n",
    "print(\"Fonctions d'Activation :\")\n",
    "print(f\"  ReLU :    {sortie_relu.data()}\")\n",
    "print(f\"  Sigmoid : {sortie_sigmoid.data()}\")\n",
    "print(f\"  Tanh :    {sortie_tanh.data()}\")\n",
    "\n",
    "# D√©montrer les propri√©t√©s math√©matiques\n",
    "print(\"\\nPropri√©t√©s Math√©matiques :\")\n",
    "print(f\"  ReLU bride les valeurs n√©gatives √† z√©ro\")\n",
    "print(f\"  Sigmoid a une sortie dans la plage 0 √† 1\")\n",
    "print(f\"  Tanh a une sortie dans la plage -1 √† 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-network",
   "metadata": {},
   "source": [
    "## 4. Exemple de R√©seau de Neurones Simple\n",
    "\n",
    "Construire un r√©seau de neurones de base en utilisant les op√©rations tensorielles de RusTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-network-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir un r√©seau de neurones simple √† 2 couches\n",
    "def passage_avant_simple(donnees_entree, poids1, biais1, poids2, biais2):\n",
    "    \"\"\"\n",
    "    Effectuer un passage avant √† travers un r√©seau de neurones √† 2 couches.\n",
    "    \"\"\"\n",
    "    # Couche 1 : Transformation lin√©aire + activation ReLU\n",
    "    couche1_lineaire = donnees_entree.matmul(poids1).add(biais1)\n",
    "    sortie_couche1 = couche1_lineaire.relu()\n",
    "    \n",
    "    # Couche 2 : Transformation lin√©aire + activation Sigmoid\n",
    "    couche2_lineaire = sortie_couche1.matmul(poids2).add(biais2)\n",
    "    sortie = couche2_lineaire.sigmoid()\n",
    "    \n",
    "    return sortie, sortie_couche1\n",
    "\n",
    "# Initialiser les param√®tres du r√©seau\n",
    "taille_entree, taille_cachee, taille_sortie = 3, 4, 2\n",
    "\n",
    "# Cr√©er des donn√©es d'entr√©e (taille_lot=2, taille_entree=3)\n",
    "donnees_entree = rustorch.PyTensor([0.5, -0.2, 1.0, -1.0, 0.8, 0.3], [2, 3])\n",
    "\n",
    "# Initialiser poids et biais avec de petites valeurs al√©atoires\n",
    "poids1 = rustorch.randn([taille_entree, taille_cachee]).mul(rustorch.PyTensor([0.1], [1]))\n",
    "biais1 = rustorch.zeros([1, taille_cachee])\n",
    "poids2 = rustorch.randn([taille_cachee, taille_sortie]).mul(rustorch.PyTensor([0.1], [1]))\n",
    "biais2 = rustorch.zeros([1, taille_sortie])\n",
    "\n",
    "# Passage avant\n",
    "sortie, cachee = passage_avant_simple(donnees_entree, poids1, biais1, poids2, biais2)\n",
    "\n",
    "print(\"Passage Avant du R√©seau de Neurones :\")\n",
    "print(f\"  Forme d'entr√©e : {donnees_entree.shape()}\")\n",
    "print(f\"  Donn√©es d'entr√©e : {donnees_entree.data()}\")\n",
    "print(f\"  Forme couche cach√©e : {cachee.shape()}\")\n",
    "print(f\"  Sortie couche cach√©e : {cachee.data()}\")\n",
    "print(f\"  Forme sortie finale : {sortie.shape()}\")\n",
    "print(f\"  Sortie finale : {sortie.data()}\")\n",
    "print(f\"  (Valeurs de sortie entre 0-1 gr√¢ce √† l'activation sigmoid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance",
   "metadata": {},
   "source": [
    "## 5. Comparaison de Performance\n",
    "\n",
    "Comparer les performances de RusTorch avec NumPy pour les op√©rations matricielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark de performance : Multiplication matricielle\n",
    "tailles = [100, 500, 1000]\n",
    "\n",
    "print(\"Comparaison de Performance : RusTorch vs NumPy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for taille in tailles:\n",
    "    print(f\"\\nTaille de matrice : {taille}x{taille}\")\n",
    "    \n",
    "    # Benchmark RusTorch\n",
    "    temps_debut = time.time()\n",
    "    rust_a = rustorch.randn([taille, taille])\n",
    "    rust_b = rustorch.randn([taille, taille])\n",
    "    resultat_rust = rust_a.matmul(rust_b)\n",
    "    temps_rust = time.time() - temps_debut\n",
    "    \n",
    "    # Benchmark NumPy\n",
    "    temps_debut = time.time()\n",
    "    numpy_a = np.random.randn(taille, taille).astype(np.float32)\n",
    "    numpy_b = np.random.randn(taille, taille).astype(np.float32)\n",
    "    resultat_numpy = np.dot(numpy_a, numpy_b)\n",
    "    temps_numpy = time.time() - temps_debut\n",
    "    \n",
    "    # Calculer l'acc√©l√©ration\n",
    "    acceleration = temps_numpy / temps_rust if temps_rust > 0 else float('inf')\n",
    "    \n",
    "    print(f\"  RusTorch : {temps_rust:.4f}s\")\n",
    "    print(f\"  NumPy :    {temps_numpy:.4f}s\")\n",
    "    print(f\"  Acc√©l√©ration : {acceleration:.2f}x {'(RusTorch plus rapide)' if acceleration > 1 else '(NumPy plus rapide)'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Note : Les performances peuvent varier selon la configuration syst√®me et les optimisations disponibles.\")\n",
    "print(\"Les performances de RusTorch s'am√©liorent consid√©rablement avec l'acc√©l√©ration GPU activ√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "Cette d√©monstration a pr√©sent√© les capacit√©s principales de RusTorch :\n",
    "\n",
    "‚úÖ **Cr√©ation et Manipulation de Tenseurs** : API facile √† utiliser similaire √† PyTorch  \n",
    "‚úÖ **Op√©rations Math√©matiques** : Op√©rations d'alg√®bre lin√©aire optimis√©es  \n",
    "‚úÖ **√âl√©ments Constitutifs de R√©seaux de Neurones** : Fonctions d'activation et op√©rations de couches  \n",
    "‚úÖ **Performance** : Vitesse aliment√©e par Rust avec acc√©l√©ration GPU potentielle  \n",
    "\n",
    "### √âtapes Suivantes :\n",
    "- Explorer l'acc√©l√©ration GPU avec les backends CUDA/Metal/OpenCL\n",
    "- Construire des architectures de r√©seaux de neurones plus complexes\n",
    "- Essayer les mod√®les transformer et optimiseurs avanc√©s\n",
    "- D√©couvrir le support WebGPU pour le ML bas√© sur navigateur\n",
    "\n",
    "### Ressources :\n",
    "- üìö [Documentation](https://docs.rs/rustorch)\n",
    "- üöÄ [D√©p√¥t GitHub](https://github.com/JunSuzukiJapan/rustorch)\n",
    "- üìì [Guide Complet de Configuration Jupyter](../../README_JUPYTER.md)\n",
    "\n",
    "Bon codage avec RusTorch ! ü¶Ä‚ö°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}