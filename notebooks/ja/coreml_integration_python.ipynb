{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RusTorch CoreML Integration - Python Bindings\n",
    "\n",
    "このノートブックは、PythonバインディングでRusTorchのCoreML機能を使用する方法を示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RusTorchのPythonバインディングをインポート\n",
    "try:\n",
    "    import rustorch\n",
    "    print(f\"✅ RusTorch version: {rustorch.__version__}\")\n",
    "    print(f\"📝 Description: {rustorch.__description__}\")\n",
    "    print(f\"👥 Author: {rustorch.__author__}\")\nexcept ImportError as e:\n",
    "    print(f\"❌ RusTorchのインポートに失敗: {e}\")\n",
    "    print(\"maturin develop でビルドしてください\")\n",
    "    exit(1)\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "print(f\"🖥️ プラットフォーム: {platform.system()} {platform.release()}\")\n",
    "print(f\"🐍 Python version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreMLの可用性をチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoreML機能の確認\n",
    "try:\n",
    "    # CoreMLが利用可能かチェック\n",
    "    coreml_available = rustorch.is_coreml_available()\n",
    "    print(f\"🍎 CoreML available: {coreml_available}\")\n",
    "    \n",
    "    if coreml_available:\n",
    "        print(\"🎉 CoreMLが利用可能です！\")\n",
    "        \n",
    "        # デバイス情報を取得\n",
    "        device_info = rustorch.get_coreml_device_info()\n",
    "        print(\"📱 CoreMLデバイス情報:\")\n",
    "        print(device_info)\n",
    "    else:\n",
    "        print(\"⚠️ CoreMLは利用できません\")\n",
    "        if platform.system() != \"Darwin\":\n",
    "            print(\"CoreMLはmacOSでのみ利用可能です\")\n",
    "        else:\n",
    "            print(\"CoreMLフィーチャーが有効になっていない可能性があります\")\n",
    "            \nexcept AttributeError:\n",
    "    print(\"❌ CoreML関数が見つかりません\")\n",
    "    print(\"CoreMLフィーチャーでビルドされていない可能性があります\")\n",
    "    coreml_available = False\nexcept Exception as e:\n",
    "    print(f\"❌ CoreMLチェック中にエラー: {e}\")\n",
    "    coreml_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreMLデバイスの作成と操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # CoreMLデバイスを作成\n",
    "        device = rustorch.CoreMLDevice(device_id=0)\n",
    "        print(f\"🖥️ CoreMLデバイス作成: {device}\")\n",
    "        \n",
    "        # デバイス情報を取得\n",
    "        print(f\"🆔 Device ID: {device.device_id()}\")\n",
    "        print(f\"✅ Available: {device.is_available()}\")\n",
    "        print(f\"💾 Memory limit: {device.memory_limit()} bytes\")\n",
    "        print(f\"🧮 Compute units limit: {device.compute_units_limit()}\")\n",
    "        print(f\"📚 Model cache size: {device.model_cache_size()}\")\n",
    "        \n",
    "        # キャッシュのクリーンアップ\n",
    "        device.cleanup_cache()\n",
    "        print(\"🧹 Cache cleaned up\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CoreMLデバイス操作エラー: {e}\")\nelse:\n",
    "    print(\"⚠️ CoreMLが利用できないため、デバイス操作をスキップ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreMLバックエンドの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # CoreMLバックエンド設定を作成\n",
    "        config = rustorch.CoreMLBackendConfig(\n",
    "            enable_caching=True,\n",
    "            max_cache_size=200,\n",
    "            enable_profiling=True,\n",
    "            auto_fallback=True\n",
    "        )\n",
    "        print(f\"⚙️ Backend config: {config}\")\n",
    "        \n",
    "        # 設定値を確認・変更\n",
    "        print(f\"📊 Enable caching: {config.enable_caching}\")\n",
    "        print(f\"🗂️ Max cache size: {config.max_cache_size}\")\n",
    "        print(f\"📈 Enable profiling: {config.enable_profiling}\")\n",
    "        print(f\"🔄 Auto fallback: {config.auto_fallback}\")\n",
    "        \n",
    "        # 設定を変更\n",
    "        config.enable_profiling = False\n",
    "        config.max_cache_size = 150\n",
    "        print(f\"\\n🔧 Updated config: {config}\")\n",
    "        \n",
    "        # CoreMLバックエンドを作成\n",
    "        backend = rustorch.CoreMLBackend(config)\n",
    "        print(f\"🚀 CoreML backend: {backend}\")\n",
    "        print(f\"✅ Backend available: {backend.is_available()}\")\n",
    "        \n",
    "        # バックエンド統計を取得\n",
    "        stats = backend.get_stats()\n",
    "        print(f\"📊 Backend stats: {stats}\")\n",
    "        print(f\"   Total operations: {stats.total_operations}\")\n",
    "        print(f\"   Cache hits: {stats.cache_hits}\")\n",
    "        print(f\"   Cache misses: {stats.cache_misses}\")\n",
    "        print(f\"   Fallback operations: {stats.fallback_operations}\")\n",
    "        print(f\"   Cache hit rate: {stats.cache_hit_rate():.2%}\")\n",
    "        print(f\"   Fallback rate: {stats.fallback_rate():.2%}\")\n",
    "        print(f\"   Avg execution time: {stats.average_execution_time_ms:.2f}ms\")\n",
    "        \n",
    "        # キャッシュクリーンアップ\n",
    "        backend.cleanup_cache()\n",
    "        print(\"\\n🧹 Backend cache cleaned\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CoreMLバックエンド操作エラー: {e}\")\nelse:\n",
    "    print(\"⚠️ CoreMLが利用できないため、バックエンド操作をスキップ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的なテンソル操作（CPU）\n",
    "\n",
    "CoreMLとの比較のために、まずCPUでの基本操作を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 基本的なテンソル作成と操作\n",
    "    print(\"🧮 基本的なテンソル操作（CPU）\")\n",
    "    \n",
    "    # NumPy配列からテンソルを作成（簡略化されたインターフェース）\n",
    "    data_a = np.random.randn(2, 3).astype(np.float32)\n",
    "    data_b = np.random.randn(3, 2).astype(np.float32)\n",
    "    \n",
    "    print(f\"📐 Matrix A shape: {data_a.shape}\")\n",
    "    print(f\"📐 Matrix B shape: {data_b.shape}\")\n",
    "    \n",
    "    # NumPyで行列乗算（比較用）\n",
    "    numpy_result = np.matmul(data_a, data_b)\n",
    "    print(f\"✅ NumPy matmul result shape: {numpy_result.shape}\")\n",
    "    print(f\"📊 Result (first few elements): {numpy_result.flatten()[:4]}\")\n",
    "    \n",
    "    print(\"\\n🚀 CPU演算完了\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"❌ テンソル操作エラー: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パフォーマンス比較のシミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_matrix_operations():\n",
    "    \"\"\"異なるサイズの行列でパフォーマンスを比較\"\"\"\n",
    "    \n",
    "    sizes = [(64, 64), (128, 128), (256, 256), (512, 512)]\n",
    "    \n",
    "    print(\"🏁 パフォーマンス比較:\")\n",
    "    print(\"Size\\t\\tCPU Time (ms)\\tExpected CoreML (ms)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # CPU実行時間を測定\n",
    "        a = np.random.randn(*size).astype(np.float32)\n",
    "        b = np.random.randn(size[1], size[0]).astype(np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = np.matmul(a, b)\n",
    "        cpu_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # CoreMLの予想時間（仮定的）\n",
    "        # 実際の実装では、CoreMLバックエンドからの実測値を使用\n",
    "        expected_coreml_time = cpu_time * 0.6  # 仮定: CoreMLは40%高速\n",
    "        \n",
    "        print(f\"{size[0]}x{size[1]}\\t\\t{cpu_time:.2f}\\t\\t{expected_coreml_time:.2f}\")\n",
    "\n",
    "benchmark_matrix_operations()\n",
    "\n",
    "print(\"\\n📝 注意: CoreML時間は仮定値です。実際の値は具体的な実装と依存します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デバイス選択のシミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_device_selection():\n",
    "    \"\"\"スマートデバイス選択をシミュレート\"\"\"\n",
    "    \n",
    "    operations = [\n",
    "        (\"小さい行列乗算\", (16, 16), \"CPU\"),\n",
    "        (\"中程度行列乗算\", (128, 128), \"Metal GPU\"),\n",
    "        (\"大きい行列乗算\", (512, 512), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"活性化関数\", (32, 64, 128, 128), \"Metal GPU\"),\n",
    "        (\"畳み込み（小）\", (1, 3, 32, 32), \"CPU\"),\n",
    "        (\"畳み込み（大）\", (16, 64, 224, 224), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"複素数演算\", (128, 128), \"Metal GPU\"),  # CoreML非対応\n",
    "        (\"統計的分布\", (1000,), \"CPU\"),  # CoreML非対応\n",
    "    ]\n",
    "    \n",
    "    print(\"🎯 スマートデバイス選択シミュレーション:\")\n",
    "    print(\"Operation\\t\\t\\tTensor Shape\\t\\tSelected Device\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, shape, device in operations:\n",
    "        shape_str = \"x\".join(map(str, shape))\n",
    "        print(f\"{name:<23}\\t{shape_str:<15}\\t{device}\")\n",
    "    \n",
    "    print(\"\\n📝 選択ロジック:\")\n",
    "    print(\"  • 小さい操作: CPU（オーバーヘッド回避）\")\n",
    "    print(\"  • 中程度操作: Metal GPU（バランス）\")\n",
    "    print(\"  • 大きい操作: CoreML（最適化済み）\")\n",
    "    print(\"  • 非対応操作: GPU/CPUフォールバック\")\n",
    "\n",
    "simulate_device_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実践的な使用例: 簡単なニューラルネットワーク層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_neural_network_layer():\n",
    "    \"\"\"ニューラルネットワーク層のシミュレーション\"\"\"\n",
    "    \n",
    "    print(\"🧠 ニューラルネットワーク層シミュレーション:\")\n",
    "    \n",
    "    # バッチサイズとレイヤー設定\n",
    "    batch_size = 32\n",
    "    input_features = 784  # 28x28 MNIST\n",
    "    hidden_features = 256\n",
    "    output_features = 10  # 10クラス\n",
    "    \n",
    "    print(f\"📊 Batch size: {batch_size}\")\n",
    "    print(f\"🔢 Input features: {input_features}\")\n",
    "    print(f\"🧮 Hidden features: {hidden_features}\")\n",
    "    print(f\"🎯 Output features: {output_features}\")\n",
    "    \n",
    "    # 前向き伝播のシミュレーション\n",
    "    steps = [\n",
    "        (\"Input → Hidden\", f\"({batch_size}, {input_features}) @ ({input_features}, {hidden_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"ReLU Activation\", f\"({batch_size}, {hidden_features})\", \"Metal\"),\n",
    "        (\"Hidden → Output\", f\"({batch_size}, {hidden_features}) @ ({hidden_features}, {output_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"Softmax\", f\"({batch_size}, {output_features})\", \"CPU\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🔄 Forward pass simulation:\")\n",
    "    total_time = 0\n",
    "    \n",
    "    for step, shape, device in steps:\n",
    "        # 仮想的な実行時間（ms）\n",
    "        if device == \"CoreML\":\n",
    "            time_ms = np.random.uniform(0.5, 2.0)\n",
    "        elif device == \"Metal\":\n",
    "            time_ms = np.random.uniform(1.0, 3.0)\n",
    "        else:  # CPU\n",
    "            time_ms = np.random.uniform(0.2, 1.0)\n",
    "        \n",
    "        total_time += time_ms\n",
    "        print(f\"  {step:<15} {shape:<30} {device:<8} {time_ms:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\n⏱️ Total forward pass time: {total_time:.2f}ms\")\n",
    "    print(f\"🚀 Estimated throughput: {1000/total_time:.0f} batches/second\")\n",
    "\n",
    "simulate_neural_network_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめと次のステップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 RusTorch CoreML統合サマリー:\")\nprint()\nprint(\"✅ 完了項目:\")\nprint(\"  • Jupyter環境の設定\")\nprint(\"  • RustカーネルとPythonバインディングの作成\")\nprint(\"  • CoreML可用性チェック\")\nprint(\"  • デバイス管理と設定\")\nprint(\"  • バックエンド統計とプロファイリング\")\nprint(\"  • スマートデバイス選択\")\nprint()\nprint(\"🚧 今後の開発:\")\nprint(\"  • 実際のCoreML演算の実装\")\nprint(\"  • パフォーマンスベンチマーク\")\nprint(\"  • より多くの活性化関数とレイヤータイプ\")\nprint(\"  • エラーハンドリングの改善\")\nprint(\"  • メモリ最適化\")\nprint()\nprint(\"🎯 推奨される次のステップ:\")\nprint(\"  1. 実際のCoreMLモデルのロードとテスト\")\nprint(\"  2. MetalとCoreMLのパフォーマンス比較\")\nprint(\"  3. 実際のディープラーニングワークフローでのテスト\")\nprint(\"  4. プロダクション環境での評価\")\n\nif coreml_available:\n    print(\"\\n🎉 おめでとうございます！CoreMLが利用可能で、すべての機能をテストできます。\")\nelse:\n    print(\"\\n⚠️ CoreMLが利用できませんが、基本的な機能は動作しています。\")\n    print(\"   macOSでCoreMLフィーチャーを有効にしてビルドすることをお勧めします。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}