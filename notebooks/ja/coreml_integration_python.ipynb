{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RusTorch CoreML Integration - Python Bindings\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Pythonãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã§RusTorchã®CoreMLæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RusTorchã®Pythonãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "try:\n",
    "    import rustorch\n",
    "    print(f\"âœ… RusTorch version: {rustorch.__version__}\")\n",
    "    print(f\"ğŸ“ Description: {rustorch.__description__}\")\n",
    "    print(f\"ğŸ‘¥ Author: {rustorch.__author__}\")\nexcept ImportError as e:\n",
    "    print(f\"âŒ RusTorchã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}\")\n",
    "    print(\"maturin develop ã§ãƒ“ãƒ«ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "    exit(1)\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "print(f\"ğŸ–¥ï¸ ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.system()} {platform.release()}\")\n",
    "print(f\"ğŸ Python version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreMLã®å¯ç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoreMLæ©Ÿèƒ½ã®ç¢ºèª\n",
    "try:\n",
    "    # CoreMLãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    coreml_available = rustorch.is_coreml_available()\n",
    "    print(f\"ğŸ CoreML available: {coreml_available}\")\n",
    "    \n",
    "    if coreml_available:\n",
    "        print(\"ğŸ‰ CoreMLãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼\")\n",
    "        \n",
    "        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—\n",
    "        device_info = rustorch.get_coreml_device_info()\n",
    "        print(\"ğŸ“± CoreMLãƒ‡ãƒã‚¤ã‚¹æƒ…å ±:\")\n",
    "        print(device_info)\n",
    "    else:\n",
    "        print(\"âš ï¸ CoreMLã¯åˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "        if platform.system() != \"Darwin\":\n",
    "            print(\"CoreMLã¯macOSã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™\")\n",
    "        else:\n",
    "            print(\"CoreMLãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "            \nexcept AttributeError:\n",
    "    print(\"âŒ CoreMLé–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"CoreMLãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã§ãƒ“ãƒ«ãƒ‰ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "    coreml_available = False\nexcept Exception as e:\n",
    "    print(f\"âŒ CoreMLãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    coreml_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreMLãƒ‡ãƒã‚¤ã‚¹ã®ä½œæˆã¨æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # CoreMLãƒ‡ãƒã‚¤ã‚¹ã‚’ä½œæˆ\n",
    "        device = rustorch.CoreMLDevice(device_id=0)\n",
    "        print(f\"ğŸ–¥ï¸ CoreMLãƒ‡ãƒã‚¤ã‚¹ä½œæˆ: {device}\")\n",
    "        \n",
    "        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—\n",
    "        print(f\"ğŸ†” Device ID: {device.device_id()}\")\n",
    "        print(f\"âœ… Available: {device.is_available()}\")\n",
    "        print(f\"ğŸ’¾ Memory limit: {device.memory_limit()} bytes\")\n",
    "        print(f\"ğŸ§® Compute units limit: {device.compute_units_limit()}\")\n",
    "        print(f\"ğŸ“š Model cache size: {device.model_cache_size()}\")\n",
    "        \n",
    "        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "        device.cleanup_cache()\n",
    "        print(\"ğŸ§¹ Cache cleaned up\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CoreMLãƒ‡ãƒã‚¤ã‚¹æ“ä½œã‚¨ãƒ©ãƒ¼: {e}\")\nelse:\n",
    "    print(\"âš ï¸ CoreMLãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ‡ãƒã‚¤ã‚¹æ“ä½œã‚’ã‚¹ã‚­ãƒƒãƒ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # CoreMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®šã‚’ä½œæˆ\n",
    "        config = rustorch.CoreMLBackendConfig(\n",
    "            enable_caching=True,\n",
    "            max_cache_size=200,\n",
    "            enable_profiling=True,\n",
    "            auto_fallback=True\n",
    "        )\n",
    "        print(f\"âš™ï¸ Backend config: {config}\")\n",
    "        \n",
    "        # è¨­å®šå€¤ã‚’ç¢ºèªãƒ»å¤‰æ›´\n",
    "        print(f\"ğŸ“Š Enable caching: {config.enable_caching}\")\n",
    "        print(f\"ğŸ—‚ï¸ Max cache size: {config.max_cache_size}\")\n",
    "        print(f\"ğŸ“ˆ Enable profiling: {config.enable_profiling}\")\n",
    "        print(f\"ğŸ”„ Auto fallback: {config.auto_fallback}\")\n",
    "        \n",
    "        # è¨­å®šã‚’å¤‰æ›´\n",
    "        config.enable_profiling = False\n",
    "        config.max_cache_size = 150\n",
    "        print(f\"\\nğŸ”§ Updated config: {config}\")\n",
    "        \n",
    "        # CoreMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½œæˆ\n",
    "        backend = rustorch.CoreMLBackend(config)\n",
    "        print(f\"ğŸš€ CoreML backend: {backend}\")\n",
    "        print(f\"âœ… Backend available: {backend.is_available()}\")\n",
    "        \n",
    "        # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰çµ±è¨ˆã‚’å–å¾—\n",
    "        stats = backend.get_stats()\n",
    "        print(f\"ğŸ“Š Backend stats: {stats}\")\n",
    "        print(f\"   Total operations: {stats.total_operations}\")\n",
    "        print(f\"   Cache hits: {stats.cache_hits}\")\n",
    "        print(f\"   Cache misses: {stats.cache_misses}\")\n",
    "        print(f\"   Fallback operations: {stats.fallback_operations}\")\n",
    "        print(f\"   Cache hit rate: {stats.cache_hit_rate():.2%}\")\n",
    "        print(f\"   Fallback rate: {stats.fallback_rate():.2%}\")\n",
    "        print(f\"   Avg execution time: {stats.average_execution_time_ms:.2f}ms\")\n",
    "        \n",
    "        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "        backend.cleanup_cache()\n",
    "        print(\"\\nğŸ§¹ Backend cache cleaned\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ CoreMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æ“ä½œã‚¨ãƒ©ãƒ¼: {e}\")\nelse:\n",
    "    print(\"âš ï¸ CoreMLãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æ“ä½œã‚’ã‚¹ã‚­ãƒƒãƒ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œï¼ˆCPUï¼‰\n",
    "\n",
    "CoreMLã¨ã®æ¯”è¼ƒã®ãŸã‚ã«ã€ã¾ãšCPUã§ã®åŸºæœ¬æ“ä½œã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«ä½œæˆã¨æ“ä½œ\n",
    "    print(\"ğŸ§® åŸºæœ¬çš„ãªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œï¼ˆCPUï¼‰\")\n",
    "    \n",
    "    # NumPyé…åˆ—ã‹ã‚‰ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆï¼ˆç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰\n",
    "    data_a = np.random.randn(2, 3).astype(np.float32)\n",
    "    data_b = np.random.randn(3, 2).astype(np.float32)\n",
    "    \n",
    "    print(f\"ğŸ“ Matrix A shape: {data_a.shape}\")\n",
    "    print(f\"ğŸ“ Matrix B shape: {data_b.shape}\")\n",
    "    \n",
    "    # NumPyã§è¡Œåˆ—ä¹—ç®—ï¼ˆæ¯”è¼ƒç”¨ï¼‰\n",
    "    numpy_result = np.matmul(data_a, data_b)\n",
    "    print(f\"âœ… NumPy matmul result shape: {numpy_result.shape}\")\n",
    "    print(f\"ğŸ“Š Result (first few elements): {numpy_result.flatten()[:4]}\")\n",
    "    \n",
    "    print(\"\\nğŸš€ CPUæ¼”ç®—å®Œäº†\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âŒ ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_matrix_operations():\n",
    "    \"\"\"ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ\"\"\"\n",
    "    \n",
    "    sizes = [(64, 64), (128, 128), (256, 256), (512, 512)]\n",
    "    \n",
    "    print(\"ğŸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:\")\n",
    "    print(\"Size\\t\\tCPU Time (ms)\\tExpected CoreML (ms)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # CPUå®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®š\n",
    "        a = np.random.randn(*size).astype(np.float32)\n",
    "        b = np.random.randn(size[1], size[0]).astype(np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = np.matmul(a, b)\n",
    "        cpu_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # CoreMLã®äºˆæƒ³æ™‚é–“ï¼ˆä»®å®šçš„ï¼‰\n",
    "        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€CoreMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã®å®Ÿæ¸¬å€¤ã‚’ä½¿ç”¨\n",
    "        expected_coreml_time = cpu_time * 0.6  # ä»®å®š: CoreMLã¯40%é«˜é€Ÿ\n",
    "        \n",
    "        print(f\"{size[0]}x{size[1]}\\t\\t{cpu_time:.2f}\\t\\t{expected_coreml_time:.2f}\")\n",
    "\n",
    "benchmark_matrix_operations()\n",
    "\n",
    "print(\"\\nğŸ“ æ³¨æ„: CoreMLæ™‚é–“ã¯ä»®å®šå€¤ã§ã™ã€‚å®Ÿéš›ã®å€¤ã¯å…·ä½“çš„ãªå®Ÿè£…ã¨ä¾å­˜ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒã‚¤ã‚¹é¸æŠã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_device_selection():\n",
    "    \"\"\"ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹é¸æŠã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\"\"\"\n",
    "    \n",
    "    operations = [\n",
    "        (\"å°ã•ã„è¡Œåˆ—ä¹—ç®—\", (16, 16), \"CPU\"),\n",
    "        (\"ä¸­ç¨‹åº¦è¡Œåˆ—ä¹—ç®—\", (128, 128), \"Metal GPU\"),\n",
    "        (\"å¤§ãã„è¡Œåˆ—ä¹—ç®—\", (512, 512), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"æ´»æ€§åŒ–é–¢æ•°\", (32, 64, 128, 128), \"Metal GPU\"),\n",
    "        (\"ç•³ã¿è¾¼ã¿ï¼ˆå°ï¼‰\", (1, 3, 32, 32), \"CPU\"),\n",
    "        (\"ç•³ã¿è¾¼ã¿ï¼ˆå¤§ï¼‰\", (16, 64, 224, 224), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"è¤‡ç´ æ•°æ¼”ç®—\", (128, 128), \"Metal GPU\"),  # CoreMLéå¯¾å¿œ\n",
    "        (\"çµ±è¨ˆçš„åˆ†å¸ƒ\", (1000,), \"CPU\"),  # CoreMLéå¯¾å¿œ\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹é¸æŠã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:\")\n",
    "    print(\"Operation\\t\\t\\tTensor Shape\\t\\tSelected Device\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, shape, device in operations:\n",
    "        shape_str = \"x\".join(map(str, shape))\n",
    "        print(f\"{name:<23}\\t{shape_str:<15}\\t{device}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ é¸æŠãƒ­ã‚¸ãƒƒã‚¯:\")\n",
    "    print(\"  â€¢ å°ã•ã„æ“ä½œ: CPUï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å›é¿ï¼‰\")\n",
    "    print(\"  â€¢ ä¸­ç¨‹åº¦æ“ä½œ: Metal GPUï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰\")\n",
    "    print(\"  â€¢ å¤§ãã„æ“ä½œ: CoreMLï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰\")\n",
    "    print(\"  â€¢ éå¯¾å¿œæ“ä½œ: GPU/CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\")\n",
    "\n",
    "simulate_device_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Ÿè·µçš„ãªä½¿ç”¨ä¾‹: ç°¡å˜ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_neural_network_layer():\n",
    "    \"\"\"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:\")\n",
    "    \n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è¨­å®š\n",
    "    batch_size = 32\n",
    "    input_features = 784  # 28x28 MNIST\n",
    "    hidden_features = 256\n",
    "    output_features = 10  # 10ã‚¯ãƒ©ã‚¹\n",
    "    \n",
    "    print(f\"ğŸ“Š Batch size: {batch_size}\")\n",
    "    print(f\"ğŸ”¢ Input features: {input_features}\")\n",
    "    print(f\"ğŸ§® Hidden features: {hidden_features}\")\n",
    "    print(f\"ğŸ¯ Output features: {output_features}\")\n",
    "    \n",
    "    # å‰å‘ãä¼æ’­ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    steps = [\n",
    "        (\"Input â†’ Hidden\", f\"({batch_size}, {input_features}) @ ({input_features}, {hidden_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"ReLU Activation\", f\"({batch_size}, {hidden_features})\", \"Metal\"),\n",
    "        (\"Hidden â†’ Output\", f\"({batch_size}, {hidden_features}) @ ({hidden_features}, {output_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"Softmax\", f\"({batch_size}, {output_features})\", \"CPU\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ”„ Forward pass simulation:\")\n",
    "    total_time = 0\n",
    "    \n",
    "    for step, shape, device in steps:\n",
    "        # ä»®æƒ³çš„ãªå®Ÿè¡Œæ™‚é–“ï¼ˆmsï¼‰\n",
    "        if device == \"CoreML\":\n",
    "            time_ms = np.random.uniform(0.5, 2.0)\n",
    "        elif device == \"Metal\":\n",
    "            time_ms = np.random.uniform(1.0, 3.0)\n",
    "        else:  # CPU\n",
    "            time_ms = np.random.uniform(0.2, 1.0)\n",
    "        \n",
    "        total_time += time_ms\n",
    "        print(f\"  {step:<15} {shape:<30} {device:<8} {time_ms:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ Total forward pass time: {total_time:.2f}ms\")\n",
    "    print(f\"ğŸš€ Estimated throughput: {1000/total_time:.0f} batches/second\")\n",
    "\n",
    "simulate_neural_network_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ RusTorch CoreMLçµ±åˆã‚µãƒãƒªãƒ¼:\")\nprint()\nprint(\"âœ… å®Œäº†é …ç›®:\")\nprint(\"  â€¢ Jupyterç’°å¢ƒã®è¨­å®š\")\nprint(\"  â€¢ Rustã‚«ãƒ¼ãƒãƒ«ã¨Pythonãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ä½œæˆ\")\nprint(\"  â€¢ CoreMLå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯\")\nprint(\"  â€¢ ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã¨è¨­å®š\")\nprint(\"  â€¢ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰çµ±è¨ˆã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°\")\nprint(\"  â€¢ ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒã‚¤ã‚¹é¸æŠ\")\nprint()\nprint(\"ğŸš§ ä»Šå¾Œã®é–‹ç™º:\")\nprint(\"  â€¢ å®Ÿéš›ã®CoreMLæ¼”ç®—ã®å®Ÿè£…\")\nprint(\"  â€¢ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\")\nprint(\"  â€¢ ã‚ˆã‚Šå¤šãã®æ´»æ€§åŒ–é–¢æ•°ã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—\")\nprint(\"  â€¢ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„\")\nprint(\"  â€¢ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\")\nprint()\nprint(\"ğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\nprint(\"  1. å®Ÿéš›ã®CoreMLãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒ†ã‚¹ãƒˆ\")\nprint(\"  2. Metalã¨CoreMLã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\")\nprint(\"  3. å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã®ãƒ†ã‚¹ãƒˆ\")\nprint(\"  4. ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®è©•ä¾¡\")\n\nif coreml_available:\n    print(\"\\nğŸ‰ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼CoreMLãŒåˆ©ç”¨å¯èƒ½ã§ã€ã™ã¹ã¦ã®æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã§ãã¾ã™ã€‚\")\nelse:\n    print(\"\\nâš ï¸ CoreMLãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ãŒã€åŸºæœ¬çš„ãªæ©Ÿèƒ½ã¯å‹•ä½œã—ã¦ã„ã¾ã™ã€‚\")\n    print(\"   macOSã§CoreMLãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒ“ãƒ«ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}