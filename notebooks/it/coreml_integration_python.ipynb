{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrazione CoreML di RusTorch - Binding Python\n",
    "\n",
    "Questo notebook dimostra come utilizzare la funzionalit√† CoreML di RusTorch tramite i binding Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione e Importazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importare i binding Python di RusTorch\n",
    "try:\n",
    "    import rustorch\n",
    "    print(f\"‚úÖ Versione RusTorch: {rustorch.__version__}\")\n",
    "    print(f\"üìù Descrizione: {rustorch.__description__}\")\n",
    "    print(f\"üë• Autore: {rustorch.__author__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Importazione di RusTorch fallita: {e}\")\n",
    "    print(\"Si prega di compilare con maturin develop\")\n",
    "    exit(1)\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "print(f\"üñ•Ô∏è Piattaforma: {platform.system()} {platform.release()}\")\n",
    "print(f\"üêç Versione Python: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificare la Disponibilit√† di CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificare la funzionalit√† CoreML\n",
    "try:\n",
    "    # Verificare se CoreML √® disponibile\n",
    "    coreml_available = rustorch.is_coreml_available()\n",
    "    print(f\"üçé CoreML disponibile: {coreml_available}\")\n",
    "    \n",
    "    if coreml_available:\n",
    "        print(\"üéâ CoreML √® disponibile!\")\n",
    "        \n",
    "        # Ottenere informazioni sul dispositivo\n",
    "        device_info = rustorch.get_coreml_device_info()\n",
    "        print(\"üì± Informazioni dispositivo CoreML:\")\n",
    "        print(device_info)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CoreML non √® disponibile\")\n",
    "        if platform.system() != \"Darwin\":\n",
    "            print(\"CoreML √® disponibile solo su macOS\")\n",
    "        else:\n",
    "            print(\"Le funzionalit√† CoreML potrebbero non essere abilitate\")\n",
    "            \n",
    "except AttributeError:\n",
    "    print(\"‚ùå Funzioni CoreML non trovate\")\n",
    "    print(\"Potrebbe non essere compilato con funzionalit√† CoreML\")\n",
    "    coreml_available = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore nella verifica di CoreML: {e}\")\n",
    "    coreml_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione Dispositivo CoreML e Operazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # Creare dispositivo CoreML\n",
    "        device = rustorch.CoreMLDevice(device_id=0)\n",
    "        print(f\"üñ•Ô∏è Dispositivo CoreML creato: {device}\")\n",
    "        \n",
    "        # Ottenere informazioni sul dispositivo\n",
    "        print(f\"üÜî ID dispositivo: {device.device_id()}\")\n",
    "        print(f\"‚úÖ Disponibile: {device.is_available()}\")\n",
    "        print(f\"üíæ Limite memoria: {device.memory_limit()} byte\")\n",
    "        print(f\"üßÆ Limite unit√† di calcolo: {device.compute_units_limit()}\")\n",
    "        print(f\"üìö Dimensione cache modello: {device.model_cache_size()}\")\n",
    "        \n",
    "        # Pulizia cache\n",
    "        device.cleanup_cache()\n",
    "        print(\"üßπ Cache pulita\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore operazione dispositivo CoreML: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Saltando operazioni dispositivo poich√© CoreML non √® disponibile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione Backend CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # Creare configurazione backend CoreML\n",
    "        config = rustorch.CoreMLBackendConfig(\n",
    "            enable_caching=True,\n",
    "            max_cache_size=200,\n",
    "            enable_profiling=True,\n",
    "            auto_fallback=True\n",
    "        )\n",
    "        print(f\"‚öôÔ∏è Configurazione backend: {config}\")\n",
    "        \n",
    "        # Verificare e modificare valori di configurazione\n",
    "        print(f\"üìä Abilita caching: {config.enable_caching}\")\n",
    "        print(f\"üóÇÔ∏è Dimensione max cache: {config.max_cache_size}\")\n",
    "        print(f\"üìà Abilita profiling: {config.enable_profiling}\")\n",
    "        print(f\"üîÑ Fallback automatico: {config.auto_fallback}\")\n",
    "        \n",
    "        # Modificare configurazione\n",
    "        config.enable_profiling = False\n",
    "        config.max_cache_size = 150\n",
    "        print(f\"\\nüîß Configurazione aggiornata: {config}\")\n",
    "        \n",
    "        # Creare backend CoreML\n",
    "        backend = rustorch.CoreMLBackend(config)\n",
    "        print(f\"üöÄ Backend CoreML: {backend}\")\n",
    "        print(f\"‚úÖ Backend disponibile: {backend.is_available()}\")\n",
    "        \n",
    "        # Ottenere statistiche backend\n",
    "        stats = backend.get_stats()\n",
    "        print(f\"üìä Statistiche backend: {stats}\")\n",
    "        print(f\"   Operazioni totali: {stats.total_operations}\")\n",
    "        print(f\"   Hit cache: {stats.cache_hits}\")\n",
    "        print(f\"   Miss cache: {stats.cache_misses}\")\n",
    "        print(f\"   Operazioni fallback: {stats.fallback_operations}\")\n",
    "        print(f\"   Tasso hit cache: {stats.cache_hit_rate():.2%}\")\n",
    "        print(f\"   Tasso fallback: {stats.fallback_rate():.2%}\")\n",
    "        print(f\"   Tempo esecuzione medio: {stats.average_execution_time_ms:.2f}ms\")\n",
    "        \n",
    "        # Pulizia cache\n",
    "        backend.cleanup_cache()\n",
    "        print(\"\\nüßπ Cache backend pulita\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore operazione backend CoreML: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Saltando operazioni backend poich√© CoreML non √® disponibile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operazioni Tensore di Base (CPU)\n",
    "\n",
    "Per confrontare con CoreML, eseguiamo prima operazioni di base su CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creazione e operazioni tensore di base\n",
    "    print(\"üßÆ Operazioni tensore di base (CPU)\")\n",
    "    \n",
    "    # Creare tensori da array NumPy (interfaccia semplificata)\n",
    "    data_a = np.random.randn(2, 3).astype(np.float32)\n",
    "    data_b = np.random.randn(3, 2).astype(np.float32)\n",
    "    \n",
    "    print(f\"üìê Forma matrice A: {data_a.shape}\")\n",
    "    print(f\"üìê Forma matrice B: {data_b.shape}\")\n",
    "    \n",
    "    # Moltiplicazione matrici con NumPy (per confronto)\n",
    "    numpy_result = np.matmul(data_a, data_b)\n",
    "    print(f\"‚úÖ Forma risultato matmul NumPy: {numpy_result.shape}\")\n",
    "    print(f\"üìä Risultato (primi elementi): {numpy_result.flatten()[:4]}\")\n",
    "    \n",
    "    print(\"\\nüöÄ Operazioni CPU completate\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Errore operazione tensore: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulazione Confronto Prestazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_matrix_operations():\n",
    "    \"\"\"Confrontare prestazioni con diverse dimensioni matrici\"\"\"\n",
    "    \n",
    "    sizes = [(64, 64), (128, 128), (256, 256), (512, 512)]\n",
    "    \n",
    "    print(\"üèÅ Confronto prestazioni:\")\n",
    "    print(\"Dimensione\\t\\tTempo CPU (ms)\\tCoreML Atteso (ms)\")\n",
    "    print(\"-\" * 58)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Misurare tempo esecuzione CPU\n",
    "        a = np.random.randn(*size).astype(np.float32)\n",
    "        b = np.random.randn(size[1], size[0]).astype(np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = np.matmul(a, b)\n",
    "        cpu_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Tempo CoreML atteso (ipotetico)\n",
    "        # Nell'implementazione reale, usare misurazioni reali dal backend CoreML\n",
    "        expected_coreml_time = cpu_time * 0.6  # Assunzione: CoreML √® 40% pi√π veloce\n",
    "        \n",
    "        print(f\"{size[0]}x{size[1]}\\t\\t{cpu_time:.2f}\\t\\t{expected_coreml_time:.2f}\")\n",
    "\n",
    "benchmark_matrix_operations()\n",
    "\n",
    "print(\"\\nüìù Nota: I tempi CoreML sono ipotetici. I valori reali dipendono dall'implementazione specifica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulazione Selezione Dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_device_selection():\n",
    "    \"\"\"Simulare selezione intelligente dispositivo\"\"\"\n",
    "    \n",
    "    operations = [\n",
    "        (\"Moltiplicazione matrice piccola\", (16, 16), \"CPU\"),\n",
    "        (\"Moltiplicazione matrice media\", (128, 128), \"Metal GPU\"),\n",
    "        (\"Moltiplicazione matrice grande\", (512, 512), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"Funzione attivazione\", (32, 64, 128, 128), \"Metal GPU\"),\n",
    "        (\"Convoluzione piccola\", (1, 3, 32, 32), \"CPU\"),\n",
    "        (\"Convoluzione grande\", (16, 64, 224, 224), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"Operazioni numeri complessi\", (128, 128), \"Metal GPU\"),  # CoreML non supportato\n",
    "        (\"Distribuzione statistica\", (1000,), \"CPU\"),  # CoreML non supportato\n",
    "    ]\n",
    "    \n",
    "    print(\"üéØ Simulazione selezione intelligente dispositivo:\")\n",
    "    print(\"Operazione\\t\\t\\tForma Tensore\\t\\tDispositivo Selezionato\")\n",
    "    print(\"-\" * 78)\n",
    "    \n",
    "    for name, shape, device in operations:\n",
    "        shape_str = \"x\".join(map(str, shape))\n",
    "        print(f\"{name:<31}\\t{shape_str:<15}\\t{device}\")\n",
    "    \n",
    "    print(\"\\nüìù Logica selezione:\")\n",
    "    print(\"  ‚Ä¢ Operazioni piccole: CPU (evitare overhead)\")\n",
    "    print(\"  ‚Ä¢ Operazioni medie: Metal GPU (bilanciato)\")\n",
    "    print(\"  ‚Ä¢ Operazioni grandi: CoreML (ottimizzato)\")\n",
    "    print(\"  ‚Ä¢ Operazioni non supportate: fallback GPU/CPU\")\n",
    "\n",
    "simulate_device_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio Pratico: Strato Semplice Rete Neurale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_neural_network_layer():\n",
    "    \"\"\"Simulare strato rete neurale\"\"\"\n",
    "    \n",
    "    print(\"üß† Simulazione strato rete neurale:\")\n",
    "    \n",
    "    # Dimensione batch e configurazione strato\n",
    "    batch_size = 32\n",
    "    input_features = 784  # 28x28 MNIST\n",
    "    hidden_features = 256\n",
    "    output_features = 10  # 10 classi\n",
    "    \n",
    "    print(f\"üìä Dimensione batch: {batch_size}\")\n",
    "    print(f\"üî¢ Feature input: {input_features}\")\n",
    "    print(f\"üßÆ Feature nascoste: {hidden_features}\")\n",
    "    print(f\"üéØ Feature output: {output_features}\")\n",
    "    \n",
    "    # Simulazione forward pass\n",
    "    steps = [\n",
    "        (\"Input ‚Üí Nascosto\", f\"({batch_size}, {input_features}) @ ({input_features}, {hidden_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"Attivazione ReLU\", f\"({batch_size}, {hidden_features})\", \"Metal\"),\n",
    "        (\"Nascosto ‚Üí Output\", f\"({batch_size}, {hidden_features}) @ ({hidden_features}, {output_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"Softmax\", f\"({batch_size}, {output_features})\", \"CPU\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîÑ Simulazione forward pass:\")\n",
    "    total_time = 0\n",
    "    \n",
    "    for step, shape, device in steps:\n",
    "        # Tempo esecuzione virtuale (ms)\n",
    "        if device == \"CoreML\":\n",
    "            time_ms = np.random.uniform(0.5, 2.0)\n",
    "        elif device == \"Metal\":\n",
    "            time_ms = np.random.uniform(1.0, 3.0)\n",
    "        else:  # CPU\n",
    "            time_ms = np.random.uniform(0.2, 1.0)\n",
    "        \n",
    "        total_time += time_ms\n",
    "        print(f\"  {step:<15} {shape:<30} {device:<8} {time_ms:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Tempo totale forward pass: {total_time:.2f}ms\")\n",
    "    print(f\"üöÄ Throughput stimato: {1000/total_time:.0f} batch/secondo\")\n",
    "\n",
    "simulate_neural_network_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riepilogo e Prossimi Passi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Riepilogo Integrazione CoreML RusTorch:\")\n",
    "print()\n",
    "print(\"‚úÖ Elementi completati:\")\n",
    "print(\"  ‚Ä¢ Configurazione ambiente Jupyter\")\n",
    "print(\"  ‚Ä¢ Creazione kernel Rust e binding Python\")\n",
    "print(\"  ‚Ä¢ Verifica disponibilit√† CoreML\")\n",
    "print(\"  ‚Ä¢ Gestione dispositivi e configurazione\")\n",
    "print(\"  ‚Ä¢ Statistiche e profiling backend\")\n",
    "print(\"  ‚Ä¢ Selezione intelligente dispositivo\")\n",
    "print()\n",
    "print(\"üöß Sviluppo futuro:\")\n",
    "print(\"  ‚Ä¢ Implementazione reale operazioni CoreML\")\n",
    "print(\"  ‚Ä¢ Benchmarking prestazioni\")\n",
    "print(\"  ‚Ä¢ Pi√π funzioni attivazione e tipi strato\")\n",
    "print(\"  ‚Ä¢ Miglioramenti gestione errori\")\n",
    "print(\"  ‚Ä¢ Ottimizzazione memoria\")\n",
    "print()\n",
    "print(\"üéØ Prossimi passi raccomandati:\")\n",
    "print(\"  1. Caricare e testare modelli CoreML reali\")\n",
    "print(\"  2. Confrontare prestazioni Metal e CoreML\")\n",
    "print(\"  3. Testare con workflow deep learning reali\")\n",
    "print(\"  4. Valutare in ambiente produzione\")\n",
    "\n",
    "if coreml_available:\n",
    "    print(\"\\nüéâ Congratulazioni! CoreML √® disponibile e tutte le funzionalit√† possono essere testate.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è CoreML non √® disponibile, ma le funzionalit√† di base stanno funzionando.\")\n",
    "    print(\"   Raccomandiamo di compilare con funzionalit√† CoreML abilitate su macOS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}