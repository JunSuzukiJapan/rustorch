{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RusTorch CoreML Integration - Python Bindings\n",
    "\n",
    "This notebook demonstrates how to use RusTorch's CoreML functionality through Python bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RusTorch Python bindings\n",
    "try:\n",
    "    import rustorch\n",
    "    print(f\"‚úÖ RusTorch version: {rustorch.__version__}\")\n",
    "    print(f\"üìù Description: {rustorch.__description__}\")\n",
    "    print(f\"üë• Author: {rustorch.__author__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import RusTorch: {e}\")\n",
    "    print(\"Please build with maturin develop\")\n",
    "    exit(1)\n",
    "\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "print(f\"üñ•Ô∏è Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"üêç Python version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CoreML Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CoreML functionality\n",
    "try:\n",
    "    # Check if CoreML is available\n",
    "    coreml_available = rustorch.is_coreml_available()\n",
    "    print(f\"üçé CoreML available: {coreml_available}\")\n",
    "    \n",
    "    if coreml_available:\n",
    "        print(\"üéâ CoreML is available!\")\n",
    "        \n",
    "        # Get device information\n",
    "        device_info = rustorch.get_coreml_device_info()\n",
    "        print(\"üì± CoreML device information:\")\n",
    "        print(device_info)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CoreML is not available\")\n",
    "        if platform.system() != \"Darwin\":\n",
    "            print(\"CoreML is only available on macOS\")\n",
    "        else:\n",
    "            print(\"CoreML features may not be enabled\")\n",
    "            \n",
    "except AttributeError:\n",
    "    print(\"‚ùå CoreML functions not found\")\n",
    "    print(\"May not be built with CoreML features\")\n",
    "    coreml_available = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking CoreML: {e}\")\n",
    "    coreml_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML Device Creation and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # Create CoreML device\n",
    "        device = rustorch.CoreMLDevice(device_id=0)\n",
    "        print(f\"üñ•Ô∏è CoreML device created: {device}\")\n",
    "        \n",
    "        # Get device information\n",
    "        print(f\"üÜî Device ID: {device.device_id()}\")\n",
    "        print(f\"‚úÖ Available: {device.is_available()}\")\n",
    "        print(f\"üíæ Memory limit: {device.memory_limit()} bytes\")\n",
    "        print(f\"üßÆ Compute units limit: {device.compute_units_limit()}\")\n",
    "        print(f\"üìö Model cache size: {device.model_cache_size()}\")\n",
    "        \n",
    "        # Cache cleanup\n",
    "        device.cleanup_cache()\n",
    "        print(\"üßπ Cache cleaned up\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CoreML device operation error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping device operations as CoreML is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML Backend Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if coreml_available:\n",
    "    try:\n",
    "        # Create CoreML backend configuration\n",
    "        config = rustorch.CoreMLBackendConfig(\n",
    "            enable_caching=True,\n",
    "            max_cache_size=200,\n",
    "            enable_profiling=True,\n",
    "            auto_fallback=True\n",
    "        )\n",
    "        print(f\"‚öôÔ∏è Backend config: {config}\")\n",
    "        \n",
    "        # Check and modify configuration values\n",
    "        print(f\"üìä Enable caching: {config.enable_caching}\")\n",
    "        print(f\"üóÇÔ∏è Max cache size: {config.max_cache_size}\")\n",
    "        print(f\"üìà Enable profiling: {config.enable_profiling}\")\n",
    "        print(f\"üîÑ Auto fallback: {config.auto_fallback}\")\n",
    "        \n",
    "        # Modify configuration\n",
    "        config.enable_profiling = False\n",
    "        config.max_cache_size = 150\n",
    "        print(f\"\\nüîß Updated config: {config}\")\n",
    "        \n",
    "        # Create CoreML backend\n",
    "        backend = rustorch.CoreMLBackend(config)\n",
    "        print(f\"üöÄ CoreML backend: {backend}\")\n",
    "        print(f\"‚úÖ Backend available: {backend.is_available()}\")\n",
    "        \n",
    "        # Get backend statistics\n",
    "        stats = backend.get_stats()\n",
    "        print(f\"üìä Backend stats: {stats}\")\n",
    "        print(f\"   Total operations: {stats.total_operations}\")\n",
    "        print(f\"   Cache hits: {stats.cache_hits}\")\n",
    "        print(f\"   Cache misses: {stats.cache_misses}\")\n",
    "        print(f\"   Fallback operations: {stats.fallback_operations}\")\n",
    "        print(f\"   Cache hit rate: {stats.cache_hit_rate():.2%}\")\n",
    "        print(f\"   Fallback rate: {stats.fallback_rate():.2%}\")\n",
    "        print(f\"   Avg execution time: {stats.average_execution_time_ms:.2f}ms\")\n",
    "        \n",
    "        # Cache cleanup\n",
    "        backend.cleanup_cache()\n",
    "        print(\"\\nüßπ Backend cache cleaned\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CoreML backend operation error: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping backend operations as CoreML is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations (CPU)\n",
    "\n",
    "For comparison with CoreML, let's first perform basic operations on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Basic tensor creation and operations\n",
    "    print(\"üßÆ Basic tensor operations (CPU)\")\n",
    "    \n",
    "    # Create tensors from NumPy arrays (simplified interface)\n",
    "    data_a = np.random.randn(2, 3).astype(np.float32)\n",
    "    data_b = np.random.randn(3, 2).astype(np.float32)\n",
    "    \n",
    "    print(f\"üìê Matrix A shape: {data_a.shape}\")\n",
    "    print(f\"üìê Matrix B shape: {data_b.shape}\")\n",
    "    \n",
    "    # Matrix multiplication with NumPy (for comparison)\n",
    "    numpy_result = np.matmul(data_a, data_b)\n",
    "    print(f\"‚úÖ NumPy matmul result shape: {numpy_result.shape}\")\n",
    "    print(f\"üìä Result (first few elements): {numpy_result.flatten()[:4]}\")\n",
    "    \n",
    "    print(\"\\nüöÄ CPU operations completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tensor operation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_matrix_operations():\n",
    "    \"\"\"Compare performance with different matrix sizes\"\"\"\n",
    "    \n",
    "    sizes = [(64, 64), (128, 128), (256, 256), (512, 512)]\n",
    "    \n",
    "    print(\"üèÅ Performance comparison:\")\n",
    "    print(\"Size\\t\\tCPU Time (ms)\\tExpected CoreML (ms)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Measure CPU execution time\n",
    "        a = np.random.randn(*size).astype(np.float32)\n",
    "        b = np.random.randn(size[1], size[0]).astype(np.float32)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = np.matmul(a, b)\n",
    "        cpu_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Expected CoreML time (hypothetical)\n",
    "        # In actual implementation, use real measurements from CoreML backend\n",
    "        expected_coreml_time = cpu_time * 0.6  # Assumption: CoreML is 40% faster\n",
    "        \n",
    "        print(f\"{size[0]}x{size[1]}\\t\\t{cpu_time:.2f}\\t\\t{expected_coreml_time:.2f}\")\n",
    "\n",
    "benchmark_matrix_operations()\n",
    "\n",
    "print(\"\\nüìù Note: CoreML times are hypothetical. Actual values depend on specific implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Selection Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_device_selection():\n",
    "    \"\"\"Simulate smart device selection\"\"\"\n",
    "    \n",
    "    operations = [\n",
    "        (\"Small matrix multiplication\", (16, 16), \"CPU\"),\n",
    "        (\"Medium matrix multiplication\", (128, 128), \"Metal GPU\"),\n",
    "        (\"Large matrix multiplication\", (512, 512), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"Activation function\", (32, 64, 128, 128), \"Metal GPU\"),\n",
    "        (\"Small convolution\", (1, 3, 32, 32), \"CPU\"),\n",
    "        (\"Large convolution\", (16, 64, 224, 224), \"CoreML\" if coreml_available else \"Metal GPU\"),\n",
    "        (\"Complex number operations\", (128, 128), \"Metal GPU\"),  # CoreML not supported\n",
    "        (\"Statistical distribution\", (1000,), \"CPU\"),  # CoreML not supported\n",
    "    ]\n",
    "    \n",
    "    print(\"üéØ Smart device selection simulation:\")\n",
    "    print(\"Operation\\t\\t\\tTensor Shape\\t\\tSelected Device\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, shape, device in operations:\n",
    "        shape_str = \"x\".join(map(str, shape))\n",
    "        print(f\"{name:<23}\\t{shape_str:<15}\\t{device}\")\n",
    "    \n",
    "    print(\"\\nüìù Selection logic:\")\n",
    "    print(\"  ‚Ä¢ Small operations: CPU (avoid overhead)\")\n",
    "    print(\"  ‚Ä¢ Medium operations: Metal GPU (balanced)\")\n",
    "    print(\"  ‚Ä¢ Large operations: CoreML (optimized)\")\n",
    "    print(\"  ‚Ä¢ Unsupported operations: GPU/CPU fallback\")\n",
    "\n",
    "simulate_device_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Simple Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_neural_network_layer():\n",
    "    \"\"\"Simulate neural network layer\"\"\"\n",
    "    \n",
    "    print(\"üß† Neural network layer simulation:\")\n",
    "    \n",
    "    # Batch size and layer configuration\n",
    "    batch_size = 32\n",
    "    input_features = 784  # 28x28 MNIST\n",
    "    hidden_features = 256\n",
    "    output_features = 10  # 10 classes\n",
    "    \n",
    "    print(f\"üìä Batch size: {batch_size}\")\n",
    "    print(f\"üî¢ Input features: {input_features}\")\n",
    "    print(f\"üßÆ Hidden features: {hidden_features}\")\n",
    "    print(f\"üéØ Output features: {output_features}\")\n",
    "    \n",
    "    # Forward pass simulation\n",
    "    steps = [\n",
    "        (\"Input ‚Üí Hidden\", f\"({batch_size}, {input_features}) @ ({input_features}, {hidden_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"ReLU Activation\", f\"({batch_size}, {hidden_features})\", \"Metal\"),\n",
    "        (\"Hidden ‚Üí Output\", f\"({batch_size}, {hidden_features}) @ ({hidden_features}, {output_features})\", \"CoreML\" if coreml_available else \"Metal\"),\n",
    "        (\"Softmax\", f\"({batch_size}, {output_features})\", \"CPU\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîÑ Forward pass simulation:\")\n",
    "    total_time = 0\n",
    "    \n",
    "    for step, shape, device in steps:\n",
    "        # Virtual execution time (ms)\n",
    "        if device == \"CoreML\":\n",
    "            time_ms = np.random.uniform(0.5, 2.0)\n",
    "        elif device == \"Metal\":\n",
    "            time_ms = np.random.uniform(1.0, 3.0)\n",
    "        else:  # CPU\n",
    "            time_ms = np.random.uniform(0.2, 1.0)\n",
    "        \n",
    "        total_time += time_ms\n",
    "        print(f\"  {step:<15} {shape:<30} {device:<8} {time_ms:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Total forward pass time: {total_time:.2f}ms\")\n",
    "    print(f\"üöÄ Estimated throughput: {1000/total_time:.0f} batches/second\")\n",
    "\n",
    "simulate_neural_network_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã RusTorch CoreML Integration Summary:\")\n",
    "print()\n",
    "print(\"‚úÖ Completed items:\")\n",
    "print(\"  ‚Ä¢ Jupyter environment setup\")\n",
    "print(\"  ‚Ä¢ Rust kernel and Python bindings creation\")\n",
    "print(\"  ‚Ä¢ CoreML availability check\")\n",
    "print(\"  ‚Ä¢ Device management and configuration\")\n",
    "print(\"  ‚Ä¢ Backend statistics and profiling\")\n",
    "print(\"  ‚Ä¢ Smart device selection\")\n",
    "print()\n",
    "print(\"üöß Future development:\")\n",
    "print(\"  ‚Ä¢ Actual CoreML operation implementation\")\n",
    "print(\"  ‚Ä¢ Performance benchmarking\")\n",
    "print(\"  ‚Ä¢ More activation functions and layer types\")\n",
    "print(\"  ‚Ä¢ Error handling improvements\")\n",
    "print(\"  ‚Ä¢ Memory optimization\")\n",
    "print()\n",
    "print(\"üéØ Recommended next steps:\")\n",
    "print(\"  1. Load and test actual CoreML models\")\n",
    "print(\"  2. Compare Metal and CoreML performance\")\n",
    "print(\"  3. Test with real deep learning workflows\")\n",
    "print(\"  4. Evaluate in production environment\")\n",
    "\n",
    "if coreml_available:\n",
    "    print(\"\\nüéâ Congratulations! CoreML is available and all features can be tested.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è CoreML is not available, but basic features are working.\")\n",
    "    print(\"   We recommend building with CoreML features enabled on macOS.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}