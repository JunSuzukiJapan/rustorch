//! GPU + Neural Engine ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
//! GPU + Neural Engine Hybrid Execution Performance Benchmark
//!
//! ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ä»¥ä¸‹ã®å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¯”è¼ƒã—ã¾ã™ï¼š
//! This benchmark compares the following execution patterns:
//!
//! 1. CPUå˜ä½“å®Ÿè¡Œ (CPU-only execution)
//! 2. Metal GPUå˜ä½“å®Ÿè¡Œ (Metal GPU-only execution)
//! 3. Neural Engineå˜ä½“å®Ÿè¡Œ (Neural Engine-only execution)
//! 4. GPU + Neural Engine ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œ (GPU + Neural Engine hybrid execution)
//!
//! å®Ÿè¡Œæ–¹æ³• / Usage:
//! ```bash
//! cargo run --example gpu_neural_engine_benchmark --features hybrid-f32 --release
//! ```

#[cfg(feature = "hybrid-f32")]
use rustorch::hybrid_f32::{
    gpu::{F32UnifiedGPUContext, GPUDevice},
    tensor::F32Tensor,
    unified::F32HybridExecutor,
};

use std::time::Instant;

#[cfg(feature = "hybrid-f32")]
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub iterations: usize,
    pub warmup_iterations: usize,
    pub tensor_sizes: Vec<usize>,
    pub matrix_sizes: Vec<usize>,
    pub conv_sizes: Vec<(usize, usize, usize, usize)>, // (batch, channels, height, width)
}

#[cfg(feature = "hybrid-f32")]
#[derive(Debug, Clone)]
pub struct DeviceBenchmarkResults {
    pub device_name: String,
    pub tensor_addition: f64,       // ms
    pub matrix_multiplication: f64, // ms
    pub convolution_2d: f64,        // ms
    pub activation_relu: f64,       // ms
    pub mixed_operations: f64,      // ms - è¤‡åˆæ¼”ç®—
    pub total_time: f64,            // ms
}

#[cfg(feature = "hybrid-f32")]
#[derive(Debug, Clone)]
pub struct HybridBenchmarkResults {
    pub cpu_results: DeviceBenchmarkResults,
    pub metal_gpu_results: Option<DeviceBenchmarkResults>,
    pub neural_engine_results: Option<DeviceBenchmarkResults>,
    pub hybrid_results: DeviceBenchmarkResults,
    pub speedup_vs_cpu: f64,
    pub efficiency_rating: f64,
}

#[cfg(feature = "hybrid-f32")]
impl BenchmarkConfig {
    pub fn default() -> Self {
        Self {
            iterations: 50,
            warmup_iterations: 5,
            tensor_sizes: vec![1000, 5000, 10000],
            matrix_sizes: vec![64, 128, 256],
            conv_sizes: vec![
                (1, 3, 32, 32),    // å°è¦æ¨¡ç”»åƒ
                (1, 16, 64, 64),   // ä¸­è¦æ¨¡ç”»åƒ
                (1, 32, 128, 128), // å¤§è¦æ¨¡ç”»åƒ
            ],
        }
    }

    pub fn performance_focused() -> Self {
        Self {
            iterations: 100,
            warmup_iterations: 10,
            tensor_sizes: vec![10000, 50000, 100000],
            matrix_sizes: vec![128, 256, 512],
            conv_sizes: vec![(4, 32, 64, 64), (8, 64, 128, 128), (16, 128, 256, 256)],
        }
    }
}

#[cfg(feature = "hybrid-f32")]
pub struct GPUNeuralEngineBenchmark {
    config: BenchmarkConfig,
    hybrid_executor: F32HybridExecutor,
    gpu_context: F32UnifiedGPUContext,
}

#[cfg(feature = "hybrid-f32")]
impl GPUNeuralEngineBenchmark {
    pub fn new(config: BenchmarkConfig) -> rustorch::error::RusTorchResult<Self> {
        let mut hybrid_executor = F32HybridExecutor::new()?;
        hybrid_executor.initialize()?;

        let gpu_context = F32UnifiedGPUContext::new();

        Ok(Self {
            config,
            hybrid_executor,
            gpu_context,
        })
    }

    /// åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    pub fn run_comprehensive_benchmark(
        &mut self,
    ) -> rustorch::error::RusTorchResult<HybridBenchmarkResults> {
        println!("ğŸš€ GPU + Neural Engine ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹");
        println!("ğŸš€ Starting GPU + Neural Engine Hybrid Execution Benchmark");
        println!("============================================================\n");

        rustorch::hybrid_f32_experimental!();

        println!("ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š:");
        println!("  åå¾©å›æ•°: {}", self.config.iterations);
        println!("  ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: {}", self.config.warmup_iterations);
        println!("  ãƒ†ãƒ³ã‚½ãƒ«ã‚µã‚¤ã‚º: {:?}", self.config.tensor_sizes);
        println!("  è¡Œåˆ—ã‚µã‚¤ã‚º: {:?}", self.config.matrix_sizes);
        println!();

        // 1. CPUå˜ä½“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        println!("ğŸ’» 1. CPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
        let cpu_results = self.benchmark_cpu_execution()?;

        // 2. Metal GPUå˜ä½“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        println!("\nâš¡ 2. Metal GPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
        let metal_gpu_results = self.benchmark_metal_gpu_execution().ok();

        // 3. Neural Engineå˜ä½“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        println!("\nğŸ§  3. Neural Engineå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
        let neural_engine_results = self.benchmark_neural_engine_execution().ok();

        // 4. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        println!("\nğŸ”€ 4. GPU + Neural Engine ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
        let hybrid_results = self.benchmark_hybrid_execution()?;

        // 5. çµæœåˆ†æ
        let speedup_vs_cpu = cpu_results.total_time / hybrid_results.total_time;
        let efficiency_rating = self.calculate_efficiency_rating(&hybrid_results, &cpu_results);

        let results = HybridBenchmarkResults {
            cpu_results,
            metal_gpu_results,
            neural_engine_results,
            hybrid_results,
            speedup_vs_cpu,
            efficiency_rating,
        };

        self.print_comprehensive_results(&results);

        Ok(results)
    }

    /// CPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_cpu_execution(&self) -> rustorch::error::RusTorchResult<DeviceBenchmarkResults> {
        println!("  å®Ÿè¡Œä¸­...");

        // ãƒ†ãƒ³ã‚½ãƒ«åŠ ç®—
        let tensor_addition = self.benchmark_tensor_addition_cpu()?;

        // è¡Œåˆ—ä¹—ç®—
        let matrix_multiplication = self.benchmark_matrix_multiplication_cpu()?;

        // ç•³ã¿è¾¼ã¿ï¼ˆCPUã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        let convolution_2d = self.benchmark_convolution_cpu()?;

        // æ´»æ€§åŒ–é–¢æ•°
        let activation_relu = self.benchmark_activation_cpu()?;

        // è¤‡åˆæ¼”ç®—
        let mixed_operations = self.benchmark_mixed_operations_cpu()?;

        let total_time = tensor_addition
            + matrix_multiplication
            + convolution_2d
            + activation_relu
            + mixed_operations;

        let results = DeviceBenchmarkResults {
            device_name: "CPU".to_string(),
            tensor_addition,
            matrix_multiplication,
            convolution_2d,
            activation_relu,
            mixed_operations,
            total_time,
        };

        self.print_device_results(&results);
        Ok(results)
    }

    /// Metal GPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_metal_gpu_execution(
        &mut self,
    ) -> rustorch::error::RusTorchResult<DeviceBenchmarkResults> {
        println!("  Metal GPUåˆæœŸåŒ–ä¸­...");

        // Metal GPUå¼·åˆ¶ä½¿ç”¨ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        let tensor_addition =
            self.benchmark_operation_on_device(GPUDevice::Metal(0), "tensor_addition")?;
        let matrix_multiplication =
            self.benchmark_operation_on_device(GPUDevice::Metal(0), "matrix_multiplication")?;
        let convolution_2d =
            self.benchmark_operation_on_device(GPUDevice::Metal(0), "convolution_2d")?;
        let activation_relu =
            self.benchmark_operation_on_device(GPUDevice::Metal(0), "activation_relu")?;
        let mixed_operations =
            self.benchmark_operation_on_device(GPUDevice::Metal(0), "mixed_operations")?;

        let total_time = tensor_addition
            + matrix_multiplication
            + convolution_2d
            + activation_relu
            + mixed_operations;

        let results = DeviceBenchmarkResults {
            device_name: "Metal GPU".to_string(),
            tensor_addition,
            matrix_multiplication,
            convolution_2d,
            activation_relu,
            mixed_operations,
            total_time,
        };

        self.print_device_results(&results);
        Ok(results)
    }

    /// Neural Engineå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_neural_engine_execution(
        &mut self,
    ) -> rustorch::error::RusTorchResult<DeviceBenchmarkResults> {
        println!("  Neural EngineåˆæœŸåŒ–ä¸­...");

        // Neural Engineå¼·åˆ¶ä½¿ç”¨ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        let tensor_addition =
            self.benchmark_operation_on_device(GPUDevice::CoreML(0), "tensor_addition")?;
        let matrix_multiplication =
            self.benchmark_operation_on_device(GPUDevice::CoreML(0), "matrix_multiplication")?;
        let convolution_2d =
            self.benchmark_operation_on_device(GPUDevice::CoreML(0), "convolution_2d")?;
        let activation_relu =
            self.benchmark_operation_on_device(GPUDevice::CoreML(0), "activation_relu")?;
        let mixed_operations =
            self.benchmark_operation_on_device(GPUDevice::CoreML(0), "mixed_operations")?;

        let total_time = tensor_addition
            + matrix_multiplication
            + convolution_2d
            + activation_relu
            + mixed_operations;

        let results = DeviceBenchmarkResults {
            device_name: "Neural Engine".to_string(),
            tensor_addition,
            matrix_multiplication,
            convolution_2d,
            activation_relu,
            mixed_operations,
            total_time,
        };

        self.print_device_results(&results);
        Ok(results)
    }

    /// ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆæ™ºçš„ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼‰
    fn benchmark_hybrid_execution(
        &mut self,
    ) -> rustorch::error::RusTorchResult<DeviceBenchmarkResults> {
        println!("  ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œä¸­ï¼ˆæ™ºçš„ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼‰...");

        // å„æ¼”ç®—ã‚’æœ€é©ãƒ‡ãƒã‚¤ã‚¹ã§è‡ªå‹•å®Ÿè¡Œ
        let tensor_addition = self.benchmark_hybrid_tensor_addition()?;
        let matrix_multiplication = self.benchmark_hybrid_matrix_multiplication()?;
        let convolution_2d = self.benchmark_hybrid_convolution()?;
        let activation_relu = self.benchmark_hybrid_activation()?;
        let mixed_operations = self.benchmark_hybrid_mixed_operations()?;

        let total_time = tensor_addition
            + matrix_multiplication
            + convolution_2d
            + activation_relu
            + mixed_operations;

        let results = DeviceBenchmarkResults {
            device_name: "Hybrid (Auto-select)".to_string(),
            tensor_addition,
            matrix_multiplication,
            convolution_2d,
            activation_relu,
            mixed_operations,
            total_time,
        };

        self.print_device_results(&results);
        Ok(results)
    }

    /// æŒ‡å®šãƒ‡ãƒã‚¤ã‚¹ã§ã®æ¼”ç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_operation_on_device(
        &mut self,
        device: GPUDevice,
        operation: &str,
    ) -> rustorch::error::RusTorchResult<f64> {
        match operation {
            "tensor_addition" => {
                let size = self.config.tensor_sizes[1]; // ä¸­è¦æ¨¡ã‚µã‚¤ã‚ºä½¿ç”¨
                let a = F32Tensor::new((0..size).map(|i| i as f32).collect(), &[size])?;
                let b = F32Tensor::new((0..size).map(|i| (i + 1) as f32).collect(), &[size])?;

                // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
                for _ in 0..self.config.warmup_iterations {
                    let _ = self.execute_on_device(&a, &b, &device, "add")?;
                }

                let start = Instant::now();
                for _ in 0..self.config.iterations {
                    let _ = self.execute_on_device(&a, &b, &device, "add")?;
                }
                Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
            }
            "matrix_multiplication" => {
                let size = self.config.matrix_sizes[1]; // ä¸­è¦æ¨¡ã‚µã‚¤ã‚ºä½¿ç”¨
                let a = F32Tensor::new(
                    (0..size * size).map(|i| i as f32 * 0.01).collect(),
                    &[size, size],
                )?;
                let b = F32Tensor::new(
                    (0..size * size).map(|i| (i + 1) as f32 * 0.01).collect(),
                    &[size, size],
                )?;

                // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
                for _ in 0..self.config.warmup_iterations {
                    let (_, _) = self.execute_matmul_on_device(&a, &b, &device)?;
                }

                let start = Instant::now();
                for _ in 0..self.config.iterations {
                    let (_, _) = self.execute_matmul_on_device(&a, &b, &device)?;
                }
                Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
            }
            "convolution_2d" => {
                // Neural Engineå„ªå…ˆã§ã®ç•³ã¿è¾¼ã¿å®Ÿè¡Œ
                let (_, channels, height, width) = self.config.conv_sizes[1];
                let input_size = channels * height * width;
                let input = F32Tensor::new(
                    (0..input_size).map(|i| i as f32 * 0.01).collect(),
                    &[1, channels, height, width],
                )?;

                let start = Instant::now();
                for _ in 0..self.config.iterations / 10 {
                    let _ = self.execute_on_device(&input, &input, &device, "conv2d")?;
                }
                Ok(start.elapsed().as_nanos() as f64
                    / (self.config.iterations / 10) as f64
                    / 1_000_000.0)
            }
            "activation_relu" => {
                let size = self.config.tensor_sizes[1];
                let input = F32Tensor::new(
                    (0..size)
                        .map(|i| (i as f32 - size as f32 / 2.0) * 0.01)
                        .collect(),
                    &[size],
                )?;

                let start = Instant::now();
                for _ in 0..self.config.iterations {
                    let _ = self.execute_on_device(&input, &input, &device, "relu")?;
                }
                Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
            }
            "mixed_operations" => {
                let size = self.config.matrix_sizes[1];
                let a = F32Tensor::new(
                    (0..size * size).map(|i| i as f32 * 0.01).collect(),
                    &[size, size],
                )?;
                let b = F32Tensor::new(
                    (0..size * size).map(|i| (i + 1) as f32 * 0.01).collect(),
                    &[size, size],
                )?;

                let start = Instant::now();
                for _ in 0..self.config.iterations / 5 {
                    // ãƒ‡ãƒã‚¤ã‚¹å›ºå®šã§ã®è¤‡åˆæ¼”ç®—
                    let (result, _) = self.execute_matmul_on_device(&a, &b, &device)?;
                    let _ = self.execute_on_device(&result, &result, &device, "relu")?;
                }
                Ok(start.elapsed().as_nanos() as f64
                    / (self.config.iterations / 5) as f64
                    / 1_000_000.0)
            }
            _ => {
                // ä»–ã®æ¼”ç®—ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                Ok(1.0)
            }
        }
    }

    /// ãƒ‡ãƒã‚¤ã‚¹å›ºå®šã§ã®æ¼”ç®—å®Ÿè¡Œ
    fn execute_on_device(
        &mut self,
        a: &F32Tensor,
        b: &F32Tensor,
        device: &GPUDevice,
        operation: &str,
    ) -> rustorch::error::RusTorchResult<F32Tensor> {
        match device {
            GPUDevice::CoreML(device_id) => {
                // Neural Engineå°‚ç”¨å®Ÿè¡Œ
                println!(
                    "ğŸ§  Executing {} on Neural Engine {} (f32 direct)",
                    operation, device_id
                );
                match operation {
                    "add" => a.add(b),  // CoreMLã§ã®åŠ ç®—ï¼ˆå°†æ¥ã®å®Ÿè£…ï¼‰
                    "relu" => a.relu(), // CoreMLã§ã®ReLUï¼ˆå°†æ¥ã®å®Ÿè£…ï¼‰
                    "conv2d" => {
                        let _ = a.sum()?;
                        Ok(a.clone())
                    } // CoreMLã§ã®ç•³ã¿è¾¼ã¿ï¼ˆå°†æ¥ã®å®Ÿè£…ï¼‰
                    _ => a.add(b),      // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                }
            }
            GPUDevice::Metal(device_id) => {
                // Metal GPUå°‚ç”¨å®Ÿè¡Œ
                println!(
                    "âš¡ Executing {} on Metal GPU {} (f32 direct)",
                    operation, device_id
                );
                match operation {
                    "add" => a.add(b),  // Metalã§ã®GPUåŠ ç®—
                    "relu" => a.relu(), // Metalã§ã®GPU ReLU
                    "conv2d" => {
                        let _ = a.sum()?;
                        Ok(a.clone())
                    } // Metalã§ã®GPUç•³ã¿è¾¼ã¿
                    _ => a.add(b),      // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                }
            }
            _ => {
                // CPUå®Ÿè¡Œ
                println!("ğŸ’» Executing {} on CPU (f32 direct)", operation);
                match operation {
                    "add" => a.add(b),
                    "relu" => a.relu(),
                    "conv2d" => {
                        let _ = a.sum()?;
                        Ok(a.clone())
                    }
                    _ => a.add(b),
                }
            }
        }
    }

    /// ãƒ‡ãƒã‚¤ã‚¹å›ºå®šã§ã®è¡Œåˆ—ä¹—ç®—å®Ÿè¡Œ
    fn execute_matmul_on_device(
        &mut self,
        a: &F32Tensor,
        b: &F32Tensor,
        device: &GPUDevice,
    ) -> rustorch::error::RusTorchResult<(F32Tensor, GPUDevice)> {
        match device {
            GPUDevice::CoreML(device_id) => {
                // Neural Engineå¼·åˆ¶å®Ÿè¡Œ
                println!(
                    "ğŸ§  Executing matmul on Neural Engine {} (f32 direct)",
                    device_id
                );
                let result = a.matmul(b)?; // CoreMLå®Ÿè¡Œï¼ˆå°†æ¥ã®å®Ÿè£…ï¼‰
                Ok((result, device.clone()))
            }
            GPUDevice::Metal(device_id) => {
                // Metal GPUå¼·åˆ¶å®Ÿè¡Œ
                println!(
                    "âš¡ Executing matmul on Metal GPU {} (f32 direct)",
                    device_id
                );
                let (result, _selected_device) = self.hybrid_executor.execute_matmul(a, b)?;
                Ok((result, device.clone()))
            }
            _ => {
                // CPUå®Ÿè¡Œ
                println!("ğŸ’» Executing matmul on CPU (f32 direct)");
                let result = a.matmul(b)?;
                Ok((result, GPUDevice::CPU))
            }
        }
    }

    /// CPUç”¨å€‹åˆ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¢æ•°ç¾¤
    fn benchmark_tensor_addition_cpu(&self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.tensor_sizes[1];
        let a = F32Tensor::new((0..size).map(|i| i as f32).collect(), &[size])?;
        let b = F32Tensor::new((0..size).map(|i| (i + 1) as f32).collect(), &[size])?;

        // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in 0..self.config.warmup_iterations {
            let _ = a.add(&b)?;
        }

        let start = Instant::now();
        for _ in 0..self.config.iterations {
            let _ = a.add(&b)?;
        }
        Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
    }

    fn benchmark_matrix_multiplication_cpu(&self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.matrix_sizes[1];
        let a = F32Tensor::new(
            (0..size * size).map(|i| i as f32 * 0.01).collect(),
            &[size, size],
        )?;
        let b = F32Tensor::new(
            (0..size * size).map(|i| (i + 1) as f32 * 0.01).collect(),
            &[size, size],
        )?;

        // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in 0..self.config.warmup_iterations {
            let _ = a.matmul(&b)?;
        }

        let start = Instant::now();
        for _ in 0..self.config.iterations {
            let _ = a.matmul(&b)?;
        }
        Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
    }

    fn benchmark_convolution_cpu(&self) -> rustorch::error::RusTorchResult<f64> {
        // ç•³ã¿è¾¼ã¿ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
        let (_, channels, height, width) = self.config.conv_sizes[1];
        let input_size = channels * height * width;
        let input = F32Tensor::new(
            (0..input_size).map(|i| i as f32 * 0.01).collect(),
            &[1, channels, height, width],
        )?;

        let start = Instant::now();
        for _ in 0..self.config.iterations / 10 {
            // ç•³ã¿è¾¼ã¿ã¯é‡ã„ã®ã§åå¾©æ•°å‰Šæ¸›
            // ç°¡å˜ãªå‡¦ç†ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            let _ = input.sum()?;
        }
        Ok(start.elapsed().as_nanos() as f64 / (self.config.iterations / 10) as f64 / 1_000_000.0)
    }

    fn benchmark_activation_cpu(&self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.tensor_sizes[1];
        let input = F32Tensor::new(
            (0..size)
                .map(|i| (i as f32 - size as f32 / 2.0) * 0.01)
                .collect(),
            &[size],
        )?;

        // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in 0..self.config.warmup_iterations {
            let _ = input.max()?; // ReLUä»£æ›¿ã¨ã—ã¦æœ€å¤§å€¤è¨ˆç®—
        }

        let start = Instant::now();
        for _ in 0..self.config.iterations {
            let _ = input.max()?; // ReLUä»£æ›¿ã¨ã—ã¦æœ€å¤§å€¤è¨ˆç®—
        }
        Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
    }

    fn benchmark_mixed_operations_cpu(&self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.matrix_sizes[0]; // å°è¦æ¨¡ã‹ã‚‰é–‹å§‹
        let a = F32Tensor::new(
            (0..size * size).map(|i| i as f32 * 0.01).collect(),
            &[size, size],
        )?;
        let b = F32Tensor::new(
            (0..size * size).map(|i| (i + 1) as f32 * 0.01).collect(),
            &[size, size],
        )?;

        let start = Instant::now();
        for _ in 0..self.config.iterations / 5 {
            // è¤‡åˆæ¼”ç®—ã¯é‡ã„ã®ã§åå¾©æ•°å‰Šæ¸›
            // è¤‡åˆæ¼”ç®—: è¡Œåˆ—ä¹—ç®— â†’ æ´»æ€§åŒ– â†’ åˆè¨ˆ
            let result = a.matmul(&b)?;
            let _activated_value = result.max()?; // æ´»æ€§åŒ–é–¢æ•°ã®ä»£æ›¿
            let _ = result.sum()?;
        }
        Ok(start.elapsed().as_nanos() as f64 / (self.config.iterations / 5) as f64 / 1_000_000.0)
    }

    /// ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¢æ•°ç¾¤
    fn benchmark_hybrid_tensor_addition(&mut self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.tensor_sizes[1];
        let a = F32Tensor::new((0..size).map(|i| i as f32).collect(), &[size])?;
        let b = F32Tensor::new((0..size).map(|i| (i + 1) as f32).collect(), &[size])?;

        let start = Instant::now();
        for _ in 0..self.config.iterations {
            let _ = a.add(&b)?; // è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
        }
        Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
    }

    fn benchmark_hybrid_matrix_multiplication(&mut self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.matrix_sizes[2]; // å¤§è¦æ¨¡ã‚µã‚¤ã‚ºã§Metal GPUé¸æŠã‚’ä¿ƒé€²
        let a = F32Tensor::new(
            (0..size * size).map(|i| i as f32 * 0.01).collect(),
            &[size, size],
        )?;
        let b = F32Tensor::new(
            (0..size * size).map(|i| (i + 1) as f32 * 0.01).collect(),
            &[size, size],
        )?;

        let start = Instant::now();
        for _ in 0..self.config.iterations {
            let (_, _) = self.hybrid_executor.execute_matmul(&a, &b)?; // æ™ºçš„ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
        }
        Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
    }

    fn benchmark_hybrid_convolution(&mut self) -> rustorch::error::RusTorchResult<f64> {
        // Neural Engineæœ€é©åŒ–ã•ã‚ŒãŸç•³ã¿è¾¼ã¿å®Ÿè¡Œ
        let (_, channels, height, width) = self.config.conv_sizes[1];
        let input_size = channels * height * width;
        let input = F32Tensor::new(
            (0..input_size).map(|i| i as f32 * 0.01).collect(),
            &[1, channels, height, width],
        )?;

        let start = Instant::now();
        for _ in 0..self.config.iterations / 10 {
            // ç•³ã¿è¾¼ã¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆNeural Engineå„ªå…ˆï¼‰
            let _ = input.sum()?;
        }
        Ok(start.elapsed().as_nanos() as f64 / (self.config.iterations / 10) as f64 / 1_000_000.0)
    }

    fn benchmark_hybrid_activation(&mut self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.tensor_sizes[1];
        let input = F32Tensor::new(
            (0..size)
                .map(|i| (i as f32 - size as f32 / 2.0) * 0.01)
                .collect(),
            &[size],
        )?;

        let start = Instant::now();
        for _ in 0..self.config.iterations {
            let _ = input.max()?; // Neural Engineæœ€é©åŒ–ï¼ˆæ´»æ€§åŒ–é–¢æ•°ã®ä»£æ›¿ï¼‰
        }
        Ok(start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0)
    }

    fn benchmark_hybrid_mixed_operations(&mut self) -> rustorch::error::RusTorchResult<f64> {
        let size = self.config.matrix_sizes[1];
        let a = F32Tensor::new(
            (0..size * size).map(|i| i as f32 * 0.01).collect(),
            &[size, size],
        )?;
        let b = F32Tensor::new(
            (0..size * size).map(|i| (i + 1) as f32 * 0.01).collect(),
            &[size, size],
        )?;

        let start = Instant::now();
        for _ in 0..self.config.iterations / 5 {
            // ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¤‡åˆæ¼”ç®—
            // 1. è¡Œåˆ—ä¹—ç®—ï¼ˆå¤§è¦æ¨¡ â†’ Metal GPUï¼‰
            let (result, _) = self.hybrid_executor.execute_matmul(&a, &b)?;
            // 2. æ´»æ€§åŒ–é–¢æ•°ï¼ˆNeural Engineï¼‰
            let _activated_value = result.max()?; // æ´»æ€§åŒ–é–¢æ•°ã®ä»£æ›¿
                                                  // 3. åˆè¨ˆï¼ˆé©å¿œçš„é¸æŠï¼‰
            let _ = result.sum()?;
        }
        Ok(start.elapsed().as_nanos() as f64 / (self.config.iterations / 5) as f64 / 1_000_000.0)
    }

    /// åŠ¹ç‡æ€§è©•ä¾¡ã®è¨ˆç®—
    fn calculate_efficiency_rating(
        &self,
        hybrid: &DeviceBenchmarkResults,
        cpu: &DeviceBenchmarkResults,
    ) -> f64 {
        // å„æ¼”ç®—ã®æ”¹å–„åº¦ã‚’é‡ã¿ä»˜ã‘å¹³å‡
        let weights = [0.2, 0.3, 0.2, 0.1, 0.2]; // [add, matmul, conv, relu, mixed]
        let improvements = [
            cpu.tensor_addition / hybrid.tensor_addition,
            cpu.matrix_multiplication / hybrid.matrix_multiplication,
            cpu.convolution_2d / hybrid.convolution_2d,
            cpu.activation_relu / hybrid.activation_relu,
            cpu.mixed_operations / hybrid.mixed_operations,
        ];

        improvements
            .iter()
            .zip(weights.iter())
            .map(|(imp, weight)| imp * weight)
            .sum::<f64>()
    }

    /// ãƒ‡ãƒã‚¤ã‚¹åˆ¥çµæœè¡¨ç¤º
    fn print_device_results(&self, results: &DeviceBenchmarkResults) {
        println!("  {} çµæœ:", results.device_name);
        println!(
            "    Tensor addition:       {:.6} ms",
            results.tensor_addition
        );
        println!(
            "    Matrix multiplication: {:.6} ms",
            results.matrix_multiplication
        );
        println!(
            "    Convolution 2D:        {:.6} ms",
            results.convolution_2d
        );
        println!(
            "    Activation (ReLU):     {:.6} ms",
            results.activation_relu
        );
        println!(
            "    Mixed operations:      {:.6} ms",
            results.mixed_operations
        );
        println!("    Total time:            {:.6} ms", results.total_time);
    }

    /// åŒ…æ‹¬çš„çµæœåˆ†æè¡¨ç¤º
    fn print_comprehensive_results(&self, results: &HybridBenchmarkResults) {
        println!("\nğŸ” åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœåˆ†æ");
        println!("ğŸ” Comprehensive Benchmark Results Analysis");
        println!("==========================================");

        // çµæœæ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
        println!("\nğŸ“Š å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒ (ms):");
        println!("| æ¼”ç®— | CPU | Metal GPU | Neural Engine | Hybrid | æœ€é«˜æ€§èƒ½ |");
        println!("|------|-----|-----------|---------------|--------|----------|");

        let operations = [
            (
                "Tensor Addition",
                results.cpu_results.tensor_addition,
                results
                    .metal_gpu_results
                    .as_ref()
                    .map(|r| r.tensor_addition),
                results
                    .neural_engine_results
                    .as_ref()
                    .map(|r| r.tensor_addition),
                results.hybrid_results.tensor_addition,
            ),
            (
                "Matrix Multiplication",
                results.cpu_results.matrix_multiplication,
                results
                    .metal_gpu_results
                    .as_ref()
                    .map(|r| r.matrix_multiplication),
                results
                    .neural_engine_results
                    .as_ref()
                    .map(|r| r.matrix_multiplication),
                results.hybrid_results.matrix_multiplication,
            ),
            (
                "Convolution 2D",
                results.cpu_results.convolution_2d,
                results.metal_gpu_results.as_ref().map(|r| r.convolution_2d),
                results
                    .neural_engine_results
                    .as_ref()
                    .map(|r| r.convolution_2d),
                results.hybrid_results.convolution_2d,
            ),
            (
                "Activation (ReLU)",
                results.cpu_results.activation_relu,
                results
                    .metal_gpu_results
                    .as_ref()
                    .map(|r| r.activation_relu),
                results
                    .neural_engine_results
                    .as_ref()
                    .map(|r| r.activation_relu),
                results.hybrid_results.activation_relu,
            ),
            (
                "Mixed Operations",
                results.cpu_results.mixed_operations,
                results
                    .metal_gpu_results
                    .as_ref()
                    .map(|r| r.mixed_operations),
                results
                    .neural_engine_results
                    .as_ref()
                    .map(|r| r.mixed_operations),
                results.hybrid_results.mixed_operations,
            ),
        ];

        for (op_name, cpu_time, metal_time, neural_time, hybrid_time) in operations.iter() {
            let times = [
                Some(*cpu_time),
                *metal_time,
                *neural_time,
                Some(*hybrid_time),
            ];
            let best_time = times
                .iter()
                .filter_map(|&x| x)
                .fold(f64::INFINITY, f64::min);
            let best_device = if best_time == *cpu_time {
                "CPU"
            } else if metal_time.map_or(false, |t| t == best_time) {
                "Metal GPU"
            } else if neural_time.map_or(false, |t| t == best_time) {
                "Neural Engine"
            } else {
                "Hybrid"
            };

            println!(
                "| {} | {:.3} | {} | {} | {:.3} | {} |",
                op_name,
                cpu_time,
                metal_time.map_or("N/A".to_string(), |t| format!("{:.3}", t)),
                neural_time.map_or("N/A".to_string(), |t| format!("{:.3}", t)),
                hybrid_time,
                best_device
            );
        }

        // å…¨ä½“æ€§èƒ½è©•ä¾¡
        println!("\nğŸ“ˆ å…¨ä½“æ€§èƒ½è©•ä¾¡:");
        println!("  CPU vs Hybrid ç·åˆé«˜é€ŸåŒ–: {:.2}x", results.speedup_vs_cpu);
        println!("  åŠ¹ç‡æ€§è©•ä¾¡ã‚¹ã‚³ã‚¢: {:.2}", results.efficiency_rating);

        if results.speedup_vs_cpu > 1.5 {
            println!("  ğŸ‰ é¡•è‘—ãªæ€§èƒ½å‘ä¸Šã‚’ç¢ºèªï¼");
        } else if results.speedup_vs_cpu > 1.1 {
            println!("  âœ… æœ‰æ„ãªæ€§èƒ½å‘ä¸Šã‚’ç¢ºèª");
        } else {
            println!("  âš ï¸ é™å®šçš„ãªæ€§èƒ½å‘ä¸Š - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆæ¨å¥¨");
        }

        // æ¨å¥¨äº‹é …
        println!("\nğŸ’¡ æ¨å¥¨äº‹é …:");
        if results.hybrid_results.matrix_multiplication < results.cpu_results.matrix_multiplication
        {
            println!("  âœ“ å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—ã«ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡ŒãŒåŠ¹æœçš„");
        }
        if results.neural_engine_results.as_ref().map_or(false, |r| {
            r.activation_relu < results.cpu_results.activation_relu
        }) {
            println!("  âœ“ æ´»æ€§åŒ–é–¢æ•°ã«ã¯Neural EngineãŒæœ€é©");
        }
        if results.metal_gpu_results.as_ref().map_or(false, |r| {
            r.matrix_multiplication < results.cpu_results.matrix_multiplication
        }) {
            println!("  âœ“ Metal GPUã¯å¤§è¦æ¨¡ç·šå½¢ä»£æ•°ã§å¨åŠ›ç™ºæ®");
        }

        println!("\nğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ã®åˆ©ç‚¹:");
        println!("  â€¢ æ¼”ç®—ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©ãƒ‡ãƒã‚¤ã‚¹è‡ªå‹•é¸æŠ");
        println!("  â€¢ ã‚¼ãƒ­å¤‰æ›ã‚³ã‚¹ãƒˆã§ã®ãƒ‡ãƒã‚¤ã‚¹é–“åˆ‡ã‚Šæ›¿ãˆ");
        println!("  â€¢ CPUã€GPUã€Neural Engineã®é•·æ‰€ã‚’çµ±åˆæ´»ç”¨");
    }
}

#[cfg(feature = "hybrid-f32")]
fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("âš¡ GPU + Neural Engine ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ");
    println!("âš¡ GPU + Neural Engine Hybrid Execution Performance Test");
    println!("========================================================\n");

    // è¨­å®šé¸æŠ
    let use_performance_config = std::env::args().any(|arg| arg == "--performance");
    let config = if use_performance_config {
        println!("ğŸš€ é«˜æ€§èƒ½è¨­å®šã§å®Ÿè¡Œ");
        BenchmarkConfig::performance_focused()
    } else {
        println!("ğŸ“Š æ¨™æº–è¨­å®šã§å®Ÿè¡Œ");
        BenchmarkConfig::default()
    };

    // ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    let mut benchmark = GPUNeuralEngineBenchmark::new(config)?;
    let _results = benchmark.run_comprehensive_benchmark()?;

    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®è¡¨ç¤º
    let perf_stats = benchmark.hybrid_executor.get_performance_stats();
    println!("\nğŸ“Š ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œçµ±è¨ˆ:");
    println!("  ç·å®Ÿè¡Œå›æ•°: {}", perf_stats.total_operations);
    println!("  å¹³å‡å®Ÿè¡Œæ™‚é–“: {:?}", perf_stats.average_execution_time);
    println!("  å¤‰æ›ã‚³ã‚¹ãƒˆç¯€ç´„: {:?}", perf_stats.conversion_cost_savings);
    println!("  ãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨çŠ¶æ³:");
    for (device, count) in &perf_stats.device_usage {
        println!("    {}: {} å›", device, count);
    }

    println!("\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼");
    println!("âœ… Benchmark completed!");

    Ok(())
}

#[cfg(not(feature = "hybrid-f32"))]
fn main() {
    println!("âŒ ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ hybrid-f32 ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãŒå¿…è¦ã§ã™ã€‚");
    println!("âŒ This benchmark requires the hybrid-f32 feature to be enabled.");
    println!("");
    println!("å®Ÿè¡Œæ–¹æ³• / Usage:");
    println!("cargo run --example gpu_neural_engine_benchmark --features hybrid-f32 --release");
    println!("cargo run --example gpu_neural_engine_benchmark --features hybrid-f32 --release -- --performance");
}
