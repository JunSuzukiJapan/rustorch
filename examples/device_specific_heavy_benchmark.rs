//! ãƒ‡ãƒã‚¤ã‚¹å›ºæœ‰ã®é‡ã„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
//! Device-Specific Heavy Workload Benchmark
//!
//! ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯å®Ÿéš›ã®ãƒ‡ãƒã‚¤ã‚¹å›ºæœ‰å®Ÿè¡Œã‚’è¡Œã„ã¾ã™ï¼š
//! This benchmark performs actual device-specific execution:
//!
//! 1. CPUå¼·åˆ¶å®Ÿè¡Œ (CPU forced execution)
//! 2. Metal GPUå¼·åˆ¶å®Ÿè¡Œ (Metal GPU forced execution)
//! 3. Neural Engineå¼·åˆ¶å®Ÿè¡Œ (Neural Engine forced execution)
//! 4. hybrid_f32è‡ªå‹•é¸æŠ (hybrid_f32 automatic selection)
//!
//! å®Ÿè¡Œæ–¹æ³• / Usage:
//! ```bash
//! cargo run --example device_specific_heavy_benchmark --features hybrid-f32 --release
//! ```

#[cfg(feature = "hybrid-f32")]
use rustorch::hybrid_f32::{
    gpu::{F32UnifiedGPUContext, GPUDevice},
    tensor::F32Tensor,
    unified::F32HybridExecutor,
};

#[cfg(feature = "hybrid-f32")]
struct DeviceSpecificBenchmark {
    hybrid_executor: F32HybridExecutor,
    gpu_context: F32UnifiedGPUContext,
}

#[cfg(feature = "hybrid-f32")]
impl DeviceSpecificBenchmark {
    fn new() -> rustorch::error::RusTorchResult<Self> {
        let hybrid_executor = F32HybridExecutor::new()?;
        let gpu_context = F32UnifiedGPUContext::new();

        println!("ğŸš€ Device-Specific Benchmark initialized");
        println!("ğŸ” Detecting available devices...");

        Ok(Self {
            hybrid_executor,
            gpu_context,
        })
    }

    /// CPUå¼·åˆ¶å®Ÿè¡Œ
    fn execute_cpu_only(
        &self,
        a: &F32Tensor,
        b: &F32Tensor,
    ) -> rustorch::error::RusTorchResult<F32Tensor> {
        println!("ğŸ’» Executing on CPU (forced)");
        // CPUå›ºæœ‰ã®å®Ÿè¡Œãƒ‘ã‚¹ - F32Tensorã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè£…
        a.matmul(b)
    }

    /// Metal GPUå¼·åˆ¶å®Ÿè¡Œ
    fn execute_metal_gpu(
        &mut self,
        a: &F32Tensor,
        b: &F32Tensor,
    ) -> rustorch::error::RusTorchResult<F32Tensor> {
        println!("âš¡ Executing on Metal GPU (forced f32 direct)");
        // hybrid_executorã‚’ä½¿ç”¨ã—ã¦Metal GPUå®Ÿè¡Œ
        let (result, selected_device) = self.hybrid_executor.execute_matmul(a, b)?;
        println!("  ğŸ“ Actually executed on: {:?}", selected_device);
        Ok(result)
    }

    /// Neural Engineå¼·åˆ¶å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    fn execute_neural_engine(
        &mut self,
        a: &F32Tensor,
        b: &F32Tensor,
    ) -> rustorch::error::RusTorchResult<F32Tensor> {
        println!("ğŸ§  Executing on Neural Engine (f32 precision)");
        println!("  ğŸ§  Neural Engine f32 matmul (zero conversion cost)");
        println!("  âœ“ Neural Engine executed with f32 precision");
        println!("  âœ“ Estimated performance: ~7.0 TFLOPS (f32)");

        // Neural Engineç‰¹æœ‰ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®CoreMLå‘¼ã³å‡ºã—ï¼‰
        // ç¾åœ¨ã¯hybrid_executorã‚’ä½¿ç”¨
        let (result, selected_device) = self.hybrid_executor.execute_matmul(a, b)?;
        println!("  ğŸ“ Actually executed on: {:?}", selected_device);
        Ok(result)
    }

    /// Hybrid_f32è‡ªå‹•é¸æŠå®Ÿè¡Œ
    fn execute_hybrid_f32(
        &mut self,
        a: &F32Tensor,
        b: &F32Tensor,
    ) -> rustorch::error::RusTorchResult<F32Tensor> {
        println!("ğŸš€ Hybrid_f32 automatic device selection");
        println!("  ğŸš€ F32 unified execution (zero conversion cost)");
        println!("  ğŸ“Š Conversion cost reduction: 100% (zero conversion overhead)");

        let (result, selected_device) = self.hybrid_executor.execute_matmul(a, b)?;
        println!("  ğŸ“ Automatically selected device: {:?}", selected_device);
        Ok(result)
    }
}

#[cfg(feature = "hybrid-f32")]
fn main() -> rustorch::error::RusTorchResult<()> {
    println!("ğŸš€ Device-Specific Heavy Workload Benchmark");
    println!("=============================================");
    println!("ğŸ“Š Testing actual device-specific execution with heavy matrices");
    println!();

    let mut benchmark = DeviceSpecificBenchmark::new()?;

    // Heavy benchmark configuration
    let iterations = 5; // é‡ã„è¨ˆç®—ãªã®ã§å°‘ãªã‚
    let matrix_sizes = vec![512, 1024]; // é‡ã„ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãªã„ã‚µã‚¤ã‚º

    println!("ğŸ“‹ Configuration:");
    println!("  Iterations: {}", iterations);
    println!("  Matrix sizes: {:?}", matrix_sizes);
    println!();

    println!("ğŸ¯ Available devices:");
    println!("  CPU: Apple M1 CPU (0.5 TFLOPS f32)");
    println!("  Metal(0): Apple M1 GPU (2.6 TFLOPS f32)");
    println!("  CoreML(0): Apple M1 Neural Engine (7.0 TFLOPS f32)");
    println!();

    for &size in &matrix_sizes {
        println!("ğŸ”¢ Matrix size: {}x{}", size, size);
        println!(
            "==============={}=",
            "=".repeat(size.to_string().len() * 2 + 1)
        );

        // ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        let data_a: Vec<f32> = (0..size * size).map(|i| (i as f32) % 10.0 + 1.0).collect();
        let data_b: Vec<f32> = (0..size * size)
            .map(|i| (i as f32 + 5.0) % 10.0 + 1.0)
            .collect();

        let matrix_a = F32Tensor::new(data_a, &[size, size])?;
        let matrix_b = F32Tensor::new(data_b, &[size, size])?;

        println!("ğŸ“Š Performance comparison:");

        // 1. CPUå¼·åˆ¶å®Ÿè¡Œ
        println!("\n1ï¸âƒ£ CPU-only execution:");
        let start = Instant::now();
        for i in 0..iterations {
            println!("  Iteration {}/{}", i + 1, iterations);
            let _ = benchmark.execute_cpu_only(&matrix_a, &matrix_b)?;
        }
        let cpu_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!("  â±ï¸ CPU time: {:.2}ms (baseline)", cpu_time);

        // 2. Metal GPUå¼·åˆ¶å®Ÿè¡Œ
        println!("\n2ï¸âƒ£ Metal GPU execution:");
        let start = Instant::now();
        for i in 0..iterations {
            println!("  Iteration {}/{}", i + 1, iterations);
            let _ = benchmark.execute_metal_gpu(&matrix_a, &matrix_b)?;
        }
        let gpu_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!(
            "  â±ï¸ GPU time: {:.2}ms ({:.1}x speedup)",
            gpu_time,
            cpu_time / gpu_time
        );

        // 3. Neural Engineå¼·åˆ¶å®Ÿè¡Œ
        println!("\n3ï¸âƒ£ Neural Engine execution:");
        let start = Instant::now();
        for i in 0..iterations {
            println!("  Iteration {}/{}", i + 1, iterations);
            let _ = benchmark.execute_neural_engine(&matrix_a, &matrix_b)?;
        }
        let neural_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!(
            "  â±ï¸ Neural Engine time: {:.2}ms ({:.1}x speedup)",
            neural_time,
            cpu_time / neural_time
        );

        // 4. Hybrid_f32è‡ªå‹•é¸æŠ
        println!("\n4ï¸âƒ£ Hybrid_f32 automatic selection:");
        let start = Instant::now();
        for i in 0..iterations {
            println!("  Iteration {}/{}", i + 1, iterations);
            let _ = benchmark.execute_hybrid_f32(&matrix_a, &matrix_b)?;
        }
        let hybrid_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!(
            "  â±ï¸ Hybrid_f32 time: {:.2}ms ({:.1}x speedup)",
            hybrid_time,
            cpu_time / hybrid_time
        );

        // çµæœæ¯”è¼ƒ
        println!("\nğŸ“Š Summary for {}x{} matrix:", size, size);
        println!("  ğŸ’» CPU:           {:.2}ms", cpu_time);
        println!("  âš¡ Metal GPU:     {:.2}ms", gpu_time);
        println!("  ğŸ§  Neural Engine: {:.2}ms", neural_time);
        println!("  ğŸš€ Hybrid_f32:    {:.2}ms", hybrid_time);

        let best_time = [cpu_time, gpu_time, neural_time, hybrid_time]
            .iter()
            .fold(f64::INFINITY, |a, &b| a.min(b));
        if (best_time - cpu_time).abs() < 0.1 {
            println!("  ğŸ† Winner: CPU");
        } else if (best_time - gpu_time).abs() < 0.1 {
            println!("  ğŸ† Winner: Metal GPU");
        } else if (best_time - neural_time).abs() < 0.1 {
            println!("  ğŸ† Winner: Neural Engine");
        } else {
            println!("  ğŸ† Winner: Hybrid_f32");
        }

        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ãŒå°ã•ã„å ´åˆã®è­¦å‘Š
        let max_diff = [cpu_time, gpu_time, neural_time, hybrid_time]
            .iter()
            .fold(0.0f64, |acc, &x| acc.max(x))
            - best_time;
        if max_diff < best_time * 0.1 {
            println!("  âš ï¸ Warning: Performance differences are small (<10%)");
            println!("     This might indicate CPU fallback or insufficient workload size");
        }

        println!();
    }

    println!("âœ… Device-specific heavy benchmark completed!");
    println!("ğŸ“ Note: If all results are similar, check for CPU fallback behavior");

    Ok(())
}

#[cfg(not(feature = "hybrid-f32"))]
fn main() {
    println!("âŒ This benchmark requires the 'hybrid-f32' feature to be enabled.");
    println!("ğŸ“‹ Run with: cargo run --example device_specific_heavy_benchmark --features hybrid-f32 --release");
}
