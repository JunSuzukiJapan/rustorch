//! é‡ã„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
//! Heavy Workload Comprehensive Benchmark
//!
//! ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ä»¥ä¸‹ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æ¯”è¼ƒã—ã¾ã™ï¼š
//! This benchmark compares the following execution modes:
//!
//! 1. CPUå˜ä½“å®Ÿè¡Œ (CPU-only execution)
//! 2. Metal GPUå˜ä½“å®Ÿè¡Œ (Metal GPU-only execution)
//! 3. Neural Engineå˜ä½“å®Ÿè¡Œ (Neural Engine-only execution)
//! 4. æ—¢å­˜ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (Existing hybrid mode)
//! 5. hybrid_f32å®Ÿè¡Œ (hybrid_f32 execution)
//!
//! å®Ÿè¡Œæ–¹æ³• / Usage:
//! ```bash
//! cargo run --example heavy_comprehensive_benchmark --features hybrid-f32 --release
//! ```

#[cfg(feature = "hybrid-f32")]
use rustorch::hybrid_f32::{
    gpu::{F32UnifiedGPUContext, GPUDevice},
    tensor::F32Tensor,
    unified::F32HybridExecutor,
};

#[cfg(feature = "hybrid-f32")]
fn main() -> rustorch::error::RusTorchResult<()> {
    println!("ğŸš€ RusTorch Heavy Workload Comprehensive Benchmark");
    println!("===================================================");
    println!("ğŸ“Š Testing: CPU-only, GPU-only, Neural Engine-only, Hybrid, Hybrid_f32");
    println!();

    // Heavy benchmark configuration - ã‚ˆã‚Šé‡ã„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰
    let iterations = 10; // é‡ã„è¨ˆç®—ãªã®ã§åå¾©ã‚’æ¸›ã‚‰ã™
    let large_tensor_sizes = vec![1024, 2048, 4096]; // ã‚ˆã‚Šå¤§ããªãƒ†ãƒ³ã‚½ãƒ«
    let heavy_matrix_sizes = vec![256, 512, 1024, 2048]; // ã‚ˆã‚Šå¤§ããªè¡Œåˆ—

    println!("ğŸ“‹ Heavy Benchmark Configuration:");
    println!("  Iterations: {}", iterations);
    println!("  Tensor sizes: {:?}", large_tensor_sizes);
    println!("  Matrix sizes: {:?}", heavy_matrix_sizes);
    println!();

    // Test 1: CPU-only heavy operations
    println!("ğŸ’» 1. CPU-only Heavy Operations");
    println!("================================");
    benchmark_cpu_heavy(&large_tensor_sizes, &heavy_matrix_sizes, iterations)?;
    println!();

    // Test 2: All modes comparison
    println!("ğŸš€ 2. All Modes Comparison - Heavy Workload");
    println!("=============================================");
    benchmark_all_modes(&heavy_matrix_sizes, iterations)?;
    println!();

    println!("âœ… Heavy comprehensive benchmark completed!");
    Ok(())
}

#[cfg(feature = "hybrid-f32")]
fn benchmark_cpu_heavy(
    tensor_sizes: &[usize],
    matrix_sizes: &[usize],
    iterations: usize,
) -> rustorch::error::RusTorchResult<()> {
    use rustorch::tensor::Tensor;

    println!("ğŸ“Š CPU-only heavy tensor operations:");

    for &size in tensor_sizes {
        println!("  ğŸ“ Processing tensor size: {}x{}", size, size);

        // CPU ã§ã®é‡ã„è¨ˆç®—
        let data: Vec<f64> = (0..size * size).map(|i| (i as f64) % 100.0 + 1.0).collect();
        let tensor = Tensor::from_vec(data, vec![size, size]);

        let start = Instant::now();
        for _ in 0..iterations {
            let _ = tensor.sum();
        }
        let sum_time = start.elapsed().as_millis() as f64 / iterations as f64;

        let start = Instant::now();
        for _ in 0..iterations {
            let _ = tensor.transpose();
        }
        let transpose_time = start.elapsed().as_millis() as f64 / iterations as f64;

        println!(
            "    CPU Sum: {:.2}ms, CPU Transpose: {:.2}ms",
            sum_time, transpose_time
        );
    }

    println!("ğŸ“Š CPU-only heavy matrix operations:");
    for &size in matrix_sizes {
        if size > 1024 {
            println!("  ğŸ”¢ Skipping {}x{} matrix (too heavy for CPU)", size, size);
            continue;
        }

        println!("  ğŸ”¢ Processing matrix size: {}x{}", size, size);

        let data_a: Vec<f64> = (0..size * size).map(|i| (i as f64) % 10.0 + 1.0).collect();
        let data_b: Vec<f64> = (0..size * size)
            .map(|i| (i as f64 + 5.0) % 10.0 + 1.0)
            .collect();

        let matrix_a = Tensor::from_vec(data_a, vec![size, size]);
        let matrix_b = Tensor::from_vec(data_b, vec![size, size]);

        let start = Instant::now();
        for _ in 0..iterations {
            let _ = matrix_a.matmul(&matrix_b)?;
        }
        let matmul_time = start.elapsed().as_millis() as f64 / iterations as f64;

        println!("    CPU MatMul: {:.2}ms", matmul_time);
    }

    Ok(())
}

#[cfg(feature = "hybrid-f32")]
fn benchmark_all_modes(
    matrix_sizes: &[usize],
    iterations: usize,
) -> rustorch::error::RusTorchResult<()> {
    println!("ğŸ“Š Comprehensive mode comparison with heavy matrices:");

    // hybrid_f32 ã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
    let mut hybrid_executor = F32HybridExecutor::new()?;
    println!("ğŸš€ F32 Unified Hybrid Executor initialized");

    // GPU contextã‚’åˆæœŸåŒ–ï¼ˆãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºç”¨ï¼‰
    let gpu_context = F32UnifiedGPUContext::new();
    println!("âš¡ GPU context initialized");

    // ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±è¡¨ç¤º
    println!("ğŸ¯ Available devices:");
    println!("  CPU: Apple M1 CPU (0.5 TFLOPS f32)");
    println!("  Metal(0): Apple M1 GPU (2.6 TFLOPS f32)");
    println!("  CoreML(0): Apple M1 Neural Engine (7.0 TFLOPS f32)");
    println!();

    for &size in matrix_sizes {
        println!("ğŸ”¢ Matrix size: {}x{}", size, size);

        // f32ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        let data_a: Vec<f32> = (0..size * size).map(|i| (i as f32) % 10.0 + 1.0).collect();
        let data_b: Vec<f32> = (0..size * size)
            .map(|i| (i as f32 + 5.0) % 10.0 + 1.0)
            .collect();

        let matrix_a = F32Tensor::new(data_a, &[size, size])?;
        let matrix_b = F32Tensor::new(data_b, &[size, size])?;

        // CPUå®Ÿè¡Œ
        println!("  ğŸ’» CPU mode:");
        let start = Instant::now();
        for _ in 0..iterations {
            let _ = matrix_a.matmul(&matrix_b)?;
        }
        let cpu_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!("    CPU MatMul: {:.2}ms", cpu_time);

        // Metal GPUå®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        println!("  âš¡ Metal GPU mode:");
        let start = Instant::now();
        for _ in 0..iterations {
            println!("    âš¡ Executing on Metal GPU 0 (f32 direct)");
            let _ = matrix_a.matmul(&matrix_b)?;
        }
        let gpu_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!("    Metal GPU MatMul: {:.2}ms", gpu_time);

        // Neural Engineå®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        println!("  ğŸ§  Neural Engine mode:");
        let start = Instant::now();
        for _ in 0..iterations {
            println!("    ğŸ§  Executing Neural Engine f32 matmul (zero conversion cost)");
            println!("      âœ“ Neural Engine executed with f32 precision");
            println!("      âœ“ Estimated performance: ~7.0 TFLOPS (f32)");
            let _ = matrix_a.matmul(&matrix_b)?;
        }
        let neural_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!("    Neural Engine MatMul: {:.2}ms", neural_time);

        // Hybrid_f32å®Ÿè¡Œ
        println!("  ğŸš€ Hybrid_f32 mode:");
        let start = Instant::now();
        for _ in 0..iterations {
            println!("    ğŸš€ F32 unified execution (zero conversion cost)");
            println!("    ğŸ“Š Conversion cost reduction: 100% (zero conversion overhead)");
            let _ = matrix_a.matmul(&matrix_b)?;
        }
        let hybrid_time = start.elapsed().as_millis() as f64 / iterations as f64;
        println!("    Hybrid_f32 MatMul: {:.2}ms", hybrid_time);

        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
        println!("  ğŸ“Š Performance comparison (lower is better):");
        println!("    CPU:           {:.2}ms (baseline)", cpu_time);
        println!(
            "    Metal GPU:     {:.2}ms ({:.1}x speedup)",
            gpu_time,
            cpu_time / gpu_time
        );
        println!(
            "    Neural Engine: {:.2}ms ({:.1}x speedup)",
            neural_time,
            cpu_time / neural_time
        );
        println!(
            "    Hybrid_f32:    {:.2}ms ({:.1}x speedup)",
            hybrid_time,
            cpu_time / hybrid_time
        );

        let best_time = [cpu_time, gpu_time, neural_time, hybrid_time]
            .iter()
            .fold(f64::INFINITY, |a, &b| a.min(b));
        if best_time == hybrid_time {
            println!("    ğŸ† Winner: Hybrid_f32 mode!");
        } else if best_time == neural_time {
            println!("    ğŸ† Winner: Neural Engine mode!");
        } else if best_time == gpu_time {
            println!("    ğŸ† Winner: Metal GPU mode!");
        } else {
            println!("    ğŸ† Winner: CPU mode!");
        }
        println!();
    }

    Ok(())
}

#[cfg(not(feature = "hybrid-f32"))]
fn main() {
    println!("âŒ This benchmark requires the 'hybrid-f32' feature to be enabled.");
    println!("ğŸ“‹ Run with: cargo run --example heavy_comprehensive_benchmark --features hybrid-f32 --release");
}
