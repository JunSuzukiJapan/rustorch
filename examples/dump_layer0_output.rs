//! Layer 0 ã®å‡ºåŠ›ã‚’ãƒ€ãƒ³ãƒ—ã—ã¦ llama.cpp ã¨æ¯”è¼ƒ
//! 
//! ä½¿ç”¨æ–¹æ³•:
//! cargo run --release --example dump_layer0_output -- <model.gguf> <input_text>

use rustorch::backends::DeviceType;
use rustorch::error::RusTorchError;
use rustorch::formats::gguf::GGUFLoader;
use rustorch::models::gpt::{GPTConfig, GPTModel};
use std::env;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = env::args().collect();
    if args.len() < 3 {
        eprintln!("Usage: {} <model.gguf> <input_text>", args[0]);
        eprintln!("Example: {} tinyllama-1.1b-chat-v1.0.Q8_0.gguf \"1\"", args[0]);
        std::process::exit(1);
    }

    let model_path = &args[1];
    let input_text = &args[2];
    
    println!("ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {}", model_path);
    println!("ğŸ“ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: \"{}\"", input_text);

    // ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    let loader = GGUFLoader::from_file(model_path)?;
    let params = loader.get_model_params()?;
    let config = GPTConfig::from_model_params(&params);
    
    println!("\n=== ãƒ¢ãƒ‡ãƒ«è¨­å®š ===");
    println!("hidden_size (d_model): {}", config.d_model);
    println!("num_layers: {}", config.num_layers);
    println!("num_heads: {}", config.num_heads);
    println!("num_kv_heads: {}", config.num_kv_heads);
    
    // ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆ - å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«ç½®ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰
    // ã“ã“ã§ã¯ "1" -> [29896] ã®ã‚ˆã†ãªç°¡å˜ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æƒ³å®š
    let token_ids = if input_text == "1" {
        vec![29896]
    } else {
        eprintln!("âš ï¸  ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼: \"1\" ã®ã¿ã‚µãƒãƒ¼ãƒˆ");
        vec![29896]
    };
    
    println!("\n=== ãƒˆãƒ¼ã‚¯ãƒ³åŒ– ===");
    println!("Token IDs: {:?}", token_ids);
    
    // GPTãƒ¢ãƒ‡ãƒ«ä½œæˆ
    let mut model = GPTModel::from_gguf(model_path, DeviceType::Cpu)?;
    
    println!("\n=== ğŸ” Token Embedding å‡ºåŠ› ===");
    
    // Token embedding ã‚’æ‰‹å‹•ã§å–å¾—
    let token_emb_key = "token_embd.weight";
    let token_emb_tensor = loader.load_tensor(token_emb_key)?;
    let emb_shape = token_emb_tensor.data.shape();
    let hidden_size = emb_shape[0];
    let vocab_size = emb_shape[1];
    let emb_data = token_emb_tensor.data.as_slice().unwrap();
    
    println!("Token embedding shape: {:?}", emb_shape);
    println!("hidden_size: {}, vocab_size: {}", hidden_size, vocab_size);
    
    for (idx, &token_id) in token_ids.iter().enumerate() {
        let start = token_id * hidden_size;
        let end = start + hidden_size;
        let embedding = &emb_data[start..end];
        
        let mean: f64 = embedding.iter().sum::<f64>() / embedding.len() as f64;
        let rms = (embedding.iter().map(|&v| v * v).sum::<f64>() / embedding.len() as f64).sqrt();
        
        println!("\nğŸ“Œ Position {} (Token ID: {})", idx, token_id);
        println!("   çµ±è¨ˆ: mean={:.9}, rms={:.9}", mean, rms);
        println!("   æœ€åˆã®20è¦ç´ :");
        for i in 0..20.min(embedding.len()) {
            if i % 5 == 0 {
                print!("      ");
            }
            print!("{:12.9}", embedding[i]);
            if (i + 1) % 5 == 0 {
                println!();
            } else {
                print!(" ");
            }
        }
    }
    
    println!("\n=== ğŸ” Layer 0 RMS Norm Weight ===");
    let ln1_key = "blk.0.attn_norm.weight";
    let ln1_tensor = loader.load_tensor(ln1_key)?;
    let ln1_data = ln1_tensor.data.as_slice().unwrap();
    
    println!("RMS Norm weight shape: {:?}", ln1_tensor.data.shape());
    println!("Length: {}", ln1_data.len());
    println!("æœŸå¾…å€¤: 2048");
    
    if ln1_data.len() == 2048 {
        println!("âœ… é•·ã•ãŒæ­£ã—ã„");
    } else {
        println!("âŒ é•·ã•ãŒ {} ã§ã™ï¼", ln1_data.len());
    }
    
    let mean: f64 = ln1_data.iter().sum::<f64>() / ln1_data.len() as f64;
    let rms = (ln1_data.iter().map(|&v| v * v).sum::<f64>() / ln1_data.len() as f64).sqrt();
    println!("çµ±è¨ˆ: mean={:.9}, rms={:.9}", mean, rms);
    println!("æœ€åˆã®10è¦ç´ : {:?}", &ln1_data[0..10]);
    
    println!("\n=== ğŸ¯ Layer 0 å‡ºåŠ›è¨ˆç®—ã®æº–å‚™å®Œäº† ===");
    println!("\nllama.cpp ã¨ã®æ¯”è¼ƒæ‰‹é †:");
    println!("1. llama.cpp ã§åŒã˜å…¥åŠ› \"1\" ã‚’å‡¦ç†");
    println!("2. Layer 0 ã®å‡ºåŠ›ï¼ˆAttention + FFN å¾Œï¼‰ã‚’ãƒ€ãƒ³ãƒ—");
    println!("3. RusTorch ã® Layer 0 å‡ºåŠ›ã¨è¦ç´ ã”ã¨ã«æ¯”è¼ƒ");
    println!("4. å·®ç•°ãŒã‚ã‚‹å ´åˆã€ã©ã“ã§ç™ºç”Ÿã—ã¦ã„ã‚‹ã‹ç‰¹å®š:");
    println!("   - Token Embedding");
    println!("   - RMS Norm (Attentionå‰)");
    println!("   - Attention è¨ˆç®—");
    println!("   - RMS Norm (FFNå‰)");
    println!("   - FFN è¨ˆç®—");
    
    println!("\nğŸ’¡ ãƒ‡ãƒãƒƒã‚°ã®ãƒ’ãƒ³ãƒˆ:");
    println!("- RUSTORCH_DEBUG=1 ã‚’è¨­å®šã—ã¦è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–");
    println!("- RMS Norm ã® hidden_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ 2048 ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª");
    println!("- å„ã‚¹ãƒ†ãƒƒãƒ—ã® RMS å€¤ã‚’ llama.cpp ã¨æ¯”è¼ƒ");
    
    Ok(())
}
