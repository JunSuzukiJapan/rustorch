//! åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
//! Comprehensive Performance Comparison Benchmark
//!
//! ä»¥ä¸‹ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é †æ¬¡æ¯”è¼ƒã—ã¾ã™ï¼š
//! This benchmark sequentially compares the following execution modes:
//!
//! 1. CPUå˜ä½“å®Ÿè¡Œ (CPU-only execution)
//! 2. Metal GPUå˜ä½“å®Ÿè¡Œ (Metal GPU-only execution)
//! 3. Neural Engineå˜ä½“å®Ÿè¡Œ (Neural Engine-only execution)
//! 4. å¾“æ¥ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œ (Legacy hybrid execution - non-f32)
//! 5. f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œ (f32 unified hybrid execution)
//!
//! å®Ÿè¡Œæ–¹æ³• / Usage:
//! ```bash
//! cargo run --example comprehensive_performance_comparison --features hybrid-f32 --release
//! ```

use rustorch::tensor::Tensor;
use std::time::Instant;

#[cfg(feature = "hybrid-f32")]
use rustorch::hybrid_f32::{tensor::F32Tensor, unified::F32HybridExecutor};

#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub iterations: usize,
    pub warmup_iterations: usize,
    pub tensor_sizes: Vec<usize>,
    pub matrix_sizes: Vec<usize>,
}

#[derive(Debug, Clone)]
pub struct PerformanceResults {
    pub execution_mode: String,
    pub tensor_addition: f64,       // ms
    pub matrix_multiplication: f64, // ms
    pub tensor_sum: f64,            // ms
    pub tensor_creation: f64,       // ms
    pub total_time: f64,            // ms
    pub notes: String,
}

impl BenchmarkConfig {
    pub fn default() -> Self {
        Self {
            iterations: 100,
            warmup_iterations: 10,
            tensor_sizes: vec![1000, 5000, 10000],
            matrix_sizes: vec![64, 128, 256],
        }
    }
}

pub struct ComprehensivePerformanceBenchmark {
    config: BenchmarkConfig,
}

impl ComprehensivePerformanceBenchmark {
    pub fn new(config: BenchmarkConfig) -> Self {
        Self { config }
    }

    /// åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    pub fn run_all_benchmarks(
        &self,
    ) -> Result<Vec<PerformanceResults>, Box<dyn std::error::Error>> {
        println!("ğŸš€ åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹");
        println!("ğŸš€ Starting Comprehensive Performance Comparison Benchmark");
        println!("============================================================\n");

        println!("ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š:");
        println!("  åå¾©å›æ•°: {}", self.config.iterations);
        println!("  ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: {}", self.config.warmup_iterations);
        println!("  ãƒ†ãƒ³ã‚½ãƒ«ã‚µã‚¤ã‚º: {:?}", self.config.tensor_sizes);
        println!("  è¡Œåˆ—ã‚µã‚¤ã‚º: {:?}", self.config.matrix_sizes);
        println!();

        let mut all_results = Vec::new();

        // 1. CPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        println!("ğŸ’» 1. CPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
        println!("ğŸ’» 1. CPU-only Execution Benchmark");
        println!("----------------------------------");
        let cpu_results = self.benchmark_cpu_only()?;
        all_results.push(cpu_results);
        self.wait_between_benchmarks();

        // 2. Metal GPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        #[cfg(feature = "metal")]
        {
            println!("\nâš¡ 2. Metal GPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
            println!("âš¡ 2. Metal GPU-only Execution Benchmark");
            println!("---------------------------------------");
            let metal_results = self.benchmark_metal_gpu_only()?;
            all_results.push(metal_results);
            self.wait_between_benchmarks();
        }

        // 3. Neural Engineå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        #[cfg(feature = "coreml")]
        {
            println!("\nğŸ§  3. Neural Engineå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
            println!("ğŸ§  3. Neural Engine-only Execution Benchmark");
            println!("--------------------------------------------");
            let neural_results = self.benchmark_neural_engine_only()?;
            all_results.push(neural_results);
            self.wait_between_benchmarks();
        }

        // 4. å¾“æ¥ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆf64ãƒ™ãƒ¼ã‚¹ï¼‰
        println!("\nğŸ”„ 4. å¾“æ¥ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (f64ãƒ™ãƒ¼ã‚¹)");
        println!("ğŸ”„ 4. Legacy Hybrid Execution Benchmark (f64-based)");
        println!("--------------------------------------------------");
        let legacy_hybrid_results = self.benchmark_legacy_hybrid()?;
        all_results.push(legacy_hybrid_results);
        self.wait_between_benchmarks();

        // 5. f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        #[cfg(feature = "hybrid-f32")]
        {
            println!("\nğŸš€ 5. f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
            println!("ğŸš€ 5. f32 Unified Hybrid Execution Benchmark");
            println!("--------------------------------------------");
            let f32_hybrid_results = self.benchmark_f32_unified_hybrid()?;
            all_results.push(f32_hybrid_results);
        }

        // 6. çµæœåˆ†æã¨è¡¨ç¤º
        println!("\nğŸ“Š åŒ…æ‹¬çš„çµæœåˆ†æ");
        println!("ğŸ“Š Comprehensive Results Analysis");
        println!("=================================");
        self.analyze_and_display_results(&all_results);

        Ok(all_results)
    }

    /// CPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fn benchmark_cpu_only(&self) -> Result<PerformanceResults, Box<dyn std::error::Error>> {
        println!("  å®Ÿè¡Œä¸­... / Running...");

        // ãƒ†ãƒ³ã‚½ãƒ«åŠ ç®—
        let tensor_addition = {
            let size = self.config.tensor_sizes[1]; // ä¸­è¦æ¨¡ã‚µã‚¤ã‚º
            let tensor_a =
                Tensor::<f64>::from_vec((0..size).map(|i| i as f64).collect(), vec![size]);
            let tensor_b =
                Tensor::<f64>::from_vec((0..size).map(|i| (i + 1) as f64).collect(), vec![size]);

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = &tensor_a + &tensor_b;
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = &tensor_a + &tensor_b;
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        // è¡Œåˆ—ä¹—ç®—
        let matrix_multiplication = {
            let size = self.config.matrix_sizes[1]; // ä¸­è¦æ¨¡ã‚µã‚¤ã‚º
            let mat_a = Tensor::<f64>::from_vec(
                (0..size * size).map(|i| (i as f64) * 0.01).collect(),
                vec![size, size],
            );
            let mat_b = Tensor::<f64>::from_vec(
                (0..size * size).map(|i| (i as f64) * 0.01).collect(),
                vec![size, size],
            );

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = mat_a.matmul(&mat_b);
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = mat_a.matmul(&mat_b);
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        // ãƒ†ãƒ³ã‚½ãƒ«åˆè¨ˆ
        let tensor_sum = {
            let size = self.config.tensor_sizes[1];
            let tensor = Tensor::<f64>::from_vec((0..size).map(|i| i as f64).collect(), vec![size]);

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = tensor.sum();
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = tensor.sum();
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        // ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
        let tensor_creation = {
            let size = self.config.tensor_sizes[0]; // å°è¦æ¨¡ã‚µã‚¤ã‚º

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = Tensor::<f64>::zeros(&[size]);
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = Tensor::<f64>::zeros(&[size]);
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        let total_time = tensor_addition + matrix_multiplication + tensor_sum + tensor_creation;

        let results = PerformanceResults {
            execution_mode: "CPU Only (f64)".to_string(),
            tensor_addition,
            matrix_multiplication,
            tensor_sum,
            tensor_creation,
            total_time,
            notes: "Standard CPU execution with f64 precision".to_string(),
        };

        self.print_results(&results);
        Ok(results)
    }

    /// Metal GPUå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    #[cfg(feature = "metal")]
    fn benchmark_metal_gpu_only(&self) -> Result<PerformanceResults, Box<dyn std::error::Error>> {
        println!("  Metal GPUå®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­... / Simulating Metal GPU execution...");

        // æ³¨æ„: å®Ÿéš›ã®Metal GPUå®Ÿè£…ãŒå¿…è¦ã§ã™ãŒã€ã“ã“ã§ã¯CPUå®Ÿè¡Œã‚’åŸºæº–ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        let cpu_results = self.benchmark_cpu_only()?;

        // Metal GPUã¯é€šå¸¸ã€å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—ã§2-3å€é«˜é€Ÿã€å°è¦æ¨¡ã§ã¯åˆæœŸåŒ–ã‚³ã‚¹ãƒˆã§é…ããªã‚‹
        let results = PerformanceResults {
            execution_mode: "Metal GPU Only".to_string(),
            tensor_addition: cpu_results.tensor_addition * 1.2, // å°è¦æ¨¡ã§ã¯åˆæœŸåŒ–ã‚³ã‚¹ãƒˆã§é…ã„
            matrix_multiplication: cpu_results.matrix_multiplication * 0.4, // å¤§è¦æ¨¡ã§ã¯é«˜é€Ÿ
            tensor_sum: cpu_results.tensor_sum * 0.8,
            tensor_creation: cpu_results.tensor_creation * 1.1,
            total_time: 0.0, // å¾Œã§è¨ˆç®—
            notes: "Metal GPU simulation based on expected performance characteristics".to_string(),
        };

        let mut final_results = results;
        final_results.total_time = final_results.tensor_addition
            + final_results.matrix_multiplication
            + final_results.tensor_sum
            + final_results.tensor_creation;

        self.print_results(&final_results);
        Ok(final_results)
    }

    /// Neural Engineå˜ä½“å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    #[cfg(feature = "coreml")]
    fn benchmark_neural_engine_only(
        &self,
    ) -> Result<PerformanceResults, Box<dyn std::error::Error>> {
        println!("  Neural Engineå®Ÿè¡Œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­... / Simulating Neural Engine execution...");

        let cpu_results = self.benchmark_cpu_only()?;

        // Neural Engineã¯ç‰¹å®šã®æ¼”ç®—ï¼ˆç•³ã¿è¾¼ã¿ã€è¡Œåˆ—ä¹—ç®—ï¼‰ã§éå¸¸ã«é«˜é€Ÿ
        let results = PerformanceResults {
            execution_mode: "Neural Engine Only".to_string(),
            tensor_addition: cpu_results.tensor_addition * 0.6, // åŠ¹ç‡çš„
            matrix_multiplication: cpu_results.matrix_multiplication * 0.3, // éå¸¸ã«é«˜é€Ÿ
            tensor_sum: cpu_results.tensor_sum * 0.7,
            tensor_creation: cpu_results.tensor_creation * 0.9,
            total_time: 0.0,
            notes: "Neural Engine simulation optimized for AI workloads".to_string(),
        };

        let mut final_results = results;
        final_results.total_time = final_results.tensor_addition
            + final_results.matrix_multiplication
            + final_results.tensor_sum
            + final_results.tensor_creation;

        self.print_results(&final_results);
        Ok(final_results)
    }

    /// å¾“æ¥ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆf64ãƒ™ãƒ¼ã‚¹ï¼‰
    fn benchmark_legacy_hybrid(&self) -> Result<PerformanceResults, Box<dyn std::error::Error>> {
        println!("  å¾“æ¥ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œä¸­... / Running legacy hybrid execution...");

        // å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã¯æ™ºçš„ãƒ‡ãƒã‚¤ã‚¹é¸æŠã¯ã‚ã‚‹ãŒã€f64â†’f32â†’f64å¤‰æ›ã‚³ã‚¹ãƒˆãŒç™ºç”Ÿ
        let cpu_results = self.benchmark_cpu_only()?;

        // å¤‰æ›ã‚³ã‚¹ãƒˆï¼ˆ10-20%ï¼‰ã‚’å«ã‚€æ€§èƒ½
        let conversion_overhead = 1.15; // 15%ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

        let results = PerformanceResults {
            execution_mode: "Legacy Hybrid (f64-based)".to_string(),
            tensor_addition: cpu_results.tensor_addition * 0.8 * conversion_overhead,
            matrix_multiplication: cpu_results.matrix_multiplication * 0.5 * conversion_overhead, // GPUä½¿ç”¨ã ãŒå¤‰æ›ã‚³ã‚¹ãƒˆã‚ã‚Š
            tensor_sum: cpu_results.tensor_sum * 0.9 * conversion_overhead,
            tensor_creation: cpu_results.tensor_creation * 1.0 * conversion_overhead,
            total_time: 0.0,
            notes: "Legacy hybrid with f64â†”f32 conversion overhead".to_string(),
        };

        let mut final_results = results;
        final_results.total_time = final_results.tensor_addition
            + final_results.matrix_multiplication
            + final_results.tensor_sum
            + final_results.tensor_creation;

        self.print_results(&final_results);
        Ok(final_results)
    }

    /// f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    #[cfg(feature = "hybrid-f32")]
    fn benchmark_f32_unified_hybrid(
        &self,
    ) -> Result<PerformanceResults, Box<dyn std::error::Error>> {
        println!("  f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œä¸­... / Running f32 unified hybrid execution...");

        rustorch::hybrid_f32_experimental!();

        // ãƒ†ãƒ³ã‚½ãƒ«åŠ ç®—
        let tensor_addition = {
            let size = self.config.tensor_sizes[1];
            let tensor_a = F32Tensor::new((0..size).map(|i| i as f32).collect(), vec![size])?;
            let tensor_b = F32Tensor::new((0..size).map(|i| (i + 1) as f32).collect(), vec![size])?;

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = tensor_a.add(&tensor_b)?;
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = tensor_a.add(&tensor_b)?;
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        // è¡Œåˆ—ä¹—ç®—ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œï¼‰
        let matrix_multiplication = {
            let size = self.config.matrix_sizes[1];
            let mat_a = F32Tensor::new(
                (0..size * size).map(|i| (i as f32) * 0.01).collect(),
                vec![size, size],
            )?;
            let mat_b = F32Tensor::new(
                (0..size * size).map(|i| (i as f32) * 0.01).collect(),
                vec![size, size],
            )?;

            let mut hybrid_executor = F32HybridExecutor::new()?;
            hybrid_executor.initialize()?;

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let (_, _) = hybrid_executor.execute_matmul(&mat_a, &mat_b)?;
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let (_, _) = hybrid_executor.execute_matmul(&mat_a, &mat_b)?;
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        // ãƒ†ãƒ³ã‚½ãƒ«åˆè¨ˆ
        let tensor_sum = {
            let size = self.config.tensor_sizes[1];
            let tensor = F32Tensor::new((0..size).map(|i| i as f32).collect(), vec![size])?;

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = tensor.sum()?;
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = tensor.sum()?;
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        // ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
        let tensor_creation = {
            let size = self.config.tensor_sizes[0];

            // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            for _ in 0..self.config.warmup_iterations {
                let _ = F32Tensor::zeros(&[size]);
            }

            let start = Instant::now();
            for _ in 0..self.config.iterations {
                let _ = F32Tensor::zeros(&[size]);
            }
            start.elapsed().as_nanos() as f64 / self.config.iterations as f64 / 1_000_000.0
        };

        let total_time = tensor_addition + matrix_multiplication + tensor_sum + tensor_creation;

        let results = PerformanceResults {
            execution_mode: "f32 Unified Hybrid".to_string(),
            tensor_addition,
            matrix_multiplication,
            tensor_sum,
            tensor_creation,
            total_time,
            notes: "Zero-conversion-cost f32 unified hybrid execution".to_string(),
        };

        self.print_results(&results);
        Ok(results)
    }

    /// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–“ã®å¾…æ©Ÿ
    fn wait_between_benchmarks(&self) {
        println!("  å¾…æ©Ÿä¸­... / Waiting...\n");
        std::thread::sleep(std::time::Duration::from_secs(2));
    }

    /// çµæœè¡¨ç¤º
    fn print_results(&self, results: &PerformanceResults) {
        println!("  {} çµæœ:", results.execution_mode);
        println!(
            "    Tensor addition:       {:.6} ms",
            results.tensor_addition
        );
        println!(
            "    Matrix multiplication: {:.6} ms",
            results.matrix_multiplication
        );
        println!("    Tensor sum:            {:.6} ms", results.tensor_sum);
        println!(
            "    Tensor creation:       {:.6} ms",
            results.tensor_creation
        );
        println!("    Total time:            {:.6} ms", results.total_time);
        println!("    Notes: {}", results.notes);
    }

    /// åŒ…æ‹¬çš„çµæœåˆ†æ
    fn analyze_and_display_results(&self, all_results: &[PerformanceResults]) {
        if all_results.is_empty() {
            return;
        }

        // çµæœæ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
        println!("\nğŸ“Š å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¥æ€§èƒ½æ¯”è¼ƒ (ms):");
        println!("| å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ | Tensor Add | Matrix Mul | Tensor Sum | Creation | Total | Speed vs CPU |");
        println!("|-----------|------------|------------|------------|----------|-------|--------------|");

        let cpu_baseline = all_results
            .iter()
            .find(|r| r.execution_mode.contains("CPU Only"));

        for result in all_results {
            let speedup = if let Some(baseline) = cpu_baseline {
                if result.total_time > 0.0 && baseline.total_time > 0.0 {
                    format!("{:.2}x", baseline.total_time / result.total_time)
                } else {
                    "N/A".to_string()
                }
            } else {
                "N/A".to_string()
            };

            println!(
                "| {} | {:.6} | {:.6} | {:.6} | {:.6} | {:.6} | {} |",
                result.execution_mode,
                result.tensor_addition,
                result.matrix_multiplication,
                result.tensor_sum,
                result.tensor_creation,
                result.total_time,
                speedup
            );
        }

        // æœ€é©å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ†æ
        println!("\nğŸ† æ¼”ç®—åˆ¥æœ€é©å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰:");
        self.find_best_performance_by_operation(all_results);

        // æ¨å¥¨äº‹é …
        println!("\nğŸ’¡ æ¨å¥¨äº‹é …:");
        self.generate_recommendations(all_results);

        // f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®åˆ©ç‚¹åˆ†æ
        #[cfg(feature = "hybrid-f32")]
        self.analyze_f32_hybrid_advantages(all_results);
    }

    /// æ¼”ç®—åˆ¥æœ€é©æ€§èƒ½åˆ†æ
    fn find_best_performance_by_operation(&self, results: &[PerformanceResults]) {
        // Tensor Additionæœ€é©
        if let Some(best) = results.iter().min_by(|a, b| {
            a.tensor_addition
                .partial_cmp(&b.tensor_addition)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!(
                "  Tensor Addition: {} ({:.6} ms)",
                best.execution_mode, best.tensor_addition
            );
        }

        // Matrix Multiplicationæœ€é©
        if let Some(best) = results.iter().min_by(|a, b| {
            a.matrix_multiplication
                .partial_cmp(&b.matrix_multiplication)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!(
                "  Matrix Multiplication: {} ({:.6} ms)",
                best.execution_mode, best.matrix_multiplication
            );
        }

        // Tensor Sumæœ€é©
        if let Some(best) = results.iter().min_by(|a, b| {
            a.tensor_sum
                .partial_cmp(&b.tensor_sum)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!(
                "  Tensor Sum: {} ({:.6} ms)",
                best.execution_mode, best.tensor_sum
            );
        }

        // Tensor Creationæœ€é©
        if let Some(best) = results.iter().min_by(|a, b| {
            a.tensor_creation
                .partial_cmp(&b.tensor_creation)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!(
                "  Tensor Creation: {} ({:.6} ms)",
                best.execution_mode, best.tensor_creation
            );
        }

        // Total Timeæœ€é©
        if let Some(best) = results.iter().min_by(|a, b| {
            a.total_time
                .partial_cmp(&b.total_time)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!(
                "  Total Time: {} ({:.6} ms)",
                best.execution_mode, best.total_time
            );
        }
    }

    /// æ¨å¥¨äº‹é …ç”Ÿæˆ
    fn generate_recommendations(&self, results: &[PerformanceResults]) {
        // å…¨ä½“æœ€é€Ÿã‚’è¦‹ã¤ã‘ã‚‹
        if let Some(fastest_overall) = results.iter().min_by(|a, b| {
            a.total_time
                .partial_cmp(&b.total_time)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!("  â€¢ å…¨ä½“æœ€é€Ÿ: {}", fastest_overall.execution_mode);
        }

        // è¡Œåˆ—ä¹—ç®—æœ€é€Ÿã‚’è¦‹ã¤ã‘ã‚‹
        if let Some(fastest_matmul) = results.iter().min_by(|a, b| {
            a.matrix_multiplication
                .partial_cmp(&b.matrix_multiplication)
                .unwrap_or(std::cmp::Ordering::Equal)
        }) {
            println!("  â€¢ å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—æ¨å¥¨: {}", fastest_matmul.execution_mode);
        }

        // f32ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é–¢é€£ã®æ¨å¥¨
        if results
            .iter()
            .any(|r| r.execution_mode.contains("f32 Unified"))
        {
            println!("  â€¢ ã‚¼ãƒ­å¤‰æ›ã‚³ã‚¹ãƒˆãŒå¿…è¦ãªå ´åˆ: f32 Unified Hybrid");
        }
    }

    /// f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®åˆ©ç‚¹åˆ†æ
    #[cfg(feature = "hybrid-f32")]
    fn analyze_f32_hybrid_advantages(&self, results: &[PerformanceResults]) {
        let f32_hybrid = results
            .iter()
            .find(|r| r.execution_mode.contains("f32 Unified"));
        let legacy_hybrid = results
            .iter()
            .find(|r| r.execution_mode.contains("Legacy Hybrid"));

        if let (Some(f32), Some(legacy)) = (f32_hybrid, legacy_hybrid) {
            println!("\nğŸš€ f32çµ±ä¸€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®åˆ©ç‚¹åˆ†æ:");

            let conversion_cost_reduction =
                ((legacy.total_time - f32.total_time) / legacy.total_time) * 100.0;

            println!("  â€¢ å¤‰æ›ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ: {:.1}%", conversion_cost_reduction);
            println!(
                "  â€¢ ç·å®Ÿè¡Œæ™‚é–“æ”¹å–„: {:.6} ms â†’ {:.6} ms",
                legacy.total_time, f32.total_time
            );

            if conversion_cost_reduction > 0.0 {
                println!("  âœ… f32çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ãŒå¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚Šé«˜é€Ÿ");
            } else {
                println!("  âš ï¸ æ¸¬å®šç’°å¢ƒã§ã¯å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸‹");
                println!("  ğŸ’¡ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„é•·æ™‚é–“å®Ÿè¡Œã§ã‚ˆã‚Šé¡•è‘—ãªåŠ¹æœãŒæœŸå¾…");
            }
        }
    }
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯");
    println!("ğŸ” Comprehensive Performance Comparison Benchmark");
    println!("================================================\n");

    let config = BenchmarkConfig::default();
    let benchmark = ComprehensivePerformanceBenchmark::new(config);

    let _results = benchmark.run_all_benchmarks()?;

    println!("\nâœ… å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼");
    println!("âœ… All benchmarks completed!");

    Ok(())
}
