//! Distributed training demonstration
//! ÂàÜÊï£Â≠¶Áøí„Éá„É¢
//!
//! This example demonstrates how to use RusTorch's distributed training capabilities
//! with PyTorch-compatible APIs.
//!
//! „Åì„ÅÆ‰æã„Åß„ÅØ„ÄÅPyTorch‰∫íÊèõAPI„Çí‰ΩøÁî®„Åó„ÅüRusTorch„ÅÆÂàÜÊï£Â≠¶ÁøíÊ©üËÉΩ„ÅÆ
//! ‰ΩøÁî®ÊñπÊ≥ï„ÇíÂÆüÊºî„Åó„Åæ„Åô„ÄÇ

use rustorch::{
    autograd::Variable,
    distributed::{self, DistributedBackend, DistributedDataParallel},
    error::RusTorchResult,
    nn::{Linear, Sequential},
    tensor::Tensor,
};
use std::time::Duration;

fn main() -> RusTorchResult<()> {
    println!("üöÄ RusTorch Distributed Training Demo");
    println!("=====================================");

    // Initialize distributed training
    setup_distributed_training()?;

    // Create and train model
    let model = create_model()?;
    let ddp_model = setup_ddp_model(model)?;

    // Run training simulation
    run_training_simulation(&ddp_model)?;

    // Cleanup
    distributed::destroy_process_group()?;

    println!("‚úÖ Distributed training demo completed successfully!");
    Ok(())
}

/// Setup distributed training environment
/// ÂàÜÊï£Â≠¶ÁøíÁí∞Â¢É„Çí„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
fn setup_distributed_training() -> RusTorchResult<()> {
    println!("üì° Setting up distributed training...");

    // Set default environment variables if not set
    if std::env::var("RANK").is_err() {
        std::env::set_var("RANK", "0");
    }
    if std::env::var("WORLD_SIZE").is_err() {
        std::env::set_var("WORLD_SIZE", "1");
    }
    if std::env::var("MASTER_ADDR").is_err() {
        std::env::set_var("MASTER_ADDR", "localhost");
    }
    if std::env::var("MASTER_PORT").is_err() {
        std::env::set_var("MASTER_PORT", "29500");
    }

    // Initialize process group
    distributed::init_process_group(
        DistributedBackend::TCP, // Use TCP for simplicity in demo
        None,                    // Use environment variables
        None,                    // Use environment variables
        None,                    // Use environment variables
        Some(Duration::from_secs(30)),
    )?;

    println!("  ‚úì Process group initialized");
    println!("  ‚úì Rank: {:?}", distributed::get_rank());
    println!("  ‚úì World size: {:?}", distributed::get_world_size());

    Ok(())
}

/// Create a simple neural network model
/// „Ç∑„É≥„Éó„É´„Å™„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„É¢„Éá„É´„Çí‰ΩúÊàê
fn create_model() -> RusTorchResult<Sequential<f32>> {
    println!("üß† Creating neural network model...");

    let mut model = Sequential::<f32>::new();
    model.add_module(Linear::<f32>::new(784, 256));
    model.add_module(Linear::<f32>::new(256, 128));
    model.add_module(Linear::<f32>::new(128, 10));

    println!("  ‚úì Model created with 3 linear layers");
    println!("  ‚úì Architecture: 784 ‚Üí 256 ‚Üí 128 ‚Üí 10");

    Ok(model)
}

/// Setup DistributedDataParallel wrapper
/// DistributedDataParallel„É©„ÉÉ„Éë„Éº„Çí„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
fn setup_ddp_model(
    model: Sequential<f32>,
) -> RusTorchResult<DistributedDataParallel<f32, Sequential<f32>>> {
    println!("‚öôÔ∏è  Setting up DistributedDataParallel...");

    let device_ids = vec![0]; // Use device 0 for demo
    let ddp_model = distributed::wrap_module(model, Some(device_ids))?;

    println!("  ‚úì DDP wrapper created");
    println!("  ‚úì Device IDs: {:?}", ddp_model.device_ids());

    Ok(ddp_model)
}

/// Run training simulation
/// Â≠¶Áøí„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„ÇíÂÆüË°å
fn run_training_simulation(
    ddp_model: &DistributedDataParallel<f32, Sequential<f32>>,
) -> RusTorchResult<()> {
    println!("üèÉ Running training simulation...");

    let batch_size = 32;
    let num_batches = 5;

    for epoch in 1..=3 {
        println!("  üìä Epoch {}/3", epoch);

        for batch in 1..=num_batches {
            // Create synthetic training data
            let input: Tensor<f32> = Tensor::randn(&[batch_size, 784]);
            let _target: Tensor<f32> = Tensor::randn(&[batch_size, 10]);

            // Forward pass
            let input_var = Variable::new(input, false);
            let output = ddp_model.forward(&input_var)?;
            // Simple shape verification without complex borrowing
            let expected_shape = vec![batch_size, 10];
            assert!(output.data().read().unwrap().shape() == &expected_shape[..]);

            // Simulate backward pass and gradient synchronization
            ddp_model.sync_gradients()?;

            if batch % 2 == 0 {
                println!("    ‚úì Batch {}/{} completed", batch, num_batches);
            }
        }
    }

    Ok(())
}

/// Demonstrate async gradient synchronization
/// ÈùûÂêåÊúüÂãæÈÖçÂêåÊúü„ÅÆ„Éá„É¢
#[cfg(feature = "nccl")]
fn demo_async_gradient_sync() -> RusTorchResult<()> {
    use rustorch::distributed::async_gradient::{AsyncConfig, AsyncGradientSynchronizer, Priority};

    println!("‚ö° Demonstrating async gradient synchronization...");

    let config = AsyncConfig {
        max_concurrent_ops: 8,
        sync_timeout: Duration::from_secs(10),
        enable_compression: false,
        compression_threshold: 1024 * 1024,
        enable_bucketing: true,
        bucket_size_mb: 25,
    };

    let synchronizer = AsyncGradientSynchronizer::new(config)?;

    // Submit several gradients for async sync
    let mut request_ids = Vec::new();
    for i in 0..5 {
        let gradient: Tensor<f32> = Tensor::randn(&[100, 100]);
        let param_name = format!("layer_{}", i);

        let request_id = synchronizer.submit_gradient(param_name, gradient, Priority::Normal)?;
        request_ids.push(request_id);
    }

    // Wait for completion
    for request_id in request_ids {
        synchronizer.wait_for_completion(request_id, Duration::from_secs(5))?;
    }

    println!("  ‚úì Async gradient sync completed");
    Ok(())
}

/// Benchmark different communication patterns
/// Áï∞„Å™„ÇãÈÄö‰ø°„Éë„Çø„Éº„É≥„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ
fn benchmark_communication() -> RusTorchResult<()> {
    println!("‚è±Ô∏è  Benchmarking communication patterns...");

    let tensor_sizes = vec![
        (vec![100], "small"),
        (vec![1000], "medium"),
        (vec![10000], "large"),
    ];

    for (shape, label) in tensor_sizes {
        let tensor: Tensor<f32> = Tensor::randn(&shape);
        let start = std::time::Instant::now();

        // Test all-reduce
        let mut tensor_copy = tensor.clone();
        distributed::all_reduce(&mut tensor_copy, distributed::ReduceOp::Sum, None, false)?;
        let all_reduce_time = start.elapsed();

        // Test broadcast
        let start = std::time::Instant::now();
        let mut tensor_copy = tensor.clone();
        distributed::broadcast(&mut tensor_copy, 0, None, false)?;
        let broadcast_time = start.elapsed();

        // Test all-gather
        let start = std::time::Instant::now();
        let mut tensor_list = Vec::new();
        distributed::all_gather(&mut tensor_list, &tensor, None, false)?;
        let all_gather_time = start.elapsed();

        println!(
            "  {} tensor ({} elements):",
            label,
            shape.iter().product::<usize>()
        );
        println!("    All-reduce: {:?}", all_reduce_time);
        println!("    Broadcast:  {:?}", broadcast_time);
        println!("    All-gather: {:?}", all_gather_time);
    }

    Ok(())
}

/// Demonstrate different backends
/// Áï∞„Å™„Çã„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„ÅÆ„Éá„É¢
fn demo_backends() -> RusTorchResult<()> {
    println!("üîß Testing different communication backends...");

    let backends = vec![
        (DistributedBackend::TCP, "TCP"),
        (DistributedBackend::Gloo, "Gloo"),
    ];

    for (backend, name) in backends {
        println!("  Testing {} backend:", name);

        // Clean up any previous state
        let _ = distributed::destroy_process_group();

        // Try to initialize with this backend
        let result = distributed::init_process_group(
            backend,
            Some("tcp://localhost:29500"),
            Some(1),
            Some(0),
            Some(Duration::from_secs(5)),
        );

        match result {
            Ok(()) => {
                println!("    ‚úì {} backend initialized successfully", name);

                // Test basic operation
                let mut tensor: Tensor<f32> = Tensor::ones(&[10]);
                let op_result =
                    distributed::all_reduce(&mut tensor, distributed::ReduceOp::Sum, None, false);

                if op_result.is_ok() {
                    println!("    ‚úì All-reduce operation successful");
                } else {
                    println!("    ‚ö†Ô∏è  All-reduce operation failed: {:?}", op_result);
                }
            }
            Err(e) => {
                println!("    ‚ùå {} backend failed to initialize: {}", name, e);
            }
        }
    }

    Ok(())
}

/// Advanced features demonstration
/// È´òÂ∫¶Ê©üËÉΩ„ÅÆ„Éá„É¢
fn demo_advanced_features() -> RusTorchResult<()> {
    println!("üöÄ Demonstrating advanced distributed features...");

    // Test custom process groups
    let ranks = vec![0];
    let custom_group = distributed::new_group(ranks, Some(Duration::from_secs(10)), None)?;
    println!(
        "  ‚úì Custom process group created (size: {})",
        custom_group.size()
    );

    // Test monitoring
    let stats = distributed::monitoring::get_communication_stats()?;
    println!("  ‚úì Communication stats retrieved");
    println!("    Operations: {}", stats.total_operations);
    println!("    Avg latency: {:.2}ms", stats.average_latency_ms);

    #[cfg(feature = "nccl")]
    {
        // Test NCCL-specific features
        use rustorch::distributed::nccl_integration::NCCLOps;

        let nccl_config = NCCLOps::get_optimal_config(4, 16.0);
        println!("  ‚úì NCCL optimal config generated");
        println!("    Bucket size: {}MB", nccl_config.bucket_size_mb);
        println!("    Streams: {}", nccl_config.num_streams);
        println!("    Compression: {}", nccl_config.compression_enabled);
    }

    #[cfg(feature = "nccl")]
    {
        // Test async features
        demo_async_gradient_sync()?;
    }

    Ok(())
}

/// Print system information
/// „Ç∑„Çπ„ÉÜ„É†ÊÉÖÂ†±„ÇíÂç∞Âà∑
fn print_system_info() {
    println!("üíª System Information:");
    println!("  OS: {}", std::env::consts::OS);
    println!("  Architecture: {}", std::env::consts::ARCH);

    if let Ok(hostname) = hostname::get() {
        println!("  Hostname: {:?}", hostname);
    }

    #[cfg(feature = "cuda")]
    {
        use rustorch::gpu::DeviceType;
        let cuda_available = DeviceType::Cuda(0).is_available();
        println!("  CUDA available: {}", cuda_available);
    }

    #[cfg(feature = "nccl")]
    {
        println!("  NCCL support: enabled");
    }
    #[cfg(not(feature = "nccl"))]
    {
        println!("  NCCL support: disabled");
    }

    println!();
}
