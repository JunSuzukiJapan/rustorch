//! Distributed training demonstration
//! åˆ†æ•£å­¦ç¿’ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
//! 
//! This example demonstrates how to use RusTorch's distributed training capabilities
//! including data parallel training, model parallel training, and cluster management.

use rustorch::tensor::Tensor;
use rustorch::autograd::Variable;
use rustorch::nn::{Linear, Module};
use rustorch::distributed::{
    init_process_group, get_world_size, get_rank, DistributedBackend, ProcessGroup,
    data_parallel::DataParallel,
    model_parallel::ModelParallel,
    optimizer::{DistributedOptimizer, GradientSyncStrategy, DistributedOptimizerBuilder},
    backends::{BackendFactory, GlooBackend},
    cluster::{ClusterManager, ClusterConfig, NodeInfo, NodeCapabilities, NodeStatus, ClusterTopology, FaultToleranceConfig},
};
use std::sync::Arc;
use std::any::Any;

/// Simple neural network for demonstration
/// ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚·ãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
#[derive(Debug)]
struct SimpleNet {
    fc1: Linear<f32>,
    fc2: Linear<f32>,
    fc3: Linear<f32>,
}

impl SimpleNet {
    fn new() -> Self {
        Self {
            fc1: Linear::new(784, 256),
            fc2: Linear::new(256, 128),
            fc3: Linear::new(128, 10),
        }
    }
}

impl Module<f32> for SimpleNet {
    fn forward(&self, input: &Variable<f32>) -> Variable<f32> {
        let x = self.fc1.forward(input);
        // Note: ReLU activation would be applied here in a real implementation
        let x = self.fc2.forward(&x);
        // Note: ReLU activation would be applied here in a real implementation
        self.fc3.forward(&x)
    }
    
    fn parameters(&self) -> Vec<Variable<f32>> {
        let mut params = Vec::new();
        params.extend(self.fc1.parameters());
        params.extend(self.fc2.parameters());
        params.extend(self.fc3.parameters());
        params
    }
    
    fn as_any(&self) -> &dyn Any {
        self
    }
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ RusTorch Distributed Training Demo");
    println!("=====================================");

    // Demo 1: Basic Process Group Setup
    // ãƒ‡ãƒ¢1: åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    demo_process_group_setup()?;

    // Demo 2: Data Parallel Training
    // ãƒ‡ãƒ¢2: ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å­¦ç¿’
    demo_data_parallel_training()?;

    // Demo 3: Model Parallel Training
    // ãƒ‡ãƒ¢3: ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’
    demo_model_parallel_training()?;

    // Demo 4: Distributed Optimizer
    // ãƒ‡ãƒ¢4: åˆ†æ•£ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    demo_distributed_optimizer()?;

    // Demo 5: Cluster Management
    // ãƒ‡ãƒ¢5: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†
    demo_cluster_management()?;

    // Demo 6: Fault Tolerance
    // ãƒ‡ãƒ¢6: éšœå®³è€æ€§
    demo_fault_tolerance()?;

    println!("\nâœ… All distributed training demos completed successfully!");
    Ok(())
}

/// Demonstrate process group setup
/// ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
fn demo_process_group_setup() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ“¡ Demo 1: Process Group Setup");
    println!("------------------------------");

    // Initialize process group with TCP backend
    // TCPãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆæœŸåŒ–
    let result = init_process_group(
        DistributedBackend::TCP,
        0,    // rank
        4,    // world_size
        "localhost".to_string(),
        12345,
    );

    match result {
        Ok(_) => {
            println!("âœ… Process group initialized successfully");
            println!("   Backend: TCP");
            println!("   Rank: {}", get_rank().unwrap_or(0));
            println!("   World Size: {}", get_world_size().unwrap_or(1));
        },
        Err(e) => {
            println!("âš ï¸  Process group initialization failed: {:?}", e);
            println!("   This is expected in single-process demo mode");
        }
    }

    // Demonstrate different backends
    // ç•°ãªã‚‹ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    let backends = [
        DistributedBackend::TCP,
        DistributedBackend::Gloo,
        #[cfg(feature = "nccl")]
        DistributedBackend::NCCL,
    ];

    for backend in &backends {
        println!("   Supported backend: {:?}", backend);
    }

    Ok(())
}

/// Demonstrate data parallel training
/// ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å­¦ç¿’ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
fn demo_data_parallel_training() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ”„ Demo 2: Data Parallel Training");
    println!("----------------------------------");

    // Create model
    // ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    let model = SimpleNet::new();
    println!("âœ… Created simple neural network model");

    // Create process group for data parallel
    // ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ç”¨ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
    let process_group = ProcessGroup::new(
        0, 4, DistributedBackend::Gloo,
        "localhost".to_string(), 12345
    );

    // Create data parallel wrapper
    // ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆ
    let mut data_parallel = DataParallel::new(
        Box::new(model),
        vec![0, 1, 2, 3], // device_ids
        process_group,
    )?;

    println!("âœ… Created data parallel wrapper");
    println!("   Devices: [0, 1, 2, 3]");
    println!("   Sync strategy: {:?}", data_parallel.get_sync_strategy());

    // Create sample input batch
    // ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ãƒãƒƒãƒã‚’ä½œæˆ
    let batch_size = 32;
    let input = Variable::new(Tensor::zeros(&[batch_size, 784]));
    let target = Variable::new(Tensor::zeros(&[batch_size, 10]));

    println!("âœ… Created sample batch");
    println!("   Input shape: {:?}", input.data().shape());
    println!("   Target shape: {:?}", target.data().shape());

    // Forward pass
    // ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    let output = data_parallel.forward(&input);
    println!("âœ… Forward pass completed");
    println!("   Output shape: {:?}", output.data().shape());

    // Simulate gradient synchronization
    // å‹¾é…åŒæœŸã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    match data_parallel.sync_gradients() {
        Ok(_) => println!("âœ… Gradient synchronization completed"),
        Err(e) => println!("âš ï¸  Gradient sync failed (expected in demo): {:?}", e),
    }

    Ok(())
}

/// Demonstrate model parallel training
/// ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
fn demo_model_parallel_training() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ§© Demo 3: Model Parallel Training");
    println!("-----------------------------------");

    // Create process group for model parallel
    // ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ç”¨ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
    let process_group = ProcessGroup::new(
        0, 4, DistributedBackend::Gloo,
        "localhost".to_string(), 12346
    );

    // Create model parallel manager
    // ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
    let mut model_parallel = ModelParallel::new(process_group)?;

    println!("âœ… Created model parallel manager");

    // Add model partitions
    // ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
    let fc1 = Linear::new(784, 256);
    let fc2 = Linear::new(256, 128);
    let fc3 = Linear::new(128, 10);

    model_parallel.add_partition(0, Box::new(fc1), vec![1])?;
    model_parallel.add_partition(1, Box::new(fc2), vec![2])?;
    model_parallel.add_partition(2, Box::new(fc3), vec![])?;

    println!("âœ… Added model partitions");
    println!("   Partition 0: Linear(784 -> 256) -> Device 1");
    println!("   Partition 1: Linear(256 -> 128) -> Device 2");
    println!("   Partition 2: Linear(128 -> 10) -> Output");

    // Create sample input
    // ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ã‚’ä½œæˆ
    let input = Variable::new(Tensor::zeros(&[32, 784]));
    println!("âœ… Created sample input: {:?}", input.data().shape());

    // Forward pass through partitioned model
    // ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–ãƒ¢ãƒ‡ãƒ«ã§ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
    match model_parallel.forward(&input) {
        Ok(output) => {
            println!("âœ… Model parallel forward pass completed");
            println!("   Output shape: {:?}", output.data().shape());
        },
        Err(e) => {
            println!("âš ï¸  Model parallel forward failed (expected in demo): {:?}", e);
        }
    }

    // Demonstrate pipeline parallelism
    // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—æ€§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    let pipeline_config = model_parallel.create_pipeline_config(4, 2)?;
    println!("âœ… Created pipeline configuration");
    println!("   Micro-batches: {}", pipeline_config.num_microbatches);
    println!("   Pipeline stages: {}", pipeline_config.num_stages);

    Ok(())
}

/// Demonstrate distributed optimizer
/// åˆ†æ•£ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
fn demo_distributed_optimizer() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nâš¡ Demo 4: Distributed Optimizer");
    println!("--------------------------------");

    // Create process group
    // ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
    let process_group = ProcessGroup::new(
        0, 4, DistributedBackend::Gloo,
        "localhost".to_string(), 12347
    );

    // Create backend
    // ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½œæˆ
    let backend = Arc::new(GlooBackend::new(process_group)?);
    println!("âœ… Created Gloo communication backend");

    // Create distributed SGD optimizer
    // åˆ†æ•£SGDã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    let mut sgd_optimizer = DistributedOptimizerBuilder::sgd(0.01, 0.9, 0.0001)
        .backend(backend.clone())
        .sync_strategy(GradientSyncStrategy::Synchronous)
        .bucket_size(25 * 1024 * 1024) // 25MB buckets
        .build()?;

    println!("âœ… Created distributed SGD optimizer");
    println!("   Learning rate: 0.01");
    println!("   Momentum: 0.9");
    println!("   Sync strategy: Synchronous");
    println!("   Bucket size: 25MB");

    // Create distributed Adam optimizer
    // åˆ†æ•£Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ
    let mut adam_optimizer = DistributedOptimizerBuilder::adam(0.001, 0.9, 0.999, 1e-8, 0.0001)
        .backend(backend.clone())
        .sync_strategy(GradientSyncStrategy::LocalSGD { sync_frequency: 10 })
        .build()?;

    println!("âœ… Created distributed Adam optimizer");
    println!("   Learning rate: 0.001");
    println!("   Sync strategy: Local SGD (freq=10)");

    // Demonstrate different sync strategies
    // ç•°ãªã‚‹åŒæœŸæˆ¦ç•¥ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    let strategies = [
        GradientSyncStrategy::Synchronous,
        GradientSyncStrategy::Asynchronous,
        GradientSyncStrategy::LocalSGD { sync_frequency: 5 },
        GradientSyncStrategy::Compressed { compression_ratio: 0.1 },
        GradientSyncStrategy::Hierarchical,
    ];

    for strategy in &strategies {
        println!("   Supported strategy: {:?}", strategy);
    }

    // Simulate training step
    // å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    let mut parameters = vec![
        Tensor::zeros(&[256, 784]),
        Tensor::zeros(&[256]),
        Tensor::zeros(&[128, 256]),
        Tensor::zeros(&[128]),
    ];

    match sgd_optimizer.step(&mut parameters) {
        Ok(_) => println!("âœ… Distributed optimizer step completed"),
        Err(e) => println!("âš ï¸  Optimizer step failed (expected in demo): {:?}", e),
    }

    println!("   Step count: {}", sgd_optimizer.step_count());

    Ok(())
}

/// Demonstrate cluster management
/// ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
fn demo_cluster_management() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ–¥ï¸  Demo 5: Cluster Management");
    println!("------------------------------");

    // Create cluster configuration
    // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã‚’ä½œæˆ
    let node1 = NodeInfo {
        node_id: 0,
        address: "192.168.1.100".to_string(),
        port: 12345,
        gpu_count: 4,
        capabilities: NodeCapabilities {
            memory_gb: 64.0,
            cpu_cores: 16,
            gpu_memory_gb: 32.0,
            network_bandwidth_gbps: 10.0,
        },
        status: NodeStatus::Available,
    };

    let node2 = NodeInfo {
        node_id: 1,
        address: "192.168.1.101".to_string(),
        port: 12345,
        gpu_count: 8,
        capabilities: NodeCapabilities {
            memory_gb: 128.0,
            cpu_cores: 32,
            gpu_memory_gb: 80.0,
            network_bandwidth_gbps: 25.0,
        },
        status: NodeStatus::Available,
    };

    let cluster_config = ClusterConfig {
        master_addr: "192.168.1.1".to_string(),
        master_port: 12345,
        worker_nodes: vec![node1, node2],
        topology: ClusterTopology::Flat,
        fault_tolerance: FaultToleranceConfig {
            enable_failover: true,
            heartbeat_interval: 30,
            node_timeout: 120,
            max_retries: 3,
            checkpoint_frequency: 100,
        },
    };

    println!("âœ… Created cluster configuration");
    println!("   Master: {}:{}", cluster_config.master_addr, cluster_config.master_port);
    println!("   Worker nodes: {}", cluster_config.worker_nodes.len());
    println!("   Topology: {:?}", cluster_config.topology);
    println!("   Fault tolerance: enabled");

    // Create cluster manager
    // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
    let mut cluster_manager = ClusterManager::new(cluster_config)?;
    println!("âœ… Created cluster manager");

    // Get cluster status
    // ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—
    let status = cluster_manager.get_cluster_status();
    println!("âœ… Cluster status:");
    println!("   Total nodes: {}", status.total_nodes);
    println!("   Available nodes: {}", status.available_nodes);
    println!("   Total GPUs: {}", status.total_gpus);
    println!("   Active jobs: {}", status.active_jobs);

    // Create process group for distributed job
    // åˆ†æ•£ã‚¸ãƒ§ãƒ–ç”¨ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
    match cluster_manager.create_process_group(
        "training_job_1".to_string(),
        4,
        DistributedBackend::Gloo,
    ) {
        Ok(pg) => {
            println!("âœ… Created process group for distributed job");
            println!("   Job ID: training_job_1");
            println!("   World size: {}", pg.world_size);
            println!("   Backend: {:?}", pg.backend);
        },
        Err(e) => {
            println!("âš ï¸  Process group creation failed (expected in demo): {:?}", e);
        }
    }

    Ok(())
}

/// Demonstrate fault tolerance
/// éšœå®³è€æ€§ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
fn demo_fault_tolerance() -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ›¡ï¸  Demo 6: Fault Tolerance");
    println!("---------------------------");

    // Create cluster with fault tolerance enabled
    // éšœå®³è€æ€§ã‚’æœ‰åŠ¹ã«ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ä½œæˆ
    let fault_config = FaultToleranceConfig {
        enable_failover: true,
        heartbeat_interval: 10, // 10 seconds
        node_timeout: 30,       // 30 seconds
        max_retries: 5,
        checkpoint_frequency: 50, // Every 50 steps
    };

    println!("âœ… Fault tolerance configuration:");
    println!("   Failover: enabled");
    println!("   Heartbeat interval: {} seconds", fault_config.heartbeat_interval);
    println!("   Node timeout: {} seconds", fault_config.node_timeout);
    println!("   Max retries: {}", fault_config.max_retries);
    println!("   Checkpoint frequency: {} steps", fault_config.checkpoint_frequency);

    // Simulate node failure and recovery
    // ãƒãƒ¼ãƒ‰éšœå®³ã¨å¾©æ—§ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    let node = NodeInfo {
        node_id: 0,
        address: "192.168.1.100".to_string(),
        port: 12345,
        gpu_count: 4,
        capabilities: NodeCapabilities {
            memory_gb: 64.0,
            cpu_cores: 16,
            gpu_memory_gb: 32.0,
            network_bandwidth_gbps: 10.0,
        },
        status: NodeStatus::Available,
    };

    let cluster_config = ClusterConfig {
        master_addr: "192.168.1.1".to_string(),
        master_port: 12345,
        worker_nodes: vec![node],
        topology: ClusterTopology::Flat,
        fault_tolerance: fault_config,
    };

    let mut cluster_manager = ClusterManager::new(cluster_config)?;

    // Simulate node failure
    // ãƒãƒ¼ãƒ‰éšœå®³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    println!("ğŸ”¥ Simulating node failure...");
    match cluster_manager.handle_node_failure(0) {
        Ok(_) => println!("âœ… Node failure handled successfully"),
        Err(e) => println!("âš ï¸  Node failure handling failed (expected in demo): {:?}", e),
    }

    // Show recovery mechanisms
    // å¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¡¨ç¤º
    println!("ğŸ”„ Recovery mechanisms:");
    println!("   â€¢ Automatic failover to backup nodes");
    println!("   â€¢ Job migration and state restoration");
    println!("   â€¢ Gradient synchronization recovery");
    println!("   â€¢ Checkpoint-based model recovery");

    Ok(())
}

/// Helper function to print section separator
/// ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒºåˆ‡ã‚Šã‚’å°åˆ·ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
#[allow(dead_code)]
fn print_separator(title: &str) {
    println!("\n{}", "=".repeat(50));
    println!("{}", title);
    println!("{}", "=".repeat(50));
}
