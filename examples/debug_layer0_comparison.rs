//! Layer 0 comparison tool - Debug RusTorch vs llama.cpp
//!
//! ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã‚’è¡Œã„ã¾ã™ï¼š
//! 1. GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³1(BOS)ã®åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º
//! 2. Layer 0ã®RMSNormè¨ˆç®—ã‚’æ‰‹å‹•å®Ÿè£…
//! 3. RoPEé©ç”¨ã‚’æ‰‹å‹•å®Ÿè£…
//! 4. Attentionè¨ˆç®—ã‚’æ®µéšçš„ã«å®Ÿè¡Œ
//! 5. å„ã‚¹ãƒ†ãƒƒãƒ—ã§RusTorchã®å‡ºåŠ›ã¨æ¯”è¼ƒ

use anyhow::Result;
use rustorch::formats::gguf::GGUFLoader;

fn main() -> Result<()> {
    let model_path = std::env::args()
        .nth(1)
        .unwrap_or_else(|| {
            std::env::var("HOME")
                .unwrap_or_else(|_| "/Users/junsuzuki".to_string())
                + "/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
        });

    println!("ğŸ” GGUF Layer 0 Comparison Tool");
    println!("================================\n");
    println!("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {}\n", model_path);

    // Load GGUF
    let loader = GGUFLoader::from_file(&model_path)?;

    // Step 1: ãƒˆãƒ¼ã‚¯ãƒ³1(BOS)ã®åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º
    println!("ğŸ“Š Step 1: ãƒˆãƒ¼ã‚¯ãƒ³1(BOS)ã®åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡º");
    println!("------------------------------------------");

    let token_embd = loader.load_tensor("token_embd.weight")?;

    println!("token_embd.weight shape: {:?}", token_embd.shape());

    // GGUFå½¢å¼: [hidden_size, vocab_size] = [2048, 32000]
    // Token 1ã®åŸ‹ã‚è¾¼ã¿: flatten()[dim * 32000 + 1] for dim in 0..2048
    let hidden_size = 2048;
    let vocab_size = 32000;
    let token_id = 1usize; // BOS token

    // Tensorã‚’flattenã—ã¦ã‹ã‚‰å€¤ã‚’å–å¾—
    let token_embd_flat = token_embd.data.iter().copied().collect::<Vec<_>>();
    println!("token_embd.weight total elements: {}", token_embd_flat.len());

    let mut embedding = Vec::with_capacity(hidden_size);
    for dim in 0..hidden_size {
        let idx = dim * vocab_size + token_id;
        embedding.push(token_embd_flat[idx]);
    }

    println!("\nâœ… ãƒˆãƒ¼ã‚¯ãƒ³ {} ã®åŸ‹ã‚è¾¼ã¿ï¼ˆæœ€åˆã®10å€¤ï¼‰:", token_id);
    for i in 0..10 {
        println!("  embedding[{}] = {:.10}", i, embedding[i]);
    }

    // çµ±è¨ˆæƒ…å ±
    let sum: f64 = embedding.iter().sum();
    let mean = sum / embedding.len() as f64;
    let sum_sq: f64 = embedding.iter().map(|&x| x * x).sum();
    let rms = (sum_sq / embedding.len() as f64).sqrt();
    let min = embedding.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max = embedding.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    let non_zero = embedding.iter().filter(|&&x| x.abs() > 1e-10).count();

    println!("\nğŸ“ˆ åŸ‹ã‚è¾¼ã¿çµ±è¨ˆ:");
    println!("  RMS:      {:.10}", rms);
    println!("  Mean:     {:.10}", mean);
    println!("  Min:      {:.10}", min);
    println!("  Max:      {:.10}", max);
    println!("  Non-zero: {}/{}", non_zero, embedding.len());

    // Step 2: Layer 0ã®RMSNorm weightã‚’æŠ½å‡º
    println!("\nğŸ“Š Step 2: Layer 0ã®RMSNorm weightã‚’æŠ½å‡º");
    println!("------------------------------------------");

    let attn_norm = loader.load_tensor("blk.0.attn_norm.weight")?;
    let attn_norm_flat = attn_norm.data.iter().copied().collect::<Vec<_>>();

    println!("blk.0.attn_norm.weight shape: {:?}", attn_norm.shape());
    println!("blk.0.attn_norm.weight elements: {}", attn_norm_flat.len());

    println!("\næœ€åˆã®10å€¤:");
    for i in 0..10 {
        println!("  attn_norm[{}] = {:.10}", i, attn_norm_flat[i]);
    }

    // Step 3: RMSNormã‚’æ‰‹å‹•è¨ˆç®—
    println!("\nğŸ“Š Step 3: RMSNormã‚’æ‰‹å‹•è¨ˆç®—");
    println!("------------------------------------------");

    let epsilon = 1e-5;

    // RMSNorm formula: x * weight / sqrt(mean(x^2) + epsilon)
    let sum_sq: f64 = embedding.iter().map(|&x| x * x).sum();
    let mean_sq = sum_sq / embedding.len() as f64;
    let rms_norm_scale = 1.0 / (mean_sq + epsilon).sqrt();

    println!("sum_sq:         {:.10}", sum_sq);
    println!("mean_sq:        {:.10}", mean_sq);
    println!("rms_norm_scale: {:.10}", rms_norm_scale);

    let mut normalized = Vec::with_capacity(hidden_size);
    for i in 0..hidden_size {
        let normed = embedding[i] * rms_norm_scale * attn_norm_flat[i];
        normalized.push(normed);
    }

    println!("\nâœ… RMSNormé©ç”¨å¾Œï¼ˆæœ€åˆã®10å€¤ï¼‰:");
    for i in 0..10 {
        println!("  normalized[{}] = {:.10}", i, normalized[i]);
    }

    // æ­£è¦åŒ–å¾Œã®çµ±è¨ˆ
    let sum: f64 = normalized.iter().sum();
    let mean = sum / normalized.len() as f64;
    let sum_sq: f64 = normalized.iter().map(|&x| x * x).sum();
    let rms = (sum_sq / normalized.len() as f64).sqrt();
    let min = normalized.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max = normalized.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

    println!("\nğŸ“ˆ æ­£è¦åŒ–å¾Œã®çµ±è¨ˆ:");
    println!("  RMS:  {:.10}", rms);
    println!("  Mean: {:.10}", mean);
    println!("  Min:  {:.10}", min);
    println!("  Max:  {:.10}", max);

    // Step 4: Q projection weightã‚’æŠ½å‡ºã—ã¦æœ€åˆã®å€¤ã‚’è¨ˆç®—
    println!("\nğŸ“Š Step 4: Q projectionè¨ˆç®—ï¼ˆæœ€åˆã®10æ¬¡å…ƒã®ã¿ï¼‰");
    println!("------------------------------------------");

    let q_weight = loader.load_tensor("blk.0.attn_q.weight")?;
    let q_weight_flat = q_weight.data.iter().copied().collect::<Vec<_>>();

    println!("blk.0.attn_q.weight shape: {:?}", q_weight.shape());
    println!("blk.0.attn_q.weight elements: {}", q_weight_flat.len());

    // Q projection: normalized @ q_weight
    // q_weight shape: [hidden_size, hidden_size] = [2048, 2048]
    // normalized shape: [1, 2048]
    // result: [1, 2048]

    println!("\nQ projection æœ€åˆã®10æ¬¡å…ƒã‚’è¨ˆç®—:");
    for i in 0..10 {
        let mut sum = 0.0;
        for j in 0..hidden_size {
            // Row-major layout: q_weight[j, i] = data[j * 2048 + i]
            let idx = j * hidden_size + i;
            sum += normalized[j] * q_weight_flat[idx];
        }
        println!("  q[{}] = {:.10}", i, sum);
    }

    // Step 5: RoPEã®å‘¨æ³¢æ•°ã‚’è¨ˆç®—
    println!("\nğŸ“Š Step 5: RoPEå‘¨æ³¢æ•°ã®è¨ˆç®—");
    println!("------------------------------------------");

    let rope_base = 10000.0_f64;
    let head_dim = 64; // 2048 / 32 heads
    let position = 0; // æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³

    println!("RoPE base: {}", rope_base);
    println!("Head dimension: {}", head_dim);
    println!("Position: {}", position);

    println!("\næœ€åˆã®10æ¬¡å…ƒã®RoPEå‘¨æ³¢æ•°:");
    for i in 0..5 {
        let dim = i * 2;
        let freq = 1.0 / rope_base.powf(dim as f64 / head_dim as f64);
        let angle = position as f64 * freq;
        let cos_val = angle.cos();
        let sin_val = angle.sin();
        println!("  dim {}: freq={:.10}, angle={:.10}, cos={:.10}, sin={:.10}",
                 dim, freq, angle, cos_val, sin_val);
    }

    // Step 6: output_norm.weightã‚’ç¢ºèª
    println!("\nğŸ“Š Step 6: output_norm.weightï¼ˆæœ€çµ‚RMSNormï¼‰ã‚’ç¢ºèª");
    println!("------------------------------------------");

    let output_norm = loader.load_tensor("output_norm.weight")?;
    let output_norm_flat = output_norm.data.iter().copied().collect::<Vec<_>>();

    println!("output_norm.weight shape: {:?}", output_norm.shape());
    println!("output_norm.weight elements: {}", output_norm_flat.len());

    println!("\næœ€åˆã®10å€¤:");
    for i in 0..10 {
        println!("  output_norm[{}] = {:.10}", i, output_norm_flat[i]);
    }

    // çµ±è¨ˆ
    let sum: f64 = output_norm_flat.iter().sum();
    let mean = sum / output_norm_flat.len() as f64;
    let sum_sq: f64 = output_norm_flat.iter().map(|&x| x * x).sum();
    let rms = (sum_sq / output_norm_flat.len() as f64).sqrt();
    let min = output_norm_flat.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max = output_norm_flat.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

    println!("\noutput_norm.weightçµ±è¨ˆ:");
    println!("  RMS:  {:.10}", rms);
    println!("  Mean: {:.10}", mean);
    println!("  Min:  {:.10}", min);
    println!("  Max:  {:.10}", max);

    // Step 7: output.weightã‚’ç¢ºèª
    println!("\nğŸ“Š Step 7: output.weightï¼ˆlm_headï¼‰ã‚’ç¢ºèª");
    println!("------------------------------------------");

    let output_weight = loader.load_tensor("output.weight")?;
    let output_flat = output_weight.data.iter().copied().collect::<Vec<_>>();

    println!("output.weight shape: {:?}", output_weight.shape());
    println!("output.weight elements: {}", output_flat.len());

    println!("\næœ€åˆã®10å€¤:");
    for i in 0..10 {
        println!("  output[{}] = {:.10}", i, output_flat[i]);
    }

    // Token 13487ã®é‡ã¿ã‚’ç¢ºèªï¼ˆRusTorchã§æœ€é«˜ãƒ­ã‚¸ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
    println!("\nâš ï¸  Token 13487 (diplom - RusTorchã§æœ€é«˜ãƒ­ã‚¸ãƒƒãƒˆ) ã®é‡ã¿:");
    println!("output.weight layout: [hidden_size, vocab_size] = [2048, 32000]");
    println!("Token 13487ã®é‡ã¿ = output_weight[:, 13487]");

    let token_13487_start = 13487;
    println!("\næœ€åˆã®10æ¬¡å…ƒ:");
    for dim in 0..10 {
        let idx = dim * vocab_size + token_13487_start;
        println!("  dim {}: {:.10}", dim, output_flat[idx]);
    }

    // çµ±è¨ˆ
    let mut token_13487_weights = Vec::with_capacity(hidden_size);
    for dim in 0..hidden_size {
        let idx = dim * vocab_size + token_13487_start;
        token_13487_weights.push(output_flat[idx]);
    }

    let sum: f64 = token_13487_weights.iter().sum();
    let mean = sum / token_13487_weights.len() as f64;
    let sum_sq: f64 = token_13487_weights.iter().map(|&x| x * x).sum();
    let rms = (sum_sq / token_13487_weights.len() as f64).sqrt();
    let min = token_13487_weights.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max = token_13487_weights.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

    println!("\nToken 13487ã®é‡ã¿çµ±è¨ˆ:");
    println!("  RMS:  {:.10}", rms);
    println!("  Mean: {:.10}", mean);
    println!("  Min:  {:.10}", min);
    println!("  Max:  {:.10}", max);

    println!("\nâœ… æ¯”è¼ƒæº–å‚™å®Œäº†");
    println!("=====================================");
    println!("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:");
    println!("1. RusTorchã‚’å®Ÿè¡Œã—ã¦ä¸Šè¨˜ã®å€¤ã¨æ¯”è¼ƒ");
    println!("2. ç™ºæ•£ã—ã¦ã„ã‚‹ç®‡æ‰€ã‚’ç‰¹å®š");
    println!("3. è©²å½“ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£");

    Ok(())
}