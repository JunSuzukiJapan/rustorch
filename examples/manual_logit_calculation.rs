/// æ‰‹å‹•ã§logitã‚’è¨ˆç®—ã—ã¦matmulã®çµæœã¨æ¯”è¼ƒ
///
/// Token 20780ãŒå¸¸ã«æœ€é«˜logitã«ãªã‚‹ç†ç”±ã‚’çªãæ­¢ã‚ã‚‹

use rustorch::formats::gguf::GGUFLoader;
use rustorch::hybrid_f32::error::{F32Result, F32Error};
use rustorch::hybrid_f32::models::{F32LlamaModel, DeviceType};

fn main() -> F32Result<()> {
    println!("ğŸ§® æ‰‹å‹•Logitè¨ˆç®—ãƒ†ã‚¹ãƒˆ\n");

    let model_path = std::env::var("HOME").unwrap() +
        "/.rustorch/models/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf";

    // ãƒ¢ãƒ‡ãƒ«ã®forward pass
    println!("ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¨æ¨è«–...");
    let mut model = F32LlamaModel::from_gguf_with_device(&model_path, DeviceType::Cpu)?;
    let input = vec![1]; // BOS
    let logits = model.forward(&input)?;
    let logits_data = logits.as_slice();

    println!("âœ… Forward passå®Œäº†");
    println!("   Logit Token 450: {:.8}", logits_data[450]);
    println!("   Logit Token 20780: {:.8}", logits_data[20780]);
    println!("   Logit Token 12517: {:.8}", logits_data[12517]);

    // æœ€å¾Œã®hidden stateã‚’å–å¾—ï¼ˆ/tmp/hidden_state.txtã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ï¼‰
    println!("\nğŸ“„ Hidden stateã‚’èª­ã¿è¾¼ã¿...");
    let hidden_state_str = std::fs::read_to_string("/tmp/hidden_state.txt")
        .map_err(|e| F32Error::device_error(format!("Failed to read hidden state: {}", e)))?;

    let hidden_state: Vec<f64> = hidden_state_str
        .lines()
        .map(|line| line.trim().parse::<f64>().unwrap_or(0.0))
        .collect();

    println!("âœ… Hidden stateèª­ã¿è¾¼ã¿å®Œäº†: {} è¦ç´ ", hidden_state.len());
    println!("   First 10: {:?}", &hidden_state[0..10]);

    // output.weightã‚’ç›´æ¥èª­ã¿è¾¼ã¿
    println!("\nğŸ“‚ output.weightã‚’èª­ã¿è¾¼ã¿...");
    let loader = GGUFLoader::from_file(&model_path)
        .map_err(|e| F32Error::device_error(format!("Failed to load GGUF: {}", e)))?;

    let output_tensor = loader.load_tensor("output.weight")
        .map_err(|e| F32Error::device_error(format!("Failed to load tensor: {}", e)))?;

    let output_data: Vec<f64> = output_tensor.data.iter().cloned().collect();
    println!("âœ… output.weightèª­ã¿è¾¼ã¿å®Œäº†");
    println!("   Shape: {:?}", output_tensor.shape());
    println!("   Data length: {}", output_data.len());

    // æ‰‹å‹•ã§Token 450ã®logitã‚’è¨ˆç®—
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ§® æ‰‹å‹•è¨ˆç®—: Token 450");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let hidden_size = 2048;
    let vocab_size = 32000;
    let token_450 = 450;

    // output.weightã¯ [hidden_size, vocab_size] = [2048, 32000]
    // Token 450ã®åˆ— = [output_data[0*32000 + 450], output_data[1*32000 + 450], ..., output_data[2047*32000 + 450]]
    let mut logit_450_manual = 0.0f64;
    for dim in 0..hidden_size {
        let weight_idx = dim * vocab_size + token_450;
        logit_450_manual += hidden_state[dim] * output_data[weight_idx];
    }

    println!("æ‰‹å‹•è¨ˆç®— logit: {:.8}", logit_450_manual);
    println!("Matmul logit:    {:.8}", logits_data[450] as f64);
    println!("å·®åˆ†:            {:.8}", (logit_450_manual - logits_data[450] as f64).abs());

    if (logit_450_manual - logits_data[450] as f64).abs() < 0.01 {
        println!("âœ… ä¸€è‡´ï¼");
    } else {
        println!("âŒ ä¸ä¸€è‡´ï¼");
    }

    // Token 20780ã‚‚è¨ˆç®—
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ§® æ‰‹å‹•è¨ˆç®—: Token 20780");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let token_20780 = 20780;
    let mut logit_20780_manual = 0.0f64;
    for dim in 0..hidden_size {
        let weight_idx = dim * vocab_size + token_20780;
        logit_20780_manual += hidden_state[dim] * output_data[weight_idx];
    }

    println!("æ‰‹å‹•è¨ˆç®— logit: {:.8}", logit_20780_manual);
    println!("Matmul logit:    {:.8}", logits_data[20780] as f64);
    println!("å·®åˆ†:            {:.8}", (logit_20780_manual - logits_data[20780] as f64).abs());

    if (logit_20780_manual - logits_data[20780] as f64).abs() < 0.01 {
        println!("âœ… ä¸€è‡´ï¼");
    } else {
        println!("âŒ ä¸ä¸€è‡´ï¼");
    }

    // æœ€ã‚‚å¯„ä¸ãŒå¤§ãã„æ¬¡å…ƒã‚’è¦‹ã¤ã‘ã‚‹
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ” Token 20780ã§æœ€ã‚‚å¯„ä¸ãŒå¤§ãã„æ¬¡å…ƒ");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let mut contributions: Vec<(usize, f64)> = Vec::new();
    for dim in 0..hidden_size {
        let weight_idx = dim * vocab_size + token_20780;
        let contrib = hidden_state[dim] * output_data[weight_idx];
        contributions.push((dim, contrib));
    }

    contributions.sort_by(|a, b| b.1.abs().partial_cmp(&a.1.abs()).unwrap());

    println!("Top 10å¯„ä¸ï¼ˆçµ¶å¯¾å€¤ï¼‰:");
    for (i, (dim, contrib)) in contributions.iter().take(10).enumerate() {
        let hidden_val = hidden_state[*dim];
        let weight_val = output_data[dim * vocab_size + token_20780];
        println!("   {}. dim[{}]: contrib={:.6}, hidden={:.6}, weight={:.6}",
                 i + 1, dim, contrib, hidden_val, weight_val);
    }

    // Token 450ã§ã‚‚åŒæ§˜ã«
    println!("\nğŸ” Token 450ã§æœ€ã‚‚å¯„ä¸ãŒå¤§ãã„æ¬¡å…ƒ\n");

    let mut contributions_450: Vec<(usize, f64)> = Vec::new();
    for dim in 0..hidden_size {
        let weight_idx = dim * vocab_size + token_450;
        let contrib = hidden_state[dim] * output_data[weight_idx];
        contributions_450.push((dim, contrib));
    }

    contributions_450.sort_by(|a, b| b.1.abs().partial_cmp(&a.1.abs()).unwrap());

    println!("Top 10å¯„ä¸ï¼ˆçµ¶å¯¾å€¤ï¼‰:");
    for (i, (dim, contrib)) in contributions_450.iter().take(10).enumerate() {
        let hidden_val = hidden_state[*dim];
        let weight_val = output_data[dim * vocab_size + token_450];
        println!("   {}. dim[{}]: contrib={:.6}, hidden={:.6}, weight={:.6}",
                 i + 1, dim, contrib, hidden_val, weight_val);
    }

    Ok(())
}
