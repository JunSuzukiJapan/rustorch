//! RMS Norm ã¨ Token Embedding ã®æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
//! llama.cpp ã¨ã®æ¯”è¼ƒç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ãƒ³ãƒ—ã—ã¾ã™

use rustorch::formats::gguf::GGUFLoader;
use std::env;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = env::args().collect();
    if args.len() < 2 {
        eprintln!("Usage: {} <model.gguf>", args[0]);
        eprintln!("Example: {} tinyllama-1.1b-chat-v1.0.Q8_0.gguf", args[0]);
        std::process::exit(1);
    }

    let model_path = &args[1];
    println!("ğŸ“‚ Loading model: {}", model_path);

    let loader = GGUFLoader::from_file(model_path)?;
    let params = loader.get_model_params()?;
    
    println!("\n=== ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===");
    println!("vocab_size: {}", params.vocab_size);
    println!("hidden_size: {}", params.hidden_size);
    println!("num_layers: {}", params.num_layers);
    println!("num_heads: {}", params.num_heads);
    println!("num_kv_heads: {}", params.num_kv_heads);
    println!("context_length: {}", params.context_length);

    // âœ… 1. RMS Norm ã® hidden_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª
    println!("\n=== âœ… 1. RMS Norm hidden_size ç¢ºèª ===");
    println!("d_model (hidden_size): {}", params.hidden_size);
    println!("æœŸå¾…å€¤: 2048");
    if params.hidden_size == 2048 {
        println!("âœ… æ­£ã—ã„å€¤ã§ã™");
    } else {
        println!("âŒ å€¤ãŒç•°ãªã‚Šã¾ã™ï¼");
    }

    // RMS Norm weight ã®ç¢ºèª
    println!("\n=== RMS Norm Weight ç¢ºèª ===");
    let rms_norm_keys = vec![
        "blk.0.attn_norm.weight",
        "blk.0.ffn_norm.weight",
        "output_norm.weight",
    ];

    for key in &rms_norm_keys {
        if let Some(_tensor_info) = loader.get_tensor(key) {
            let tensor = loader.load_tensor(key)?;
            let shape = tensor.data.shape();
            let data_slice = tensor.data.as_slice().unwrap();
            println!("\nğŸ” {}", key);
            println!("   Shape: {:?}", shape);
            println!("   Length: {}", data_slice.len());
            println!("   æœŸå¾…å€¤: [2048]");
            
            if data_slice.len() == 2048 {
                println!("   âœ… é•·ã•ãŒæ­£ã—ã„");
            } else {
                println!("   âŒ é•·ã•ãŒç•°ãªã‚Šã¾ã™ï¼");
            }
            
            // çµ±è¨ˆæƒ…å ±
            let mean: f64 = data_slice.iter().sum::<f64>() / data_slice.len() as f64;
            let min = data_slice.iter().cloned().fold(f64::INFINITY, f64::min);
            let max = data_slice.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
            let rms = (data_slice.iter().map(|&v| v * v).sum::<f64>() / data_slice.len() as f64).sqrt();
            
            println!("   çµ±è¨ˆ: mean={:.6}, min={:.6}, max={:.6}, rms={:.6}", mean, min, max, rms);
            println!("   æœ€åˆã®10è¦ç´ : {:?}", &data_slice[0..10.min(data_slice.len())]);
        } else {
            println!("âŒ {} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", key);
        }
    }

    // âœ… 2. Token Embedding å€¤ã®ç¢ºèª
    println!("\n\n=== âœ… 2. Token Embedding å€¤ç¢ºèª ===");
    println!("llama.cpp ã¨ã®æ¯”è¼ƒç”¨");
    
    let token_emb_key = "token_embd.weight";
    if let Some(_tensor_info) = loader.get_tensor(token_emb_key) {
        let tensor = loader.load_tensor(token_emb_key)?;
        let shape = tensor.data.shape();
        let data_slice = tensor.data.as_slice().unwrap();
        
        println!("\nğŸ“Š Token Embedding ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±:");
        println!("   Shape: {:?}", shape);
        println!("   Total elements: {}", data_slice.len());
        
        let hidden_size = shape[0];
        let vocab_size = shape[1];
        println!("   hidden_size (shape[0]): {}", hidden_size);
        println!("   vocab_size (shape[1]): {}", vocab_size);
        
        // ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³: "1" ã®ãƒˆãƒ¼ã‚¯ãƒ³ID (é€šå¸¸29896)
        let test_tokens = vec![
            (29896, "Token 29896 (\"1\")"),
            (1, "Token 1"),
            (2, "Token 2"),
            (0, "Token 0 (BOS)"),
        ];
        
        println!("\nğŸ” ãƒ†ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿:");
        for (token_id, description) in test_tokens {
            if token_id >= vocab_size {
                println!("\nâŒ {} - ç¯„å›²å¤–", description);
                continue;
            }
            
            let start = token_id * hidden_size;
            let end = start + hidden_size;
            let embedding = &data_slice[start..end];
            
            // çµ±è¨ˆè¨ˆç®—
            let mean: f64 = embedding.iter().sum::<f64>() / embedding.len() as f64;
            let sq_sum: f64 = embedding.iter().map(|&v| v * v).sum();
            let rms = (sq_sum / embedding.len() as f64).sqrt();
            let min = embedding.iter().cloned().fold(f64::INFINITY, f64::min);
            let max = embedding.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
            
            println!("\nğŸ“Œ {}", description);
            println!("   æœ€åˆã®10è¦ç´ : {:?}", &embedding[0..10]);
            println!("   çµ±è¨ˆ: mean={:.9}, rms={:.9}", mean, rms);
            println!("   ç¯„å›²: min={:.9}, max={:.9}", min, max);
            
            // llama.cpp æ¯”è¼ƒç”¨ã®å®Œå…¨ãªãƒ€ãƒ³ãƒ—ï¼ˆæœ€åˆã®20è¦ç´ ï¼‰
            println!("   [llama.cppæ¯”è¼ƒç”¨] æœ€åˆã®20è¦ç´ :");
            for i in 0..20.min(embedding.len()) {
                if i % 5 == 0 {
                    print!("      ");
                }
                print!("{:12.9}", embedding[i]);
                if (i + 1) % 5 == 0 {
                    println!();
                } else {
                    print!(" ");
                }
            }
        }
    } else {
        println!("âŒ Token embedding ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“");
    }

    // âœ… 3. Layer 0 ã®é‡ã¿ç¢ºèªï¼ˆllama.cppæ¯”è¼ƒã®æº–å‚™ï¼‰
    println!("\n\n=== âœ… 3. Layer 0 é‡ã¿ç¢ºèª ===");
    println!("llama.cpp ã¨ã® Layer 0 å‡ºåŠ›æ¯”è¼ƒã®æº–å‚™");
    
    let layer0_keys = vec![
        ("blk.0.attn_norm.weight", "Attention RMS Norm"),
        ("blk.0.attn_q.weight", "Query projection"),
        ("blk.0.attn_k.weight", "Key projection"),
        ("blk.0.attn_v.weight", "Value projection"),
        ("blk.0.attn_output.weight", "Attention output"),
        ("blk.0.ffn_norm.weight", "FFN RMS Norm"),
        ("blk.0.ffn_gate.weight", "FFN gate"),
        ("blk.0.ffn_up.weight", "FFN up"),
        ("blk.0.ffn_down.weight", "FFN down"),
    ];
    
    for (key, description) in layer0_keys {
        if let Some(_tensor_info) = loader.get_tensor(key) {
            let tensor = loader.load_tensor(key)?;
            let shape = tensor.data.shape();
            let data_slice = tensor.data.as_slice().unwrap();
            
            println!("\nğŸ” {} ({})", description, key);
            println!("   Shape: {:?}", shape);
            println!("   Elements: {}", data_slice.len());
            
            // çµ±è¨ˆ
            let mean: f64 = data_slice.iter().sum::<f64>() / data_slice.len() as f64;
            let rms = (data_slice.iter().map(|&v| v * v).sum::<f64>() / data_slice.len() as f64).sqrt();
            let min = data_slice.iter().cloned().fold(f64::INFINITY, f64::min);
            let max = data_slice.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
            
            println!("   çµ±è¨ˆ: mean={:.9}, rms={:.9}", mean, rms);
            println!("   ç¯„å›²: [{:.9}, {:.9}]", min, max);
            println!("   æœ€åˆã®5è¦ç´ : {:?}", &data_slice[0..5.min(data_slice.len())]);
        } else {
            println!("âŒ {} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", key);
        }
    }

    println!("\n\n=== ğŸ¯ æ¤œè¨¼å®Œäº† ===");
    println!("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:");
    println!("1. hidden_size ãŒ 2048 ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª");
    println!("2. Token 29896 ã®åŸ‹ã‚è¾¼ã¿ã‚’ llama.cpp ã®å‡ºåŠ›ã¨æ¯”è¼ƒ");
    println!("3. Layer 0 ã®å‡ºåŠ›ã‚’ llama.cpp ã¨ 1å¯¾1 ã§æ¯”è¼ƒ");
    
    Ok(())
}
